{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:50.868551Z",
     "start_time": "2026-01-07T03:49:50.031061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from utils.helpers import *"
   ],
   "id": "833e2155a44bd6c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "8c565965fce1dc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:50.874823Z",
     "start_time": "2026-01-07T03:49:50.872164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "d168509011200b21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:50.881321Z",
     "start_time": "2026-01-07T03:49:50.878002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "ef60c6a882f064d1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:51.060918Z",
     "start_time": "2026-01-07T03:49:51.058727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "e0613ee1ad154d3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:51.347303Z",
     "start_time": "2026-01-07T03:49:51.345098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "aea9d86581d16b25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:53.436010Z",
     "start_time": "2026-01-07T03:49:53.433443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "1a7c69161bd5f8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "85287aecd00bcda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:53.458386Z",
     "start_time": "2026-01-07T03:49:53.455999Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "f8fbe1e84f3bb189",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:55.508331Z",
     "start_time": "2026-01-07T03:49:55.504874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "b12cb95ea94b6f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:55.883639Z",
     "start_time": "2026-01-07T03:49:55.881550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "7758bb35c5218c2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:56.159371Z",
     "start_time": "2026-01-07T03:49:56.157404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "c44dfc980e752980",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:56.478925Z",
     "start_time": "2026-01-07T03:49:56.476873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "88095c1c82c5ad36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:56.867757Z",
     "start_time": "2026-01-07T03:49:56.865932Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "59d7edf141de197",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:57.204375Z",
     "start_time": "2026-01-07T03:49:57.202437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "e7463ce513b8ba31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:49:57.769325Z",
     "start_time": "2026-01-07T03:49:57.766931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "f15067d59c9f2f52",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:50:00.067976Z",
     "start_time": "2026-01-07T03:50:00.064750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False, False, False, False]\n",
    "warm_start = 10\n",
    "ACTOR_FREEZE = 10 * set_points_len\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "c062e1a79f80912a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:50:01.570678Z",
     "start_time": "2026-01-07T03:50:01.555351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "85da68bee773cd7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "1daeca8ba3164a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:50:03.855806Z",
     "start_time": "2026-01-07T03:50:03.843936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TD3Agent.agent import TD3Agent\n",
    "import torch"
   ],
   "id": "49c428a23b3e48b2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:42.901683Z",
     "start_time": "2026-01-07T04:23:42.898731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "n_outputs = C_aug.shape[0]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 40000\n",
    "ACTOR_LR = 5e-5\n",
    "CRITIC_LR = 5e-4\n",
    "SMOOTHING_STD = 0.005\n",
    "NOISE_CLIP = 0.01\n",
    "# EXPLORATION_NOISE_STD = 0.01\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005 # 0.01\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 256\n",
    "STD_START = 0.02\n",
    "STD_END = 0.001\n",
    "STD_DECAY_RATE = 0.99992\n",
    "STD_DECAY_MODE = \"exp\""
   ],
   "id": "e798424baebbc371",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:43.185373Z",
     "start_time": "2026-01-07T04:23:43.131431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "td3_agent = TD3Agent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    actor_hidden=ACTOR_LAYER_SIZES,\n",
    "    critic_hidden=CRITIC_LAYER_SIZES,\n",
    "    gamma=GAMMA,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    max_action=MAX_ACTION,\n",
    "    tau=TAU,\n",
    "    std_start=STD_START,\n",
    "    std_end=STD_END,\n",
    "    std_decay_rate=STD_DECAY_RATE,\n",
    "    std_decay_mode=STD_DECAY_MODE,\n",
    "    buffer_size=BUFFER_CAPACITY,\n",
    "    device=DEVICE,\n",
    "    actor_freeze=ACTOR_FREEZE,\n",
    "    mode=\"mpc\"\n",
    "    )"
   ],
   "id": "1d8ae390b2843fca",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:43.505846Z",
     "start_time": "2026-01-07T04:23:43.379254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "td3_agent.load(agent_path)"
   ],
   "id": "5d2d2b610564c69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MPC Initialization",
   "id": "fac3637c0ee4f291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:43.865338Z",
     "start_time": "2026-01-07T04:23:43.862310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1.\n",
    "R2_penalty = 1.\n",
    "Q_penalty = np.array([[Q1_penalty, 0], [0, Q2_penalty]])\n",
    "R_penalty = np.array([[R1_penalty, 0], [0, R2_penalty]])"
   ],
   "id": "211634f6f62d4c5",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:44.085592Z",
     "start_time": "2026-01-07T04:23:44.082315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "7bf97648c044ad79",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying RL Agent on the CSTR",
   "id": "65a4fc461d9d0b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:44.619802Z",
     "start_time": "2026-01-07T04:23:44.616502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reward_fn(delta_y, delta_u, Q1_penalty=1, Q2_penalty=1, R1_penalty=1, R2_penalty=1):\n",
    "\n",
    "    # Reward Calculation\n",
    "    reward = - (Q1_penalty * delta_y[0] ** 2 + Q2_penalty * delta_y[1] ** 2 +\n",
    "                R1_penalty * delta_u[0] ** 2 + R2_penalty * delta_u[1] ** 2)\n",
    "\n",
    "    return reward"
   ],
   "id": "317946f8d424ca6c",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:44.853172Z",
     "start_time": "2026-01-07T04:23:44.850521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "829372b3f310f055",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:23:45.137548Z",
     "start_time": "2026-01-07T04:23:45.124784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rl_train(system, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 test_cycle,\n",
    "                 nominal_qi, nominal_qs, nominal_ha,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"disturb\"):\n",
    "\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "    u_min, u_max = min_max_dict[\"u_min\"], min_max_dict[\"u_max\"]\n",
    "\n",
    "    y_system = np.zeros((nFE + 1, n_outputs))\n",
    "    y_system[0, :] = system.current_output\n",
    "    u_rl = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    # xhatdhat[:, 0] = np.random.uniform(low=min_max_dict[\"x_min\"], high=min_max_dict[\"x_max\"])\n",
    "    rewards = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "\n",
    "    delta_y_storage = []\n",
    "\n",
    "    # ----- helper ------\n",
    "    def map_to_bounds(a, low, high):\n",
    "        return low + ((a + 1.0) / 2.0) * (high - low)\n",
    "\n",
    "    test = False\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # train/test phase\n",
    "        if i in test_train_dict:\n",
    "            test = test_train_dict[i]\n",
    "\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # ---- RL state (scaled) ----\n",
    "        current_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i], y_sp[i, :], scaled_current_input_dev)\n",
    "\n",
    "        # ---- TD3 action ----\n",
    "        if not test:\n",
    "            action = agent.take_action(current_rl_state, explore=(not test))\n",
    "        else:\n",
    "            action = agent.act_eval(current_rl_state)\n",
    "        # Map to bounds\n",
    "        u_scaled = map_to_bounds(action, u_min, u_max)\n",
    "\n",
    "        # scale & step plant\n",
    "        u_rl[i, :] = u_scaled + ss_scaled_inputs\n",
    "        u_plant = reverse_min_max(u_rl[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_rl[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record output\n",
    "        y_system[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_system[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_system[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_rl[i, :] - ss_scaled_inputs)) + np.dot(L, (y_prev_scaled - yhat[:, i])).T\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u)\n",
    "\n",
    "        # Record rewards and delta_y\n",
    "        rewards[i] = reward\n",
    "        delta_y_storage.append(np.abs(delta_y))\n",
    "\n",
    "        # ----- Next state for TD3 -----\n",
    "        next_u_dev = u_rl[i, :] - ss_scaled_inputs\n",
    "        next_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i+1], y_sp[i, :], next_u_dev)\n",
    "\n",
    "        # Episode boundary (treat each setpoint block as an episode end)\n",
    "        # done = 1.0 if (i + 1) % boundary == 0 else 0.0\n",
    "        done = 0.0\n",
    "\n",
    "        # Buffer + train (skip if in test phase)\n",
    "        if not test:\n",
    "            agent.push(current_rl_state,\n",
    "                       action.astype(np.float32),\n",
    "                       float(reward),\n",
    "                       next_rl_state,\n",
    "                       float(done))\n",
    "            if i >= WARM_START:\n",
    "                _ = agent.train_step()  # returns loss or None\n",
    "\n",
    "        # diagnostics at sub-episode boundary\n",
    "        if i in sub_episodes_changes_dict:\n",
    "            avg_rewards.append(np.mean(rewards[max(0, i - time_in_sub_episodes + 1): i + 1]))\n",
    "            print('Sub_Episode:', sub_episodes_changes_dict[i], '| avg. reward:', avg_rewards[-1])\n",
    "            if hasattr(agent, \"_expl_sigma\"):\n",
    "                print('Exploration noise:', agent._expl_sigma)\n",
    "\n",
    "    # unscale to plant units for plotting\n",
    "    u_rl = reverse_min_max(u_rl, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha"
   ],
   "id": "c0b20eb3706f0e6e",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:42:17.289130Z",
     "start_time": "2026-01-07T04:23:45.441693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "    = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 TEST_CYCLE,\n",
    "                 nominal_qi, nominal_qs, nominal_hA,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"nominal\")"
   ],
   "id": "eb10ecbf5180672d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 1 | avg. reward: -6.3586161273256785\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.047074273357757\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.7175394832363375\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.095300134316851\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.604626935147463\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.980969085399714\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.026844001998942\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.451234469375522\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.7949109000539805\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.824769292710216\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.959411196299269\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.269337728948454\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.550787702093533\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.802720397928796\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.810614070476413\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -16.359001359518928\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.4999625231654716\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.081803621799279\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8865641915880795\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.9312753965329458\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.9757693570546223\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8667933321778415\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.860423946002463\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.9030090172874579\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7820776815411148\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7911087598938877\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.7645387141209475\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7544430983531947\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.8419433133747529\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.8329517077163178\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.8108643680714099\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.8414601387202297\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.8355606449385854\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.8585303417027192\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.8423847744663427\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.843985132260939\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.9853907368614132\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.0277483833492076\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.8765000463795124\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.7137343748326396\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.7459617384277795\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7309150741476563\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.7285390167185656\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.7181317249659034\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7507044641967948\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.7136434474168718\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.7098282611164024\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.6761317363038463\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.6520820331647446\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6482273584775463\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.6182282887827425\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6274110074240032\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6232531845977172\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5981679771976929\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6032125151350225\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5966875691850438\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5865582122923548\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5729228705923626\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5599953615696065\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5603488850728463\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5520846272872639\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5358224613191359\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5196197051872858\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5321453501561955\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.543331635968855\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5751845541845484\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.6022329733228989\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5868282290913418\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.6078263528573454\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6437848299406619\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6879489935333359\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.7631442447373921\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.7218699864335673\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.7137678597914123\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.6902365564207016\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.6664474970975429\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6583496599476203\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.6734845567674987\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.676589151790351\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6982556441968178\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.7414187843795026\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.7741490585598424\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.8098256337209078\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.7663776901250475\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.7755153093393754\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.8264726820356688\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.9677670824689923\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.1685672340295603\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.5692642134391372\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8428649925383518\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.005321759740323\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.11922182252435\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.5995906122441488\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.5760300125553834\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.9551926028252928\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.2552040766981487\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.5477259511344406\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.8200613288551751\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.7658351927182891\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.8001844245117025\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.7987112269876658\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.7919240526979557\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.7754777940713402\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.7624701626866461\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.7589332602547727\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.7565960386078597\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.7170040386296364\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6562280451228298\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6543262287469509\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6384486389135218\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6573862900813929\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6469771949812249\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6584201429471891\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6412328796665635\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.6565801034711403\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6817860130883077\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.7047113612023785\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.7417887419499966\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.7532613862117122\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.7892624928601494\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.8272789501489746\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.8689908976736163\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.9343174453021477\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.8660486962146748\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.8526201573563444\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.8703913803718586\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.8424460670092612\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.9527993341572111\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.0369467933968206\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.9403134715574515\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.8493717551318427\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.9287780602058683\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.7158373475763742\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.7136377255612331\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.1580232435902142\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.7014068278996105\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.7237930035810031\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.7502699226988188\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6126948890416268\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5780495893668169\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6534238698307527\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.6221342012760367\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5561893753226269\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5904736956963749\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.588849804625827\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5904045595004055\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6376413118818542\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5935526993833108\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.6025275123961749\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6022300790034788\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.6035249819249962\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.6085155101059891\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.626722675825472\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6326619519295111\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.6210501256534121\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6624558815660463\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.7310590492884552\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.7734983731126213\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.6837929178605535\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.759602645238555\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.7487583537245227\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.8622127244690063\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.8148165096813395\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.8445693752316936\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.9376045583328697\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.9721367857176116\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.7451468680616151\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.7432519009331725\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.8269779955265244\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.8108165357769255\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.8562542249845794\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.7985011852321778\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6244517975314091\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5977448218149071\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5662630689440665\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5818549768680764\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5884967581891815\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5982549305424842\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5869270588905301\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5700618111136344\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5735941008137144\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5806077034082721\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5958136204348732\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.6296986709168171\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.6455833004217179\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5802349414704655\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5749001469024575\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5689790819480226\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5739032016844193\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5688002831152797\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5414880992435154\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6094900108494453\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.6025570832219865\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5603829084299092\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5421310045311561\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5599387862203672\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6643534174181478\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5928427300470775\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.6491643777001055\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6133208808637113\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:46:22.792929Z",
     "start_time": "2026-01-07T04:46:22.773382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=None, prefix_name=\"agent_result\",\n",
    "    agent=None,\n",
    "    delta_y_storage=None,\n",
    "    rewards=None,\n",
    "    dist=None,\n",
    "    start_plot_idx=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation-style plotting (same colors/fonts/no legends).\n",
    "    Saves all figures + input_data.pkl to directory/prefix_name/<timestamp>.\n",
    "    Handles:\n",
    "      dist=None\n",
    "      dist=1D array\n",
    "      dist=dict with keys {\"qi\",\"qs\",\"ha\"}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name):\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, name), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    y_sp_original = np.array(y_sp, copy=True)\n",
    "\n",
    "    actor_losses = getattr(agent, \"actor_losses\", None) if agent is not None else None\n",
    "    critic_losses = getattr(agent, \"critic_losses\", None) if agent is not None else None\n",
    "    dy_arr = np.array(delta_y_storage) if delta_y_storage is not None else None\n",
    "    rewards_arr = np.array(rewards) if rewards is not None else None\n",
    "\n",
    "    input_data = {\n",
    "        \"y_sp\": y_sp_original,\n",
    "        \"steady_states\": steady_states,\n",
    "        \"nFE\": nFE,\n",
    "        \"delta_t\": delta_t,\n",
    "        \"time_in_sub_episodes\": time_in_sub_episodes,\n",
    "        \"y_mpc\": y_mpc,\n",
    "        \"u_mpc\": u_mpc,\n",
    "        \"avg_rewards\": avg_rewards,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"warm_start_plot\": warm_start_plot,\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "        \"delta_y_storage\": dy_arr,\n",
    "        \"rewards\": rewards_arr,\n",
    "        \"dist\": dist,\n",
    "        \"start_plot_idx\": start_plot_idx\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"input_data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(input_data, f)\n",
    "\n",
    "    # Canceling the deviation form (same logic)\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # Distillation-style rcParams (no bold globals; bold comes from \\mathbf in labels)\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    # Colors exactly like distillation code\n",
    "    C_QC = \"tab:green\"\n",
    "    C_QM = \"tab:orange\"\n",
    "    C_RW = \"tab:purple\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    warm_start_plot = np.atleast_1d(warm_start_plot) * delta_t\n",
    "    ws_end = float(warm_start_plot.max()) if warm_start_plot.size > 0 else 0.0\n",
    "\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1: outputs (full) --------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 0], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[0, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{\\eta}$ (L/g)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 1], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[1, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{T}$ (K)\", fontsize=18)\n",
    "    ax.set_xlabel(r\"$\\mathbf{Time}$ (hour)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95, bottom=0.12)\n",
    "    _savefig(\"fig_rl_outputs_full.png\")\n",
    "\n",
    "    # -------- last window --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{time_in_sub_episodes}.png\")\n",
    "\n",
    "    # -------- last 4x window --------\n",
    "    W4 = 4 * time_in_sub_episodes\n",
    "    time_plot_4w = np.linspace(0, W4 * delta_t, W4 + 1)\n",
    "\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[0, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[1, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{W4}.png\")\n",
    "\n",
    "    # -------- Plot 2: inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_QC, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_QM, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(\"fig_rl_inputs_full.png\")\n",
    "\n",
    "    # -------- Plot 3: reward per episode --------\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    xep = np.arange(1, len(avg_rewards) + 1)\n",
    "    plt.plot(xep, avg_rewards, \"o-\", lw=2.2, color=C_RW, zorder=2)\n",
    "    plt.ylabel(\"Avg. Reward\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    _savefig(\"fig_rl_rewards.png\")\n",
    "\n",
    "    # -------- optional losses --------\n",
    "    if actor_losses is not None and len(actor_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(actor_losses, lw=1.8, color=\"tab:blue\")\n",
    "        plt.ylabel(\"Actor Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_actor.png\")\n",
    "\n",
    "    if critic_losses is not None and len(critic_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(critic_losses, lw=1.8, color=\"tab:orange\")\n",
    "        plt.ylabel(\"Critic Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_critic.png\")\n",
    "\n",
    "    # -------- optional delta_y windows (no legend) --------\n",
    "    if dy_arr is not None and dy_arr.ndim == 2 and dy_arr.shape[1] >= 2:\n",
    "        n = dy_arr.shape[0]\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w = dy_arr[i0:n]\n",
    "        if len(w) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w[:, 0], c=\"r\")\n",
    "            plt.plot(w[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_last300.png\")\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w2 = dy_arr[j0:j1]\n",
    "        if len(w2) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w2[:, 0], c=\"r\")\n",
    "            plt.plot(w2[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_700_400.png\")\n",
    "\n",
    "    # -------- optional per-step rewards (no legend) --------\n",
    "    if rewards_arr is not None and rewards_arr.ndim == 1 and rewards_arr.size > 0:\n",
    "        n = rewards_arr.size\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w = rewards_arr[j0:j1]\n",
    "        if w.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w.size), w, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_700_400.png\")\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w2 = rewards_arr[i0:n]\n",
    "        if w2.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w2.size), w2, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_last300.png\")\n",
    "\n",
    "        plt.figure(figsize=(7.6, 4.2))\n",
    "        plt.scatter(range(rewards_arr.size), rewards_arr, s=10)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"rewards_all.png\")\n",
    "\n",
    "    # -------- disturbance (no legend) --------\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, dict) and all(k in dist for k in [\"qi\", \"qs\", \"ha\"]):\n",
    "            qi_arr = np.asarray(dist[\"qi\"]).squeeze()\n",
    "            qs_arr = np.asarray(dist[\"qs\"]).squeeze()\n",
    "            ha_arr = np.asarray(dist[\"ha\"]).squeeze()\n",
    "            n_al = min(nFE, qi_arr.shape[0], qs_arr.shape[0], ha_arr.shape[0])\n",
    "\n",
    "            def _dist_fig(t, q1, q2, hA, suffix):\n",
    "                plt.figure(figsize=(7.6, 6.2))\n",
    "\n",
    "                ax = plt.subplot(3, 1, 1)\n",
    "                ax.plot(t, q1, \"-\", lw=2, color=\"tab:blue\")\n",
    "                ax.set_ylabel(r\"$Q_i$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 2)\n",
    "                ax.plot(t, q2, \"-\", lw=2, color=\"tab:orange\")\n",
    "                ax.set_ylabel(r\"$Q_s$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 3)\n",
    "                ax.plot(t, hA, \"-\", lw=2, color=\"tab:green\")\n",
    "                ax.set_xlabel(\"Time (h)\")\n",
    "                ax.set_ylabel(r\"$h_a$ (J/Kh)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                plt.gcf().subplots_adjust(right=0.95, hspace=0.25)\n",
    "                _savefig(f\"fig_disturbances_{suffix}.png\")\n",
    "\n",
    "            _dist_fig(time_plot[:n_al], qi_arr[:n_al], qs_arr[:n_al], ha_arr[:n_al], suffix=\"full\")\n",
    "\n",
    "            if time_in_sub_episodes > 0:\n",
    "                W = min(time_in_sub_episodes, n_al)\n",
    "                t_lastW = np.linspace(0, W * delta_t, W, endpoint=False)\n",
    "                _dist_fig(\n",
    "                    t_lastW,\n",
    "                    qi_arr[n_al - W:n_al],\n",
    "                    qs_arr[n_al - W:n_al],\n",
    "                    ha_arr[n_al - W:n_al],\n",
    "                    suffix=f\"last{W}\"\n",
    "                )\n",
    "        else:\n",
    "            dist_arr = np.asarray(dist).squeeze()\n",
    "            n_al = min(nFE, dist_arr.shape[0])\n",
    "            plt.figure(figsize=(7.2, 4.2))\n",
    "            plt.plot(time_plot[start_plot_idx:n_al], dist_arr[start_plot_idx:n_al], lw=1.8, color=\"tab:blue\")\n",
    "            plt.ylabel(\"Disturbance\")\n",
    "            plt.xlabel(\"Time (h)\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"disturbance.png\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "f7a5be6d8440239a",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:46:26.782866Z",
     "start_time": "2026-01-07T04:46:23.723530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=dir_path, prefix_name=\"polymer_nominal_mpc\",\n",
    "    agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "    dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    ")"
   ],
   "id": "9d08421eb40e999f",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T15:30:29.443542Z",
     "start_time": "2026-01-07T04:54:15.336290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    set_points_number = int(C_aug.shape[0])\n",
    "    inputs_number = int(B_aug.shape[1])\n",
    "    STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "    ACTION_DIM = int(B_aug.shape[1])\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    BUFFER_CAPACITY = 40000\n",
    "    ACTOR_LR = 5e-5\n",
    "    CRITIC_LR = 5e-4\n",
    "    SMOOTHING_STD = 0.005\n",
    "    NOISE_CLIP = 0.01\n",
    "    # EXPLORATION_NOISE_STD = 0.01\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.005  # 0.01\n",
    "    MAX_ACTION = 1\n",
    "    POLICY_DELAY = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 256\n",
    "    STD_START = 0.02\n",
    "    STD_END = 0.001\n",
    "    STD_DECAY_RATE = 0.99992\n",
    "    STD_DECAY_MODE = \"exp\"\n",
    "    td3_agent = TD3Agent(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        actor_hidden=ACTOR_LAYER_SIZES,\n",
    "        critic_hidden=CRITIC_LAYER_SIZES,\n",
    "        gamma=GAMMA,\n",
    "        actor_lr=ACTOR_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        policy_delay=POLICY_DELAY,\n",
    "        target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "        noise_clip=NOISE_CLIP,\n",
    "        max_action=MAX_ACTION,\n",
    "        tau=TAU,\n",
    "        std_start=STD_START,\n",
    "        std_end=STD_END,\n",
    "        std_decay_rate=STD_DECAY_RATE,\n",
    "        std_decay_mode=STD_DECAY_MODE,\n",
    "        buffer_size=BUFFER_CAPACITY,\n",
    "        device=DEVICE,\n",
    "        actor_freeze=ACTOR_FREEZE,\n",
    "        mode=\"mpc\"\n",
    "    )\n",
    "    agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "    td3_agent.load(agent_path)\n",
    "    cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "    y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "        = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                     steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                     L, data_min, data_max, warm_start,\n",
    "                     TEST_CYCLE,\n",
    "                     nominal_qi, nominal_qs, nominal_hA,\n",
    "                     qi_change, qs_change, ha_change,\n",
    "                     reward_fn, mode=\"nominal\")\n",
    "    out_dir = plot_rl_results_disturbance(\n",
    "        y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "        y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "        directory=dir_path, prefix_name=\"polymer_nominal_mpc\",\n",
    "        agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "        dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    "    )"
   ],
   "id": "fdb4470b25c16864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.115992408964427\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.5163528113481695\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.678058307107969\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.307366719042563\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.56520850370251\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.307595702267001\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.93725901794521\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.393662840807073\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.167855242921246\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.44003724370408\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.873133772830565\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.7491830829854145\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.504538546953061\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.426856010505576\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.870690252563488\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -10.9824691359691\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.5575936382421323\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.0575007578497104\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.0327042306258019\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.9975179809456499\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.9670358600598497\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.0341226036161135\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.4297603656445312\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.5430243147739284\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.806560242027112\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.9174990647034992\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.8708020086981263\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.9070331257758548\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.769475014157266\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.7608660954608211\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.8050169133376847\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.8043082665512863\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.8482347575609773\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.8021825999623816\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.7842334490348418\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.760978958889707\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.7869451967647058\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.7437687774694206\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.7241464463989005\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.7095341703453718\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.7331711886073162\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7386394833681824\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.7374377544323594\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.7822182612806641\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7659082445949816\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.7544845789068217\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.7525195825896701\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.7538792409081942\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.8149639793337096\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.863537672977765\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.7549661276575663\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6922263537183201\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6741779509855453\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.6632847606525775\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6396450892084442\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.6133260878670551\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.6010906799280972\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5996190894683022\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5832566274062658\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5560561300419112\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.562032119090296\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5889301537872057\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5688551009702166\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5784152164751043\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5739382150566954\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5753118693209132\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5586416025252177\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5513174680289379\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5771851626008955\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5985510357504485\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5941728068181417\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6072885334029042\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5929281986582687\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.598500912930523\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.655040765904556\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.6591968647871903\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6039651000537225\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.613048274895227\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.6738178754918127\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6187331278837381\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.6524359257601966\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.6578674219385203\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6499893959264679\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.6486641687248649\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6481351698831392\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6743247027115111\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.660845707369829\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.6380048491929482\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.6267809282053772\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6476817815322583\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.6862700772036997\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.6889363866550147\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.6877054756138306\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.6642954653088714\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.6769150932544556\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.6039472318739496\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.6154347236870563\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.614301763769031\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5690683709102358\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5527178521747065\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5499924447569182\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5348244039566168\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5216050302281555\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5218351683032806\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.516196349782356\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5294138227998968\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5375191731990363\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5285242827357228\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5266994931476482\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5267429847969297\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5253416977344496\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5267267136199439\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5282123981321312\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5378026130767479\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5640575976099047\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5672858937055938\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5373355475618904\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.539564451192624\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5435792201222744\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5358809428831393\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5426131922447179\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.550093106859562\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5611099026859442\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5326133828574904\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5566086255259457\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5515530975227754\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5416065356884439\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5471087780813384\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5452208907860506\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5671940964267316\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.603227285724335\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5717397258709461\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5965356323046332\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.1465182508890621\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.8745126683328825\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.6680626533167809\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.7782784133990123\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.626909204829822\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.8159030160261339\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5640367711493138\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.7205244007001455\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.6165028559018179\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.6380050100518521\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5719190666966025\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5656875822420218\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.6000862268227336\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6044892099917967\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.6114256644197803\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.6127824116772655\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5848979584506612\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.568844433456893\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5487387420955884\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5584432875109262\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5407043415871765\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5400339489879498\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5409822447594435\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5392503998156065\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5277579779741941\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5406349735911129\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5363638561781152\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.551665257393884\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5357963378828725\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5445247454379399\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5814809453722551\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.0291030033167181\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -2.3391391881469024\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -5.705246705149099\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -2.6579557211346208\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -2.649934529704402\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -2.457055270807104\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -2.1889279433752167\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.153544811533422\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.9213687008312186\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.7903972283742298\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.8043712639179597\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.8307573849433976\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.7932759408690212\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.868183761090219\n",
      "Exploration noise: 0.0010002143250596393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\system_functions.py:61: RuntimeWarning: invalid value encountered in scalar power\n",
      "  CP = (2 * self.fi * kd * CI / kt) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 179 | avg. reward: -7928.9575265546\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -17.48051339351656\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.9377032321415291\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.8598677540385702\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.0736704399798034\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.7946652418393799\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.965126506581012\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.9705973075262692\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.8656790232296643\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.7515708399664957\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.7125146224019907\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.9176883270908734\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.7781341307944257\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.7104088961051858\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.7384170627487007\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.9110860338509906\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.886353648146325\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.0093609030453\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.8518282095052675\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.8679231655775042\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.8777728197601568\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.8058333670664556\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.401923121760042\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -8.188590870660258\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.20097442040124\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.045903945949875\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.904501912377976\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.014431473887621\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.172460444509791\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.81445998640076\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.420836046847802\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.1909125335339965\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.087849032440097\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.802212845675228\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.845897768050585\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.600015854326439\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.9757263714786975\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -9.443357819075036\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.8578199887912341\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.147338068029953\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.0557751683282424\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.945552626528129\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8893156301759242\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8941450840162136\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.799069784146915\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.7804454336479756\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7637187807449791\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7620915467109098\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.7477664804765003\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7213817651293363\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.7454162152922098\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.7426830111240128\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.9051664286323109\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.7160544151197865\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.699151329423788\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.6979737021155676\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6853816122380024\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6733875737423034\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6796379032567955\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.7193440182681107\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.7214900421089777\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.7086275587496371\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.70077885428837\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.6713431809418092\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.6826345854314428\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.6929508854699995\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7054420212494663\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.7323374043689715\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.7271171985339704\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.6765322841662186\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.660452299613659\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6465286600291189\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.6377651629862052\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6677735948818233\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.7238565752998383\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.702215236725549\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.7056248446776415\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.7434146720582373\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -3.5850623678591544\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.2431977609114215\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.9500185983276508\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.8054828559759861\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.6774728731154844\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.6278488373816561\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.6557640495534222\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.647277411549053\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.6750272473143073\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.7475045295356315\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.9606600540395064\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.0186738627730234\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.7770190281227007\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.788304485767992\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6700377050685876\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6466031259289177\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.6094149377668652\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.6117348396053649\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5767610936541915\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5780474434836027\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6332558447048429\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.6497854883956375\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.6614807835737021\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6214019570679946\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5707530956941178\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.62565700218077\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5784846993082862\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5995487776969438\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5668986888886235\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5367137256511735\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5554473091228229\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5741177341925159\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5872803495839382\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6047146859163683\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5436202569291553\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.535337183521393\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5264748385805583\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5309058978896911\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5387277852486957\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5650612139323324\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5729787494765248\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5701320173219012\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5774986645387492\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5551051820948674\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5652002951228298\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.573869027826886\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.587004974916806\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.602255243116045\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.6041097143624238\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5968382493386\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.6059269565147283\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6217404171747186\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.5502613747460967\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.7280909744124385\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6186598119543372\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6169841401359288\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6332908070803839\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6790943614778127\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.63791229488198\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.3093713952918897\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.4327660025885307\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.0893762983383979\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.9879962799740976\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.6037943243901057\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5774405399879241\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5691357014128812\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5494862818481695\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.551844379372558\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5476596325455608\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5614358854013897\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5894300698696117\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.6194902140377025\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.6092855292624745\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.646650295816038\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.6365421548976725\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5692268635436837\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5596624881711953\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.9331233425448908\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.4476099259694342\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5866962844335257\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.626522114586952\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5888024069787684\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6003155605936812\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.6189800405905794\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -5.125262348864062\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.6059609394523497\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.7105773818754934\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.580183282112229\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5962103795903901\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5803582942183426\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5481707533188576\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5163468176240432\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.521021570153094\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5601615055245363\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5600281284646562\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.6052434057147891\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5543610971094884\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5332392705786347\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5483260617458118\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.541389286264772\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5414179953145186\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5281920615492113\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5344494904054852\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5301802906051022\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5256336164547963\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5254372742411135\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5209472035385643\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5328610237541983\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5376056852145275\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5437730622551703\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5403519906153157\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5353031688587164\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.529451725690117\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5264835209659091\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.532628257755583\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5357090084846609\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5583299249931488\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5501492577088333\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5189872289790145\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5419877737758496\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5624305670116964\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5710021504336826\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5585098696477437\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.54764062171739\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.538626574252279\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5437478997090881\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5419148575637307\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.558331624049601\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5657070882502143\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.553916392908481\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5295915793029894\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5435381875122515\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5338700129403966\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5227478494236099\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5184948867416967\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.542972412538357\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5212879976688091\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5358325641703716\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5513997771690988\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5564636984006861\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5533882030295159\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5653396711397278\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5654992480018938\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5711006407313584\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.408521823692428\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.065005992599229\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.893358344039944\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.30450214140853\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.085120776799624\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.75758645635033\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.867266325775529\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.389327425931473\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.144766848455029\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.619336510871795\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.125567027450111\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.732301520538681\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.739601264795059\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.214848435969043\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -7.103092924725479\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -18.905446251397453\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.534187948418577\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.0983672178444945\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9777718066697085\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8797083394550083\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.9400186148466738\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.7166695171934495\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.7105589093058433\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.8106706443092176\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6061592513982583\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6248610989065747\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.5896595698036537\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5772805722166298\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5517199294961865\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5873111585150927\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5454430770442604\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.5473651868497934\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5309842881348831\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5750247742188571\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5961979822949028\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5668982303866668\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5445275259911008\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5345359037985449\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5314726608547344\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5336316672806801\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5289871670436562\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5444570282112581\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5572215702609301\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5437286644221014\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5482528300904363\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5384858381821424\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.6063032994513547\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5188342147791003\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.524012080304022\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5258586203821008\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5401137402538236\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5339963758102841\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.4986178377729135\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5287181390199365\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5442339338083413\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.535941994670789\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.530809944402747\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5060072580655198\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5134493642630142\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5223072638274434\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5171227494970505\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5334189772584235\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5234436923594779\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5209914634330168\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5183368050224026\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5352232268812118\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.540641987017989\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5284046797472581\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5650938072775861\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5459699436355356\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5649075486027317\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5429304487469072\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5466550539348326\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5440453552011739\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.562829244463578\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5607628420099823\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5687554915709448\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5866325381918581\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5674992291012749\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.565352676558326\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.6033521763873381\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.7094094052345892\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6993315717780197\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.7044931478535466\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -5.0127209487868525\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.7730819062183083\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.7239550026781263\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.216948848463865\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.9086570867422981\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.8380948958408367\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.0443213043475987\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.2340493047920564\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.4045032824942567\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.8402639508251573\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -2.5685781466727873\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -3.1922815746864384\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.7882429539265168\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.203371296606289\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.5691978151914847\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.3188333973078097\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.1621005024783615\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.2883872688499705\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.1861443800585452\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.059029050841886\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.0385936473636486\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.4552894439626656\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.8086556747325778\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.7374748531360967\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.8209781921678669\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.8551076470837339\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -3.4653995977759813\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.8621711560775681\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.7546273531826315\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.571442610108184\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.7455932642836863\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.1358567423843262\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.8188591486165266\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -3.5739159240856218\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.3351174250969178\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.1657804286766764\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.3544458541479052\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.316403063563022\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.5069358413057512\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.2270865670509057\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.000802642504698\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.0556408808137967\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.8155521363822067\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.8100566208735515\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.8334305623564469\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.761152975083624\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.8483832340865782\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.9691420750285005\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.7802898510717705\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.6911470403630511\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.701149922346573\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.6714140076265369\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.6228998649874247\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5970096428551459\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5730806293193467\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5901737011477322\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.7101094630170087\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5802023759735134\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5961810832214404\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5887415384545548\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.6162210913598396\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.610942476218449\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.602892842772681\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5722725258132447\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5883106190693597\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6047324791186173\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5754522638381522\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5836280085573593\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5881643394075831\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5687474390452811\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5939217305824038\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5769831357503898\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.585098137155473\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.601298723884058\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5889386277944176\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5796022308596546\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5822799527845874\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.56272223801417\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5854486398165832\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5808682756581427\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5827582597501723\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5535750813644404\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.572337797008943\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5735178384384448\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5698143027712109\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5656237429494606\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5596775663000286\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5655901268228631\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5740998378561718\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5540491502158271\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.548723302322798\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5532783060524739\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5738926969505765\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5830700047352392\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5517609893084078\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5630089816759264\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5564440097439374\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5345463173539271\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5374123152780149\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5381757528583475\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5396269008407358\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5374235236604552\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5460964322175761\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5568101932267759\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5959540927171507\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.6176103650443239\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5826436099244616\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5453744888956631\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.550651177829026\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5464698169663547\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5444283412048232\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5482206991569966\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5435506921126705\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5490822784120366\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5454628632697012\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5329869823053343\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.413802071358691\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.473487695884183\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.433370708754831\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.254150467296115\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.357028747040154\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.448776186716234\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.6650940923737085\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.171198075124503\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.783363718429412\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.218592742969581\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.656435994734527\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.6932862766527865\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.875164765151389\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.104266969358638\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.987342538896562\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -15.465943370667508\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.824023225105665\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.0044194680773828\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8207575965518729\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.7595204675825852\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.731932418625953\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.7298478307969435\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6581605920500551\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.3195981530445826\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6645231053159121\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6479378848184197\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6097655670197298\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5917628913659763\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5899874617120636\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5953190156607114\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6022245749498769\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6070521766578431\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.6226549535506208\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.616918696671344\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.629840585471096\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -3.707440169864463\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6109116235036622\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6264495523715121\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.6191088671177929\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.6212204833397007\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5802723173215647\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.6198571734829362\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.6320903378126345\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5893380594834045\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5639242144404654\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5499713151899706\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5326945836852661\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5713178916192422\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.568572018065441\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5550721321734171\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5465801800742606\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5620860780346674\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5587682169285502\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5305526817273066\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5332794798139037\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.578846237360745\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5463803684323437\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5781903529643229\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5558420786508654\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5612838585180887\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5607517382505742\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5790781438575943\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5605727162910604\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5400563248493374\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5436959582907211\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.528560302311526\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5200962661137359\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5432534629817739\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5373292491629013\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5272851422271585\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5592664264715387\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5517785759305986\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5624448339100874\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5618474727170198\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5531839916806431\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5648316464529429\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5240003497498877\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.566082660097145\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5549280216931071\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5631334988029414\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5521274712633023\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5468423917663219\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5382191455451426\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5288256697011622\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5093319281057765\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.532600186501758\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5200892758806979\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5427123381736546\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5498353788052208\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6861811482110611\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5291285222848497\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5531254674217341\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5306239189332568\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5301217138701589\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5246047982460382\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5648746793298186\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5479785670727898\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5535784971865291\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5448117498321929\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5521914603688545\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5470114064363358\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5377893715430764\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.552416747199842\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5536502655612042\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5385425222620145\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5413038480878444\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.521665064225196\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5395709237963129\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5461504494316073\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5317533476289674\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5400235228591416\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5277165005884144\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5462936504107441\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5293914860343436\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5977675661576535\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.832053903277868\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.54780410270072\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5606000082802471\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5764370624742327\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5812800978668191\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5508818196210664\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5621686225515016\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5310613085182627\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5536775394670905\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5449328620502825\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5553802940277552\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5375610484056282\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5332529445358943\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5320022122231469\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.523517876481478\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5339133577083358\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5490642758438888\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5237910554418952\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.547223331582352\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5415043355661892\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5324285389871103\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5378345007380971\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5257751674668385\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5163321074429025\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5187988935876326\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5254873412854446\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5119835377870845\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5157486317745219\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5098066750651563\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5239339513727594\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5249384932303309\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5321322563299296\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5341740347077863\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5433188934504283\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5335362255833579\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5387743466969573\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5497478381167132\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5547287178457899\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5552574512608749\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5478598003147161\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5333403461763776\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5305405509282918\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5369667217278655\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5356272988909292\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5359270603700128\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5311999720043542\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5320644414136464\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5408952062223296\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5352979386705806\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5349374831751302\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5480860885915362\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5438949205007682\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5484736520307915\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5486882905979877\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5382534673682698\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5329526336454369\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5734371193453388\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5540180244798497\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5320300529938002\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5328958883289904\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5848968786452178\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5565929531739375\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5224434848463564\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5319178050924971\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5524948739833185\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5411553964268034\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5634737621199095\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5763793761224691\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5977236739012696\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5327891452255804\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5234547568563735\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5299993839866473\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.540849862010832\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5445761244873311\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5521984173472005\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5941783020554741\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5467866947897153\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5447826995496207\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5665062974910132\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5765592396446899\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.6443581556620401\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6517818673849068\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5733847543344814\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.617580331609539\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6119248034978605\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.13028776407705\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.285304100163782\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.528747827818384\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.011526764806185\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.141213463755819\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.724055905625148\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.176510579201411\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.544072067975511\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.122920507303011\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.422898728195091\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.846290624643051\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.141185043186047\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.106335691539008\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.735515081446067\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.485012473128024\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -11.217085699008845\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -5.252975283816778\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.2018296203374856\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.0172280724892127\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8638018166405055\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8763382490108618\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6944164049209468\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6818673801343185\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.7114802696095205\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6456095973913131\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6802705778741511\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6682273804095985\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7259968948477862\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6942063786874766\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.7029433322648267\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6292956865127683\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6256709698915128\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.6227299029589399\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.629587802546715\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6426484430884477\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6319086729786221\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6764244398475305\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6465996399619569\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.629056542199432\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.6279451431449129\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.6185516030539324\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.6758285927293324\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.6014841989646312\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.6102876480063975\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.6116228491383233\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.6395494510589391\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.6295608895419292\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.640684560698447\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.605452149796626\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.609109656084178\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.6111005920355901\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6455746890473884\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6140871044507116\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.6235134006811844\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6353628491574681\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.6113398322485815\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5972823422847315\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.6107504372546799\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.604222067453729\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5855251592685955\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5910922532538072\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5701047750274472\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5589520724478703\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5511890937210961\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5531554365774434\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5559368178027994\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5430313149692271\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5496792258105948\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5470772459027946\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5463406014737742\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5401471960174524\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5456782377889708\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5379803574351715\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5396180668516689\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5335608571925121\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.530697085805896\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5422569022118394\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5398360079124932\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5398384433415996\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5328400977311905\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5311073599275639\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5224337926090094\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.54132183783964\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5473318419577726\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5489796165024821\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5648453124940213\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.6066317231136902\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5906298373738065\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5860546242596125\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5520024614279803\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5538694455160507\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5643281061081551\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5631784725652635\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5513848057641038\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.561478018219279\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5848582678574581\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.6029603355649538\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5960136660588012\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.6062347405556312\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.6159388095311276\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.6290887751135794\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.6245144846486221\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.6274591606684365\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.6246985229816774\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.622783366979194\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.671587416133328\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.610009800560167\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6363953270266022\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6085661979202274\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6118124633736315\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6591493486614945\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6436094946525425\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6448298856761242\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6560964207566984\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.6043980180099876\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6191938055921797\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.6628046798662933\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.7051644306653302\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.829044391784145\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5885231680535439\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5580347729493621\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5402156423724506\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5528730847068251\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5587418127572338\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5584964948335382\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5406377678218555\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5382619077518572\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5374920363329929\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5382061736581614\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.547016079615204\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5430451142330753\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5380225012399106\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5423834535937173\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5278865208883627\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5310224351539248\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.539544576072831\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5439520272330113\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5546978262534569\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5412431655860322\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5499487437848355\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5465751846256881\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.585868432110823\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5511969679851884\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5473678292874865\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5411260924288988\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5448811069439932\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.528795569295748\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5444874863165755\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5458280537064941\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5721314151058468\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5596076815965799\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5608274086315742\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5601802635888978\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5715550447090678\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5600717026525615\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6070155550041231\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.6289215143388373\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.6242741459185123\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.6324310227991985\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.6161872090024415\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.6218789381577247\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.6040811864034008\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.6306017638977043\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.6281136164419052\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.6261208780304302\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.6305959410167439\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.6155091995805653\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.6407655897971712\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.7087366050596158\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.6391600644607738\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.6115379631440847\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.6860748061727776\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.8425623352143364\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.9614034532678608\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.9605965319582608\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.0810504418325366\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.9381583470719772\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.869199807754971\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.7286049291271297\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.6518130893525007\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5371235408995261\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5407539217938432\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5417941275177562\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5626876441721664\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.581316268395196\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.581922937269898\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5966717227586187\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.6032297798004409\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.6112384461330717\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.6780530558284446\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.6179864469488195\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6204062953720065\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.6183345431611706\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.6015473399382568\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.6077718032658381\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5998379811288099\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6110520648503567\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5761024243403672\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5519547833987289\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5423652402162379\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.518118391002873\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.357662317864176\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.679768343241481\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.556526682095555\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.900344507549539\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.298641264016428\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.624730423723066\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.21005337765047\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.078349121901299\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.361610377492997\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.7720632530516305\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.120974825034743\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.081544228670827\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.9813127389589456\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.872922857963064\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -9.460918372358458\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.0199172776056513\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -4.5824240860827805\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8437493952243454\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.719487134602862\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.665761178583538\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6090322635265315\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6138110906931928\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.322209148825013\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6143065941891339\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6061504068414785\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.5949895935492671\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.581915462405162\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5816707781938263\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5704984393717691\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5582187704121554\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.577710399815811\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5717393867205739\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5777206380784028\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5761778762995234\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5870895613471905\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5858071695028437\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5543025191162101\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5722916419466534\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5460765401159376\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.549213609633156\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5663349734306313\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5306393995930744\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5360471002983062\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5544540004399876\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5461909004111116\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5492024003418123\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5477947737500399\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5276699274430363\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.533583943225976\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.536434333194072\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5448594123185095\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.551492976588394\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5467594036392736\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5388014041749598\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5376598200615449\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5467705888710744\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.548002987655568\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5669796034604591\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.556160214575838\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5640173816575492\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5592283029144047\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5549286077043112\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5437824210159112\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5316996617172645\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.551518891846969\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5553648842866155\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5340239229015057\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5683109671626931\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.525598592761962\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5348600849783258\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5218216420211328\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.538851142217829\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5602448866838159\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.566822258190262\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5894840539038649\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5676847096490961\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5276872812268025\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5233702083433694\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5453022702119559\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5318443184896476\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5184340044226233\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5280250410232745\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5334245120619348\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5390467159238017\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5454304133072363\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5616110361201052\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5552373901846233\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5579679168484835\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5575078203038359\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5529062237616297\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.555032578341479\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5584363465410839\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5303541075010499\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5489909719966763\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5465096222981509\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5429248768830832\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5372055313398776\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5364406237637701\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5420287172936545\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5226900551250973\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5270745211929463\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5552512218547528\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5318192781287381\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5300271374221688\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.541295287280215\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5501889877449329\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5578694072406846\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5720281502735793\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.726925307250554\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6774661512221257\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6046387445811932\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5885292828241314\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6382480918605882\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.6139855911020105\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6110090114128501\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.6046362450032773\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.6153573145501207\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.6088812119651332\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.6541856414094979\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.627170379456821\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.6095935662897154\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5919397643503834\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.6089111947929169\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.6255400556035666\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.6189282647930608\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.6258498706533903\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.6268773229290866\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.6256509817298023\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.6217477329827199\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.6208031111712994\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.623936218229899\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.6155399150064241\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.6139773058339061\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.6171762549924038\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.6674776769901517\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.6828735483716186\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.7477469999945627\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6297870786496379\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.6154816039587349\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6747111227136645\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.804743714634838\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.6747692385774975\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.0601785460162807\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.6015249492426887\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.6012652294106217\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6383970089759403\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5970703744791057\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5814779400653025\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5908644416082056\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5888703394896079\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5919623167452923\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5754933965846457\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5723465292070504\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5789637777086974\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6204328644007469\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5701069110744568\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5970874464036237\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5502761154215515\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5591446776693323\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5451768041863856\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5521055924819704\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5701680345841582\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5699715859687398\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.0211483543447093\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.2603813749408046\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.0923794483572367\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.6427544389746704\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.6109515444267264\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5849800577696883\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5918575006898458\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5998056105786808\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6118962756064018\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.621395787743824\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.6084895517284384\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.579309511147319\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.6360363967812167\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5869288152180342\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5729206119975957\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5542209487904577\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5394982293145343\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5570332822349969\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.565683532610812\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5672137341578232\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5538766069878788\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5715174833081785\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5829036097009634\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5692333722675675\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5696896694246124\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5616235388442142\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5736756888174195\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.572131856852182\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5391738556196344\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5551453934610262\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5400760627889302\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5494927712627141\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.547253291932385\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5290553475688994\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5402186734698077\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5220791562990583\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.707264457449075\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.557549223893394\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.607466892273907\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -8.222822801275065\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.320423215496827\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.357068410165811\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.614403254920265\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.716172465241542\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.5605800784339205\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.04527463342868\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.503997993727758\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.944500667354834\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.806432436876072\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.8826027358966595\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.641029766940599\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -9.991968945055227\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.2570325244743363\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.2134354595488386\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.1631549556861658\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.9180774328003568\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8749771013182874\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8637053362618636\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.8308820051457902\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.7695590993687185\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7124454113044174\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6738179587649056\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6409379503206739\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6667796347549512\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5920843251068888\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.589126716190358\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5709902044411648\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.5795476484038329\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5692080579647885\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5827745229721446\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5597369475254677\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.580061573206583\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6049188920613052\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5798882191278443\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.621235402722919\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.7396909551879608\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.8419794316178306\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.9307322553861782\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.742603425426671\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.6430797653816281\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.6686340246259878\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.6441019044334101\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.6489529710539992\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5869155005815441\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.6058542528705356\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6084481605213727\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.7721035610284909\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.7890728382493757\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.8178304098853496\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.618458128978464\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5947825012338868\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.7220575366882901\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.6524721651029936\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5850476042384563\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.59247707728285\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5808870391567674\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5658327881626181\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5768032621124778\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5663200263103358\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5619772634764102\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5564110490430505\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5559049783114466\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.545783452191607\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5568427185596293\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5357488715541595\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.554373020350443\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5436753780417773\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5493324269667529\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5400376848143991\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5706786819520883\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5598203105560343\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5517226866740961\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5604532525222798\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5531531623476198\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5518325406845254\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5520402647231297\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5481911461779416\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5403323313272689\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5495408813564143\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5621557366210765\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5633110297770759\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5965694057680191\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5611677474123934\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5725487113241041\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.564036529478431\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5724110667848966\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5561377604457823\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5740486208407208\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5798034403307556\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5885813588977057\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5733937317479001\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.576749594128588\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5739281354308198\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5724704088628149\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.566884474848583\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5794051388317134\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5912470505729882\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.6090227674327425\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.6192263917354736\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.6324173699045046\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.6397941558886742\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.6076191548054747\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5931424995238693\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.723186840244677\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5592269306580882\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5478757725387122\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5511052161598807\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5523015470478566\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5595711973481063\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6095700653964741\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5593120220043053\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5504549862411615\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5525387342588403\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5560674337390675\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5594016260698982\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5710993726582142\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5803861687966712\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5811348866841741\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5738908687580507\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5762513556817798\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.6056132845778426\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5657143723270296\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -22.46011803415281\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5925998177770394\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5729905501051473\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5527970351632661\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5792442258997026\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.6133216226634055\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5710509698976192\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.6005149091442223\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5758456606382282\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5472959391553293\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5455669896936067\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5383110174350463\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5487166604042109\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5480834965542047\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6094003328408859\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5473259562973223\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.6122232681354108\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5264711049617233\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5633085485127984\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5416609731092773\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5449012386983599\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5281693552083655\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5316262715370033\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5569544969849435\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5463293741625133\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5555819854993344\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5514294416778501\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5382756428832063\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5728970088097616\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.556998993185949\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5684409194896493\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.553493840904029\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5814520925224507\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5564320394088391\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5541427405200632\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5567315151913764\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5513960786698405\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5589692078202417\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5688195391195078\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5815111763441638\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.6056928802086982\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.6505623882107201\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.6722635675782952\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.6311826425744159\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.6618548477879415\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.8169954003796189\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6140887831596245\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.6137169819985284\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.715304779433015\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -2.8291486896122366\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.7309570928408374\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.6430710928990354\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.6917893614254536\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.6037191229766526\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5945457871606566\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.594715265466347\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5873357683714066\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.6190342112887767\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.6072052292177508\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5850773139277543\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5652402688432542\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5616343833645961\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5803175551026415\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5591126329552203\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.576236258404156\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5770558743424502\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5878106710378215\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5674477112444452\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5749065420240691\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5816282823143877\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6007642357553246\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -7.4217666754817575\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.7060374804439256\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.8449744069722085\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.171514027848559\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.806663575652838\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.488339617114743\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.269799944572908\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.784392885487475\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.033888954773005\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.427576148335395\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.891344631892487\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.858316489499285\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.442431470226873\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.709047535897436\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.8325835095655725\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.03343731661401\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.708279549902568\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -7.1450750125780065\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -7.8228248350708505\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.132816320124196\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.1850652596660882\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9561177058337\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8236943229069957\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7191312772787214\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6422960044194115\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6297272371869955\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.5999032766629783\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.5950444579484625\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.5906263477057407\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.607782206532356\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6289355184017666\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6661908035967502\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6758859980395594\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6684701474698767\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.7877563958191339\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.6468823666982142\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.7124084465155768\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6633240578612642\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6056674038720531\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5864743961352292\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5832284777736628\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5792389792345617\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.6073507450815012\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.625457941128008\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.62077305274691\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.6200305062554057\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5971539922216963\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5847628990998196\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5912908958862292\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5677254238615363\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5769433952891112\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5885764307190297\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6559945676525075\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5863429218994186\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6014558347957419\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.610515867410047\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.6294192789036397\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5874706375991442\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.6045080138369593\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5325245568604354\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5559572926916293\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5483531550096521\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5499955409571406\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5600198036550086\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.563024395212586\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5635164649509988\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5822881368872401\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5515006919107632\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5643615108091717\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5436685407051294\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5572905364669326\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5540253287629621\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5402250313315006\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6186014618695596\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6516372196178952\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.9037150961065101\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.6306284428120669\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.63451293064443\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.635626473634403\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6489177336806199\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.6438622296233271\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.7281610825658126\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.9901938836951026\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.1252360346816368\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.8453679527206127\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.8678340014447936\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.1065657209430169\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.7458415866060123\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6999598772122857\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.7116802728090746\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.6017708162150273\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.584384140694922\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5894162765488205\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5770039929386017\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5705898607260274\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5844976728902431\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5868529654300271\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5690013366824194\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5582083212793963\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5744482315467127\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5604290565542186\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5607785943993991\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5645160731672805\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5653399031684956\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5682673982102863\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.6295357453142255\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.0421050446592108\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.6953306850608427\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.7981080402769892\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.9067144870448505\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.9755004089587247\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.9893531302667591\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.0227956265870817\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.0199584130148913\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.060521497399317\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.09214207811681\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.3742098201883173\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.620696088647118\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -2.708997015826011\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -3.958695216450441\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -4.232338423556037\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -3.9901016750730425\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -4.224354395731285\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -3.8530319236917694\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -3.859867628944005\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -3.991566374149235\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -3.6404858504049593\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -3.6725769526186993\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -3.6267837654031125\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -2.290835129519115\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.4095972585620553\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.7417685327908555\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.6746940372107119\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.6739823048344169\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.6567448101654854\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.7071984806546822\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.7519323068295498\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.6910895177749468\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.867342053167027\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.6973425575255245\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.7335663506855988\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.7655350753754462\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.9958111442395763\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.7431918494993386\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.7133129769295873\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.7022556466734293\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.6927992615840556\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.671027372846909\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.7758938987448074\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6233200915969032\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.6169737078688966\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.6142184124741376\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5654440755318365\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5961390811197766\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.6004746479769055\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6139293019785693\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6078675017660734\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5784748806928828\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5649544009018282\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5769649211778696\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5608900722563699\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5597490684948808\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.585922569613529\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5669620467419683\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5683449035454241\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5779332831112953\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5708742494260948\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.544264395052939\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5611926816322393\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5683311964535949\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5770736597129441\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5779670537248615\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5551235346813079\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.571388366220332\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5718594447790424\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5572339963331977\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5518994666244957\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5506574564823608\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5452953039513088\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5485334506476248\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.540220421084782\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5446576699803373\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5459487995728413\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5498343568861188\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.558436270915987\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5581043189471614\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.554631079710136\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5422647894681183\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5466050823506517\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5616248726576817\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5803813882910203\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5874331964695146\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.6364967554333518\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.599812648041897\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5467731972593195\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.533533563501426\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5366790101407816\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5277118171829147\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5322948936208569\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5378017650476928\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5399234874843024\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5545465850840701\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5352460859044191\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -7.393300501153349\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.745987411503723\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.373684297980035\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.579958292163244\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -8.364111647435157\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.686323665899763\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.391851042044104\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.25629763423091\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.769661384223147\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.741506428205412\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.939441898733949\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.774920045557114\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.348657885953678\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.5534425168622406\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -7.055638759356539\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -14.074500478857559\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.6053407442295002\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.2897484848754157\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.531317606416453\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.828183673394123\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7257064457681582\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6952708685141132\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6738825019084578\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.5867761672807734\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.599572642612996\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6016407947319565\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6148386543801415\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6481509439969204\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.684234031131246\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6921626342185152\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6365543175169646\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6195326508010138\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5888258607202269\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5721632114133035\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5658231716950481\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5774647024405258\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6532486425292648\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6529348327211585\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.6300323299109398\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.6301526016128586\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.6193078569389769\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5782436431596407\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5767835840093575\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5679460807869496\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5616754431902875\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5726985301326772\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5651200554688898\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5405614057000399\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5471749726462672\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5416133941814851\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5553147600447249\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5449413670409813\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5286096486834534\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5220604095459699\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5202573479283241\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5332278712568244\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5181341546450551\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5136782290299164\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5254270259949034\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.537037155504693\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5209815448313131\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5276547766127835\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5323260259270656\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.515461443542673\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5275320101461533\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5310859841251259\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.512130298405525\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5437972583890319\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5332901107028226\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5273550844012235\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5278842793029276\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5274358108759905\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5273347102864497\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5241814721632264\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.55087603335356\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5482079384298938\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5599989456889879\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.549180628369297\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5550425957707089\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.532072895155251\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5291252299530089\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5335013606701823\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5675027514541815\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5353443071706664\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5438662070005936\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5724412752178821\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5922288464539772\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.566182719445325\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5587427537671904\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5505370189852661\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5764348158026028\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5401789218129252\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5213289255094407\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.555934253991259\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5413231066109723\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5398938392215169\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5395279140995334\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5349820472088624\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.548504185153423\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5346369394321324\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.534003069323087\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5360514191218501\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5308817310402268\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5795018977542544\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5716630547094668\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5508134544463174\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.56140305446827\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6111551625827133\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6629899520329298\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6184306901293672\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6296392967520235\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.591154859833525\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5731337747044996\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.56448494101116\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5871066680021351\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6069879443409095\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5879867321413476\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5728315042550417\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5286119132547445\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5436227181326497\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5506864536439587\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.55342202857925\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5877575952761807\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5731427918352011\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5558021594441233\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5444974619550391\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5465831291295861\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5442099727441709\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5783680052035365\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5533419169780223\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5782828278240402\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5436877438645261\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5506853052929432\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5407326447635481\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.546003232819248\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5586027737169759\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5277751423954713\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5558127713168968\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5620787750291476\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5405384198394193\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5702651007128313\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5713151424363198\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5579148885175522\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5571961037408597\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5496962130289573\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5564318288524246\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5260079135083966\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5346684370787458\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5275497271510704\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5306679042648568\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5285857010059953\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5305418169798822\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5443161960271206\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5630542726615034\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5548346432754631\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5667148178476463\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.54362897038161\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5327351052144338\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5753560748063496\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5476666854129357\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5751666035106906\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5532255675246592\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5570663545224737\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5586814015855639\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5599311979808284\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5724173582055717\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5376269008319834\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5473952037941199\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5378815618392285\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5429835220053387\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5353534051246833\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.539273575889343\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5337891285243609\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5378485379903113\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.533610862784133\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5443704678397393\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5316693643731584\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5402335852818263\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5483374004120125\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -2.0448176599847687\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5136784685147046\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5363518041497471\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.525935604476908\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5392273638309605\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5578241366431538\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5370793159188612\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5425941190671787\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.6008718452428611\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.7815613903250147\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.8020737818801998\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.7126888673133414\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6886345054518493\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5729490561001412\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5855850218733107\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5558736391805986\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5382944729933975\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5349245010060444\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5305687155545664\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5509008281000223\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5391550161911189\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.958215505213664\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.299521180918041\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.7877024441117815\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.952653517777012\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.607773870030026\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.879936648374683\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.3774190336993\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.280994729397844\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.921349837951184\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.930675633317451\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.930847073261622\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.8028383266762225\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.930208941003977\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.213868173966817\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -7.266633267146122\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -11.253429549334433\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.265078081856421\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -0.8491256679958261\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.7667783329040415\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8219095655063313\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7413448637608907\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8220464957646985\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6629178310138707\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.642213227757563\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7103294691429105\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6890811565782248\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.7182316085154699\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7161683905470592\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6556090944618841\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6570375756333218\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.633631892671041\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6525916026264579\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.6739478915240221\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.617395913936055\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6636616780588546\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6578828142894221\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.7095255196501352\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.8567865367037792\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.8069457223894608\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.8300308002215718\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.808039852913976\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7471137370321433\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.7474940818620845\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.567123559429637\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5468314115085515\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5827928232735123\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5756017146258856\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5501612486496166\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5525996011098061\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5529579185735158\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5727329933345956\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5960067377279715\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5895954547715685\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5895065271543296\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5900210958181222\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5960798027135914\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5923473620090656\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5895701719430497\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.6026848662726484\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5982857352140283\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5908709669667139\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.6012119855967997\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.607968063296378\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.6129712270006725\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.6157863609759384\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.6006182179474033\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5942621926157897\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5930933166830149\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5955349867482675\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5880539411732086\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.572496008684555\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5624553168505766\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5802487448855637\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5903309802210933\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.6070334444126921\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.6036680812780287\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5916038200907053\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.604692312532799\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.604507305291143\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6040346970255416\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5769995131280878\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5722875447099482\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5837430857704663\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5856118201189021\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5968022033812709\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6089439343986112\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5967768467068733\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.616750080432643\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.6263115583038009\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6174155774014314\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.6098062381608418\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.6004511746115526\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.623768844837815\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.6204997674245887\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5553277336537771\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.588584012020609\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.6412965637406545\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.6410507676906073\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5961553908122194\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.585262423319992\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5958904413716097\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5714387906564447\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5589614180024992\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5656561521104198\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5382873859812677\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5335319458770351\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5489726482931944\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5413133785755762\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5401177476733752\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5310518543877637\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5391243904513198\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.545065848536535\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5456364768199672\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5466082259571973\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5482043628899504\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5458609283996882\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5351713863328168\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5464157733101725\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5364935023075346\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5327149423505277\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5462320168142386\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5395234704299883\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5341664930528955\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5412349944463622\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5412875093747513\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5368217860748142\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.538659183102181\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5394927227579669\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5419147654735792\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5330551204040297\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.523787109817426\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5316894994691246\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5359103086636346\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5396327361952382\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5378490893372139\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5289836459638979\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5422944800983918\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5639784161644912\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5419845292732247\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.58301305598687\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5432992778078912\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5308491953843005\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5399413955357\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5218148710462225\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5297138627455789\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5297117722890995\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5694020847759563\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5128830074553598\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5388454673443785\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5303564758144077\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.535803260192816\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5234237730601606\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5321308836266642\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5221712410130688\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5572876960626815\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5881053611050003\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5380018770414258\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5281772533092405\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5413410363263107\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5308196312619582\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5076250738368489\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.524397244698847\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5151907300626375\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.529139438544877\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.519185058142751\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5212210517625884\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5178300198438212\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.532594375633201\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5252274346204837\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5098205893827956\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5168910836810546\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5274224024904135\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.522589249220051\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5274929784546627\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5272947051044156\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5346562241428137\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.6537247875476737\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5839032005534274\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.562173774287314\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.7036729291746778\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5973405107382321\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.6020034802378\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.587004516774889\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5937660681498866\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.615235991473085\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.6309964453093839\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.6506623985365216\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.6225621287507785\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.6327698379835425\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.6371133088004225\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.6649726295334952\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6672488962691696\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.6971450489481532\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.6901436882473868\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.7006076126786134\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.7423962122657686\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.7093761739456497\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.7155055352422157\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.7127113020171814\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6965154692086705\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99ad501fd4cec69c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
