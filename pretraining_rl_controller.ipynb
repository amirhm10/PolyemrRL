{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:27.673533Z",
     "start_time": "2025-07-15T14:50:25.208995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from TD3Agent.agent import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from BasicFunctions.td3_functions import *\n",
    "from TD3Agent.replay_buffer import ReplayDataset, DataLoader"
   ],
   "id": "7d5344ace6d0a16e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:27.997358Z",
     "start_time": "2025-07-15T14:50:27.976716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from TD3Agent.actor import Actor\n",
    "from TD3Agent.critic import Critic\n",
    "from TD3Agent.replay_buffer import ReplayBuffer\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 actor_layer_sizes: list,\n",
    "                 critic_layer_sizes: list,\n",
    "                 buffer_capacity: int,\n",
    "                 actor_lr: float,\n",
    "                 critic_lr: float,\n",
    "                 target_policy_smoothing_noise_std: float,\n",
    "                 noise_clip: float,\n",
    "                 exploration_noise_std: float,\n",
    "                 gamma: float,\n",
    "                 tau: float,\n",
    "                 max_action: float,\n",
    "                 policy_delay: int,\n",
    "                 device):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.actor_layer_sizes = actor_layer_sizes\n",
    "        self.critic_layer_sizes = critic_layer_sizes\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.t_std = target_policy_smoothing_noise_std\n",
    "        self.noise_clip = noise_clip\n",
    "        self.exploration_noise_std = exploration_noise_std\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.policy_delay = policy_delay\n",
    "        self.total_it = 0\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "        self.actor_target = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "        self.critic_target = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_capacity, self.state_dim, self.action_dim)\n",
    "        self.loss_fn_actor = nn.MSELoss()\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Let's implement Huber loss\n",
    "        self.loss_fn_critic = nn.MSELoss()\n",
    "\n",
    "        # Defining the lists for losses storage for later visualization\n",
    "        self.critic_losses_pretrain = []\n",
    "        self.actor_losses_pretrain = []\n",
    "        self.critic_losses = []\n",
    "        self.actor_losses = []\n",
    "\n",
    "        self.scheduler_actor = StepLR(self.actor_optimizer, step_size=150, gamma=0.1)\n",
    "        self.scheduler_critic = StepLR(self.critic_optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "        self.buffer_save = False\n",
    "\n",
    "        self.exploration_noise_std_min = exploration_noise_std * 0.1\n",
    "        self.exploration_noise_std_initial = exploration_noise_std\n",
    "        self.decay_rate = 0.99995\n",
    "\n",
    "        self.warm_start = True\n",
    "\n",
    "    def pre_train_entire_data(self, path, data_loader, epochs):\n",
    "\n",
    "        # scaler = GradScaler('cuda')  # Initialize GradScaler for mixed precision training\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "        save_path = os.path.join(path, \"PLots\", timestamp)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss_critic_epoch = 0.0\n",
    "            loss_actor_epoch = 0.0\n",
    "\n",
    "            for batch_idx, (states_batch, actions_batch, rewards_batch, next_states_batch) in enumerate(data_loader):\n",
    "                # Move data to GPU\n",
    "                states_batch = states_batch.to(self.device)\n",
    "                actions_batch = actions_batch.to(self.device)\n",
    "                rewards_batch = rewards_batch.to(self.device)\n",
    "                next_states_batch = next_states_batch.to(self.device)\n",
    "                batch_size = states_batch.size(0)\n",
    "\n",
    "                # Compute target Q-value without tracking gradients\n",
    "                with torch.no_grad():\n",
    "                    noise = (self.t_std * torch.randn(batch_size, self.action_dim, device=self.device)).clamp(\n",
    "                        -self.noise_clip, self.noise_clip)\n",
    "                    next_actions = (self.actor_target(next_states_batch) + noise).clamp(-self.max_action,\n",
    "                                                                                        self.max_action)\n",
    "                    target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "                    target_Q = torch.min(target_Q1, target_Q2)\n",
    "                    target_value = rewards_batch + self.gamma * target_Q\n",
    "\n",
    "                # --- Critic Update with Mixed Precision ---\n",
    "                # with autocast('cuda'):\n",
    "                current_Q1, current_Q2 = self.critic(states_batch, actions_batch)\n",
    "                critic_loss = self.loss_fn(current_Q1, target_value) + self.loss_fn(current_Q2, target_value)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                loss_critic_epoch += critic_loss.item() * batch_size\n",
    "\n",
    "                # --- Actor Update with Mixed Precision ---\n",
    "                # with autocast('cuda'):\n",
    "                actor_output = self.actor(states_batch)\n",
    "                actor_loss = self.loss_fn(actions_batch, actor_output)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                loss_actor_epoch += actor_loss.item() * batch_size\n",
    "\n",
    "            # Average losses over the epoch\n",
    "            num_samples = len(data_loader.dataset)\n",
    "            loss_critic_epoch /= num_samples\n",
    "            loss_actor_epoch /= num_samples\n",
    "\n",
    "            self.critic_losses_pretrain.append(loss_critic_epoch)\n",
    "            self.actor_losses_pretrain.append(loss_actor_epoch)\n",
    "\n",
    "        # \"\"\"\n",
    "        # Pre-trains actor-critic networks on an offline replay buffer\n",
    "        # without mixed-precision.  All computations are carried out\n",
    "        # in default FP32.\n",
    "        # \"\"\"\n",
    "        # # ------------------------------------------------------------------\n",
    "        # # 1.  House-keeping: create a time-stamped directory for plots\n",
    "        # # ------------------------------------------------------------------\n",
    "        # timestamp  = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "        # save_path  = os.path.join(path, \"PLots\", timestamp)\n",
    "        # os.makedirs(save_path, exist_ok=True)\n",
    "        #\n",
    "        # # ------------------------------------------------------------------\n",
    "        # # 2.  Epoch loop\n",
    "        # # ------------------------------------------------------------------\n",
    "        # for epoch in range(epochs):\n",
    "        #     loss_critic_epoch = 0.0\n",
    "        #     loss_actor_epoch  = 0.0\n",
    "        #\n",
    "        #     for batch_idx, (states, actions, rewards, next_states) in enumerate(data_loader):\n",
    "        #         # -------- move mini-batch to the same device as the networks ---\n",
    "        #         states      = states.to(self.device)\n",
    "        #         actions     = actions.to(self.device)\n",
    "        #         rewards     = rewards.to(self.device)\n",
    "        #         next_states = next_states.to(self.device)\n",
    "        #         B           = states.size(0)                       # batch size\n",
    "        #\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         # 2-A.  Compute TD target (no gradients needed)\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         with torch.no_grad():\n",
    "        #             noise = (self.t_std\n",
    "        #                      * torch.randn(B, self.action_dim, device=self.device)\n",
    "        #                     ).clamp(-self.noise_clip, self.noise_clip)\n",
    "        #\n",
    "        #             next_actions       = (self.actor_target(next_states) + noise).clamp(-self.max_action,\n",
    "        #                                                                                  self.max_action)\n",
    "        #             target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n",
    "        #             target_Q   = torch.min(target_Q1, target_Q2)\n",
    "        #             target_val = rewards + self.gamma * target_Q          # shape: [B, 1]\n",
    "        #\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         # 2-B.  Critic update (FP32)\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         current_Q1, current_Q2 = self.critic(states, actions)\n",
    "        #         critic_loss = (self.loss_fn_critic(current_Q1, target_val)\n",
    "        #                        + self.loss_fn_critic(current_Q2, target_val))\n",
    "        #\n",
    "        #         self.critic_optimizer.zero_grad()\n",
    "        #         critic_loss.backward()\n",
    "        #         self.critic_optimizer.step()\n",
    "        #\n",
    "        #         loss_critic_epoch += critic_loss.item() * B\n",
    "        #\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         # 2-C.  Actor update (FP32)\n",
    "        #         # ------------------------------------------------------------------\n",
    "        #         predicted_actions = self.actor(states)             # Ï€(s)\n",
    "        #         actor_loss = self.loss_fn_actor(predicted_actions, actions)\n",
    "        #\n",
    "        #         self.actor_optimizer.zero_grad()\n",
    "        #         actor_loss.backward()\n",
    "        #         self.actor_optimizer.step()\n",
    "        #\n",
    "        #         loss_actor_epoch += actor_loss.item() * B\n",
    "        #\n",
    "        #     # ------------------------------------------------------------------\n",
    "        #     # 3.  Book-keeping: store average epoch losses\n",
    "        #     # ------------------------------------------------------------------\n",
    "        #     N = len(data_loader.dataset)       # total samples in the loader\n",
    "        #     self.critic_losses_pretrain.append(loss_critic_epoch / N)\n",
    "        #     self.actor_losses_pretrain.append(loss_actor_epoch  / N)\n",
    "\n",
    "            if epoch == 0 or epoch % 10 == 9:\n",
    "                print(f\"{datetime.datetime.now()} Epoch {epoch + 1},\"\n",
    "                      f\" Actor Loss: {loss_actor_epoch},\"\n",
    "                      f\" Critic Loss: {loss_critic_epoch}\")\n",
    "                if epoch == 0 or epoch % 100 == 99:\n",
    "                    current_lr_cr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch + 1}, Learning rate (Critic): {current_lr_cr}\")\n",
    "                    current_lr_ac = self.actor_optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch + 1}, Learning rate (Actor): {current_lr_ac}\")\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                if epoch > 1:\n",
    "                    # Exclude the first `plot_start` losses to get a clearer plot of later epochs\n",
    "                    if epoch < 110:\n",
    "                        plot_start = 0\n",
    "                    else:\n",
    "                        plot_start = epoch - 100\n",
    "                    critic_losses_plot = self.critic_losses_pretrain[plot_start:]\n",
    "                    actor_losses_plot = self.actor_losses_pretrain[plot_start:]\n",
    "\n",
    "                    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "                    # Plot Critic Loss\n",
    "                    axs[0].plot(range(plot_start + 1, len(critic_losses_plot) + 1 + plot_start), critic_losses_plot,\n",
    "                                label=\"Critic Loss\", color='r', linewidth=2)\n",
    "                    axs[0].set_title(\"Critic Loss (Pre-train)\", fontsize=16)\n",
    "                    axs[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "                    axs[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "                    axs[0].grid(True)\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    # Plot Actor Loss\n",
    "                    axs[1].plot(range(plot_start + 1, len(actor_losses_plot) + 1 + plot_start), actor_losses_plot,\n",
    "                                label=\"Actor Loss\", color='b', linewidth=2)\n",
    "                    axs[1].set_title(\"Actor Loss (Pre-train)\", fontsize=16)\n",
    "                    axs[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "                    axs[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "                    axs[1].grid(True)\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    # Show the plot\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    filename = os.path.join(save_path, f\"{plot_start}_{epoch}.png\")\n",
    "                    plt.savefig(filename)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            # Update the learning rate schedulers at the end of the epoch\n",
    "            # self.scheduler_actor.step()\n",
    "            # self.scheduler_critic.step()\n",
    "\n",
    "            # # Clear CUDA cache and collect garbage to free up memory (helpful with large models/datasets)\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "\n",
    "    def pre_train(self, path: str, batch_size: int, epochs: int = 1000, log_interval: int = 1000):\n",
    "        \"\"\"\n",
    "        Pre-train on stored replay buffer samples using TD3-style updates\n",
    "        with mixed-precision (GradScaler + autocast).\n",
    "\n",
    "        Args:\n",
    "            path:            path to store plots\n",
    "            batch_size:      number of samples per gradient update\n",
    "            epochs:          total number of gradient-update iterations\n",
    "            log_interval:    print losses every `log_interval` steps\n",
    "        \"\"\"\n",
    "\n",
    "        # time to store the losses of critic and actor\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "\n",
    "        path = os.path.join(path, \"PLots_pretrain_normal\", timestamp)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        for it in range(1, epochs + 1):\n",
    "            # 1) Sample a random minibatch\n",
    "            states, actions, rewards, next_states = self.replay_buffer.sample_pretrain(batch_size, device=str(self.device))\n",
    "\n",
    "            # 2) Compute target Q-values (no grad)\n",
    "            with torch.no_grad():\n",
    "                noise = (self.t_std * np.random.randn(batch_size,\n",
    "                                                      self.action_dim)).clip(\n",
    "                    -self.noise_clip, self.noise_clip)\n",
    "                noise = torch.from_numpy(noise).to(dtype=torch.float32, device=self.device)\n",
    "                next_actions = (self.actor_target(next_states) + noise).clip(-1, 1)\n",
    "\n",
    "                target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n",
    "                target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "                y = rewards + self.gamma * target_Q\n",
    "\n",
    "            # 3) Critic update (mixed precision)\n",
    "            q1, q2 = self.critic(states, actions)\n",
    "\n",
    "            loss_critic = self.loss_fn_critic(q1, y) + self.loss_fn_critic(q2, y)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            self.critic_losses.append(loss_critic.item())\n",
    "\n",
    "            # 4) Delayed actor update\n",
    "            if it % self.policy_delay == 0:\n",
    "                actor_actions = self.actor(states)\n",
    "                actor_loss = self.loss_fn_actor(actor_actions, actions)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.actor_losses.append(actor_loss.item())\n",
    "\n",
    "                # 5) Soft update of targets\n",
    "                for p, p_tgt in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    p_tgt.data.mul_(1 - self.tau)\n",
    "                    p_tgt.data.add_(self.tau * p.data)\n",
    "                for p, p_tgt in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    p_tgt.data.mul_(1 - self.tau)\n",
    "                    p_tgt.data.add_(self.tau * p.data)\n",
    "\n",
    "                # store the actor loss\n",
    "                self.actor_losses_pretrain.append(actor_loss.item())\n",
    "\n",
    "            # store the critic loss\n",
    "            self.critic_losses_pretrain.append(loss_critic.item())\n",
    "\n",
    "            # 6) Logging\n",
    "            if it % log_interval == 0 or it == 1:\n",
    "                print(f\"[Pre-train] Iter {it}/{epochs}: \"\n",
    "                      f\"critic_loss={loss_critic.item():.6e}, \"\n",
    "                      f\"actor_loss={(actor_loss.item() if 'actor_loss' in locals() else float('nan')):.6e}\")\n",
    "\n",
    "\n",
    "    def train(self, n_samples=100):\n",
    "\n",
    "        self.buffer_save = True\n",
    "\n",
    "        (states_batch, actions_batch,\n",
    "         rewards_batch, next_states_batch) = self.replay_buffer.sample(n_samples, device=self.device)\n",
    "        # (states_batch, actions_batch,\n",
    "        #  rewards_batch, next_states_batch) = self.replay_buffer.sample_pretrain(n_samples, device=self.device)\n",
    "\n",
    "        loss_critic = 0\n",
    "        loss_actor = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (self.t_std * np.random.randn(n_samples,\n",
    "                                                  self.action_dim)).clip(\n",
    "                -self.noise_clip, self.noise_clip)\n",
    "            noise = torch.from_numpy(noise).to(dtype=torch.float32, device=self.device)\n",
    "            next_actions = (self.actor_target(next_states_batch) + noise).clip(-1, 1)\n",
    "\n",
    "            target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "            r = rewards_batch + self.gamma * target_Q\n",
    "\n",
    "        q1, q2 = self.critic(states_batch, actions_batch)\n",
    "\n",
    "        loss = self.loss_fn(q1, r) + self.loss_fn(q2, r)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        loss_critic += loss.item()\n",
    "        self.critic_losses.append(loss.item())\n",
    "\n",
    "        # if self.total_it % self.policy_delay == 0:\n",
    "        actions = self.actor(states_batch)\n",
    "        q = self.critic.q1_forward(states_batch, actions)\n",
    "\n",
    "        actor_loss = - torch.mean(q)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        loss_actor += actor_loss.item()\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        # print(f'Critic loss: {self.critic_losses[-1]}')\n",
    "        # print(f'Actor loss: {self.actor_losses[-1]}')\n",
    "        self.total_it += 1\n",
    "\n",
    "        self.decay_exploration()\n",
    "\n",
    "    def decay_exploration(self):\n",
    "\n",
    "        self.exploration_noise_std = max(\n",
    "            self.exploration_noise_std_min,\n",
    "            self.exploration_noise_std_initial * (self.decay_rate ** self.total_it)\n",
    "        )\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        state = state if isinstance(state, torch.Tensor) else torch.from_numpy(state).to(dtype=torch.float32,\n",
    "                                                                                         device=self.device)\n",
    "\n",
    "        # If warm start is active, choose a random action from the action space.\n",
    "        if self.warm_start:\n",
    "            action = np.random.uniform(low=-self.max_action, high=self.max_action, size=self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.actor(state).detach().cpu().numpy()\n",
    "            if explore:\n",
    "                action += np.random.randn(self.action_dim) * self.exploration_noise_std\n",
    "            action = action.clip(-self.max_action, self.max_action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save(self, path: str, name_prefix=\"agent\"):\n",
    "        path = os.path.join(path, \"models\")\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "        filename = os.path.join(path, f\"{name_prefix}_{timestamp}.pkl\")\n",
    "\n",
    "        save_dict = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_target_state_dict': self.actor_target.state_dict(),\n",
    "            'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'agent_attributes': {key: value for key, value in self.__dict__.items() if key not in ['actor', 'critic',\n",
    "                                                                                                   'actor_target',\n",
    "                                                                                                   'critic_target',\n",
    "                                                                                                   'replay_buffer',\n",
    "                                                                                                   'total_it',\n",
    "                                                                                                   'device',\n",
    "                                                                                                   'loss_fn',\n",
    "                                                                                                   'replay_buffer']}\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "        print(f\"Agent saved successfully to {filename}\")\n",
    "        return filename\n",
    "\n",
    "    def load(self, path: str):\n",
    "        with open(path, 'rb') as f:\n",
    "            loaded_dict = pickle.load(f)\n",
    "\n",
    "        self.actor.load_state_dict(loaded_dict['actor_state_dict'])\n",
    "        self.critic.load_state_dict(loaded_dict['critic_state_dict'])\n",
    "        self.actor_target.load_state_dict(loaded_dict['actor_target_state_dict'])\n",
    "        self.critic_target.load_state_dict(loaded_dict['critic_target_state_dict'])\n",
    "\n",
    "        for key, value in loaded_dict['agent_attributes'].items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        self.actor_optimizer.load_state_dict(loaded_dict['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(loaded_dict['critic_optimizer_state_dict'])\n",
    "        self.actor_optimizer.param_groups[0]['params'] = list(self.actor.parameters())\n",
    "        self.critic_optimizer.param_groups[0]['params'] = list(self.critic.parameters())\n",
    "        self.actor_optimizer.param_groups[0]['lr'] = self.actor_lr # * 1e-3\n",
    "        self.critic_optimizer.param_groups[0]['lr'] = self.critic_lr\n",
    "\n",
    "        # self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr * 1e-2)\n",
    "        # self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        print(f\"Agent loaded successfully from: {path}\")\n"
   ],
   "id": "fc4c3108bf053d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "906ae4312088c426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.015196Z",
     "start_time": "2025-07-15T14:50:28.012917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "fd5e178b0c3baa88",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.027353Z",
     "start_time": "2025-07-15T14:50:28.025224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "79492d92ea8ebfb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.052619Z",
     "start_time": "2025-07-15T14:50:28.050562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "fefe97d84d165283",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.068980Z",
     "start_time": "2025-07-15T14:50:28.067184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "978dc0855d95a24c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.091604Z",
     "start_time": "2025-07-15T14:50:28.089333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "de0a1c82e0128367",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "40038af50eec57a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:50:28.108188Z",
     "start_time": "2025-07-15T14:50:28.105914Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "2dc0a1fc3545317",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:00.472271Z",
     "start_time": "2025-07-15T14:51:00.457145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[2.8, 320.],\n",
    "                       [5., 326.]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "3f87594c63734151",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:00.918960Z",
     "start_time": "2025-07-15T14:51:00.916429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "c3b76ba44b3d8640",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:01.295055Z",
     "start_time": "2025-07-15T14:51:01.293131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "11a0deb5073c3853",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:01.555423Z",
     "start_time": "2025-07-15T14:51:01.553506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "# min_max_states"
   ],
   "id": "b310edd0c5253181",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:01.955627Z",
     "start_time": "2025-07-15T14:51:01.953130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "49c3fda12d45a9d9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:02.316837Z",
     "start_time": "2025-07-15T14:51:02.314600Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "e6e0bf0ab38a021a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:02.728644Z",
     "start_time": "2025-07-15T14:51:02.726800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "753421957cb27b91",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:03.126691Z",
     "start_time": "2025-07-15T14:51:03.123733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "2f816e491df4c70",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:03.658653Z",
     "start_time": "2025-07-15T14:51:03.653634Z"
    }
   },
   "cell_type": "code",
   "source": "min_max_dict",
   "id": "dff5ecebf376a4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_max': array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
       "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
       "          6.2029936 ]),\n",
       " 'x_min': array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
       "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
       "          -28.25906582]),\n",
       " 'y_sp_min': array([-4.91766443, -4.61204935]),\n",
       " 'y_sp_max': array([5.00776949, 3.06512771]),\n",
       " 'u_max': array([9.96, 7.3 ]),\n",
       " 'u_min': array([-10. ,  -7.5])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "9b6fc672682c9d1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:04.780557Z",
     "start_time": "2025-07-15T14:51:04.750692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "# ACTOR_LAYER_SIZES = [1024, 1024, 1024, 1024, 1024]\n",
    "# CRITIC_LAYER_SIZES = [1024, 1024, 1024, 1024, 1024]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 30_000_000\n",
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "SMOOTHING_STD = 0.000001\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORATION_NOISE_STD = 0.1\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "BATCH_SIZE = 256\n",
    "NUM_BATCHES = 1"
   ],
   "id": "daf58940b5cb9814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:06.248400Z",
     "start_time": "2025-07-15T14:51:05.486103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent\n",
    "agent = Agent(\n",
    "    STATE_DIM,\n",
    "    ACTION_DIM,\n",
    "    ACTOR_LAYER_SIZES,\n",
    "    CRITIC_LAYER_SIZES,\n",
    "    BUFFER_CAPACITY,\n",
    "    ACTOR_LR,\n",
    "    CRITIC_LR,\n",
    "    SMOOTHING_STD,\n",
    "    NOISE_CLIP,\n",
    "    EXPLORATION_NOISE_STD,\n",
    "    GAMMA,\n",
    "    TAU,\n",
    "    MAX_ACTION,\n",
    "    POLICY_DELAY,\n",
    "    DEVICE\n",
    ")"
   ],
   "id": "dbc7a407cef272a0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:06.254023Z",
     "start_time": "2025-07-15T14:51:06.251571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1 = 5\n",
    "Q2 = 1\n",
    "R1 = 1\n",
    "R2 = 1\n",
    "Q_rew = np.array([[12, 0], [0, 8]])\n",
    "R_rew = np.array([[1, 0], [0, 1]])"
   ],
   "id": "c1c13a104e1edc77",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:06.263256Z",
     "start_time": "2025-07-15T14:51:06.261153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1, Q2, R1, R2,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "3020ae61ebbd3a4f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:51:06.681527Z",
     "start_time": "2025-07-15T14:51:06.679594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steady_states_samples_number = 100000\n",
    "mpc_pretrain_samples_numbers = BUFFER_CAPACITY - steady_states_samples_number"
   ],
   "id": "e103bf85c8ac0549",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T17:58:12.518229Z",
     "start_time": "2025-07-15T14:51:07.142892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filling_the_buffer(\n",
    "        min_max_dict,\n",
    "        A_aug, B_aug, C_aug,\n",
    "        MPC_obj,\n",
    "        mpc_pretrain_samples_numbers,\n",
    "        Q_rew, R_rew,\n",
    "        agent,\n",
    "        IC_opt, bnds, cons, steady_states[\"y_ss\"], data_min, data_max, chunk_size= 100_000)"
   ],
   "id": "422899593ce0dbde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/299\n",
      "Processing chunk 2/299\n",
      "Processing chunk 3/299\n",
      "Processing chunk 4/299\n",
      "Processing chunk 5/299\n",
      "Processing chunk 6/299\n",
      "Processing chunk 7/299\n",
      "Processing chunk 8/299\n",
      "Processing chunk 9/299\n",
      "Processing chunk 10/299\n",
      "Processing chunk 11/299\n",
      "Processing chunk 12/299\n",
      "Processing chunk 13/299\n",
      "Processing chunk 14/299\n",
      "Processing chunk 15/299\n",
      "Processing chunk 16/299\n",
      "Processing chunk 17/299\n",
      "Processing chunk 18/299\n",
      "Processing chunk 19/299\n",
      "Processing chunk 20/299\n",
      "Processing chunk 21/299\n",
      "Processing chunk 22/299\n",
      "Processing chunk 23/299\n",
      "Processing chunk 24/299\n",
      "Processing chunk 25/299\n",
      "Processing chunk 26/299\n",
      "Processing chunk 27/299\n",
      "Processing chunk 28/299\n",
      "Processing chunk 29/299\n",
      "Processing chunk 30/299\n",
      "Processing chunk 31/299\n",
      "Processing chunk 32/299\n",
      "Processing chunk 33/299\n",
      "Processing chunk 34/299\n",
      "Processing chunk 35/299\n",
      "Processing chunk 36/299\n",
      "Processing chunk 37/299\n",
      "Processing chunk 38/299\n",
      "Processing chunk 39/299\n",
      "Processing chunk 40/299\n",
      "Processing chunk 41/299\n",
      "Processing chunk 42/299\n",
      "Processing chunk 43/299\n",
      "Processing chunk 44/299\n",
      "Processing chunk 45/299\n",
      "Processing chunk 46/299\n",
      "Processing chunk 47/299\n",
      "Processing chunk 48/299\n",
      "Processing chunk 49/299\n",
      "Processing chunk 50/299\n",
      "Processing chunk 51/299\n",
      "Processing chunk 52/299\n",
      "Processing chunk 53/299\n",
      "Processing chunk 54/299\n",
      "Processing chunk 55/299\n",
      "Processing chunk 56/299\n",
      "Processing chunk 57/299\n",
      "Processing chunk 58/299\n",
      "Processing chunk 59/299\n",
      "Processing chunk 60/299\n",
      "Processing chunk 61/299\n",
      "Processing chunk 62/299\n",
      "Processing chunk 63/299\n",
      "Processing chunk 64/299\n",
      "Processing chunk 65/299\n",
      "Processing chunk 66/299\n",
      "Processing chunk 67/299\n",
      "Processing chunk 68/299\n",
      "Processing chunk 69/299\n",
      "Processing chunk 70/299\n",
      "Processing chunk 71/299\n",
      "Processing chunk 72/299\n",
      "Processing chunk 73/299\n",
      "Processing chunk 74/299\n",
      "Processing chunk 75/299\n",
      "Processing chunk 76/299\n",
      "Processing chunk 77/299\n",
      "Processing chunk 78/299\n",
      "Processing chunk 79/299\n",
      "Processing chunk 80/299\n",
      "Processing chunk 81/299\n",
      "Processing chunk 82/299\n",
      "Processing chunk 83/299\n",
      "Processing chunk 84/299\n",
      "Processing chunk 85/299\n",
      "Processing chunk 86/299\n",
      "Processing chunk 87/299\n",
      "Processing chunk 88/299\n",
      "Processing chunk 89/299\n",
      "Processing chunk 90/299\n",
      "Processing chunk 91/299\n",
      "Processing chunk 92/299\n",
      "Processing chunk 93/299\n",
      "Processing chunk 94/299\n",
      "Processing chunk 95/299\n",
      "Processing chunk 96/299\n",
      "Processing chunk 97/299\n",
      "Processing chunk 98/299\n",
      "Processing chunk 99/299\n",
      "Processing chunk 100/299\n",
      "Processing chunk 101/299\n",
      "Processing chunk 102/299\n",
      "Processing chunk 103/299\n",
      "Processing chunk 104/299\n",
      "Processing chunk 105/299\n",
      "Processing chunk 106/299\n",
      "Processing chunk 107/299\n",
      "Processing chunk 108/299\n",
      "Processing chunk 109/299\n",
      "Processing chunk 110/299\n",
      "Processing chunk 111/299\n",
      "Processing chunk 112/299\n",
      "Processing chunk 113/299\n",
      "Processing chunk 114/299\n",
      "Processing chunk 115/299\n",
      "Processing chunk 116/299\n",
      "Processing chunk 117/299\n",
      "Processing chunk 118/299\n",
      "Processing chunk 119/299\n",
      "Processing chunk 120/299\n",
      "Processing chunk 121/299\n",
      "Processing chunk 122/299\n",
      "Processing chunk 123/299\n",
      "Processing chunk 124/299\n",
      "Processing chunk 125/299\n",
      "Processing chunk 126/299\n",
      "Processing chunk 127/299\n",
      "Processing chunk 128/299\n",
      "Processing chunk 129/299\n",
      "Processing chunk 130/299\n",
      "Processing chunk 131/299\n",
      "Processing chunk 132/299\n",
      "Processing chunk 133/299\n",
      "Processing chunk 134/299\n",
      "Processing chunk 135/299\n",
      "Processing chunk 136/299\n",
      "Processing chunk 137/299\n",
      "Processing chunk 138/299\n",
      "Processing chunk 139/299\n",
      "Processing chunk 140/299\n",
      "Processing chunk 141/299\n",
      "Processing chunk 142/299\n",
      "Processing chunk 143/299\n",
      "Processing chunk 144/299\n",
      "Processing chunk 145/299\n",
      "Processing chunk 146/299\n",
      "Processing chunk 147/299\n",
      "Processing chunk 148/299\n",
      "Processing chunk 149/299\n",
      "Processing chunk 150/299\n",
      "Processing chunk 151/299\n",
      "Processing chunk 152/299\n",
      "Processing chunk 153/299\n",
      "Processing chunk 154/299\n",
      "Processing chunk 155/299\n",
      "Processing chunk 156/299\n",
      "Processing chunk 157/299\n",
      "Processing chunk 158/299\n",
      "Processing chunk 159/299\n",
      "Processing chunk 160/299\n",
      "Processing chunk 161/299\n",
      "Processing chunk 162/299\n",
      "Processing chunk 163/299\n",
      "Processing chunk 164/299\n",
      "Processing chunk 165/299\n",
      "Processing chunk 166/299\n",
      "Processing chunk 167/299\n",
      "Processing chunk 168/299\n",
      "Processing chunk 169/299\n",
      "Processing chunk 170/299\n",
      "Processing chunk 171/299\n",
      "Processing chunk 172/299\n",
      "Processing chunk 173/299\n",
      "Processing chunk 174/299\n",
      "Processing chunk 175/299\n",
      "Processing chunk 176/299\n",
      "Processing chunk 177/299\n",
      "Processing chunk 178/299\n",
      "Processing chunk 179/299\n",
      "Processing chunk 180/299\n",
      "Processing chunk 181/299\n",
      "Processing chunk 182/299\n",
      "Processing chunk 183/299\n",
      "Processing chunk 184/299\n",
      "Processing chunk 185/299\n",
      "Processing chunk 186/299\n",
      "Processing chunk 187/299\n",
      "Processing chunk 188/299\n",
      "Processing chunk 189/299\n",
      "Processing chunk 190/299\n",
      "Processing chunk 191/299\n",
      "Processing chunk 192/299\n",
      "Processing chunk 193/299\n",
      "Processing chunk 194/299\n",
      "Processing chunk 195/299\n",
      "Processing chunk 196/299\n",
      "Processing chunk 197/299\n",
      "Processing chunk 198/299\n",
      "Processing chunk 199/299\n",
      "Processing chunk 200/299\n",
      "Processing chunk 201/299\n",
      "Processing chunk 202/299\n",
      "Processing chunk 203/299\n",
      "Processing chunk 204/299\n",
      "Processing chunk 205/299\n",
      "Processing chunk 206/299\n",
      "Processing chunk 207/299\n",
      "Processing chunk 208/299\n",
      "Processing chunk 209/299\n",
      "Processing chunk 210/299\n",
      "Processing chunk 211/299\n",
      "Processing chunk 212/299\n",
      "Processing chunk 213/299\n",
      "Processing chunk 214/299\n",
      "Processing chunk 215/299\n",
      "Processing chunk 216/299\n",
      "Processing chunk 217/299\n",
      "Processing chunk 218/299\n",
      "Processing chunk 219/299\n",
      "Processing chunk 220/299\n",
      "Processing chunk 221/299\n",
      "Processing chunk 222/299\n",
      "Processing chunk 223/299\n",
      "Processing chunk 224/299\n",
      "Processing chunk 225/299\n",
      "Processing chunk 226/299\n",
      "Processing chunk 227/299\n",
      "Processing chunk 228/299\n",
      "Processing chunk 229/299\n",
      "Processing chunk 230/299\n",
      "Processing chunk 231/299\n",
      "Processing chunk 232/299\n",
      "Processing chunk 233/299\n",
      "Processing chunk 234/299\n",
      "Processing chunk 235/299\n",
      "Processing chunk 236/299\n",
      "Processing chunk 237/299\n",
      "Processing chunk 238/299\n",
      "Processing chunk 239/299\n",
      "Processing chunk 240/299\n",
      "Processing chunk 241/299\n",
      "Processing chunk 242/299\n",
      "Processing chunk 243/299\n",
      "Processing chunk 244/299\n",
      "Processing chunk 245/299\n",
      "Processing chunk 246/299\n",
      "Processing chunk 247/299\n",
      "Processing chunk 248/299\n",
      "Processing chunk 249/299\n",
      "Processing chunk 250/299\n",
      "Processing chunk 251/299\n",
      "Processing chunk 252/299\n",
      "Processing chunk 253/299\n",
      "Processing chunk 254/299\n",
      "Processing chunk 255/299\n",
      "Processing chunk 256/299\n",
      "Processing chunk 257/299\n",
      "Processing chunk 258/299\n",
      "Processing chunk 259/299\n",
      "Processing chunk 260/299\n",
      "Processing chunk 261/299\n",
      "Processing chunk 262/299\n",
      "Processing chunk 263/299\n",
      "Processing chunk 264/299\n",
      "Processing chunk 265/299\n",
      "Processing chunk 266/299\n",
      "Processing chunk 267/299\n",
      "Processing chunk 268/299\n",
      "Processing chunk 269/299\n",
      "Processing chunk 270/299\n",
      "Processing chunk 271/299\n",
      "Processing chunk 272/299\n",
      "Processing chunk 273/299\n",
      "Processing chunk 274/299\n",
      "Processing chunk 275/299\n",
      "Processing chunk 276/299\n",
      "Processing chunk 277/299\n",
      "Processing chunk 278/299\n",
      "Processing chunk 279/299\n",
      "Processing chunk 280/299\n",
      "Processing chunk 281/299\n",
      "Processing chunk 282/299\n",
      "Processing chunk 283/299\n",
      "Processing chunk 284/299\n",
      "Processing chunk 285/299\n",
      "Processing chunk 286/299\n",
      "Processing chunk 287/299\n",
      "Processing chunk 288/299\n",
      "Processing chunk 289/299\n",
      "Processing chunk 290/299\n",
      "Processing chunk 291/299\n",
      "Processing chunk 292/299\n",
      "Processing chunk 293/299\n",
      "Processing chunk 294/299\n",
      "Processing chunk 295/299\n",
      "Processing chunk 296/299\n",
      "Processing chunk 297/299\n",
      "Processing chunk 298/299\n",
      "Processing chunk 299/299\n",
      "Replay buffer has been filled with generated samples.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:26:57.045756Z",
     "start_time": "2025-07-15T20:26:44.912761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_steady_state_samples(\n",
    "        min_max_dict,\n",
    "        A_aug, B_aug, C_aug,\n",
    "        MPC_obj,\n",
    "        steady_states_samples_number,\n",
    "        Q_rew, R_rew,\n",
    "        agent,\n",
    "        IC_opt, bnds, cons, steady_states[\"y_ss\"], data_min, data_max, chunk_size= 100_000)"
   ],
   "id": "e4a5c2ef20eea4b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/1\n",
      "Replay buffer has been filled up with the steady_state values.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving and Loading the Replay Buffer to make sure we have the saved replay buffer",
   "id": "1a364467c94a80f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:26:58.799378Z",
     "start_time": "2025-07-15T20:26:57.050203Z"
    }
   },
   "cell_type": "code",
   "source": "filename_buffer = agent.replay_buffer.save(dir_path)",
   "id": "7c3672c490f2eb42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer saved to C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\replay_buffer_2507151626.h5\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:26:58.815673Z",
     "start_time": "2025-07-15T20:26:58.813736Z"
    }
   },
   "cell_type": "code",
   "source": "replay_buffer = ReplayBuffer(BUFFER_CAPACITY, STATE_DIM, ACTION_DIM)",
   "id": "bf7e93e1bfe48a34",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:27:00.726272Z",
     "start_time": "2025-07-15T20:26:58.826032Z"
    }
   },
   "cell_type": "code",
   "source": "replay_buffer.load(filename_buffer)",
   "id": "5475a49a2aa9ea78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer loaded from C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\replay_buffer_2507151626.h5\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:27:00.861784Z",
     "start_time": "2025-07-15T20:27:00.739833Z"
    }
   },
   "cell_type": "code",
   "source": "agent.replay_buffer = replay_buffer",
   "id": "b94c1c6074f96715",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre training the Agent",
   "id": "5738bba63f39b443"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:27:00.866190Z",
     "start_time": "2025-07-15T20:27:00.864512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import gc\n",
    "#\n",
    "# # Clear memory before DataLoader\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ],
   "id": "3f0870bb0401a8d4",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:27:00.876206Z",
     "start_time": "2025-07-15T20:27:00.873681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ReplayDataset(torch.from_numpy(replay_buffer.states).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.actions).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.rewards).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.next_states).to(dtype=torch.float32))\n",
    "data_loader = DataLoader(dataset, batch_size=2048 * 4, shuffle=True, num_workers=10, pin_memory=True)"
   ],
   "id": "8f1d8d24a31d46c0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T20:27:00.884192Z",
     "start_time": "2025-07-15T20:27:00.882828Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "49922dd2fbfad059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:18:04.338748Z",
     "start_time": "2025-07-15T20:27:00.893338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pretraining\n",
    "EPOCHS_FOR_PRETRAIN = 3000\n",
    "\n",
    "agent.pre_train_entire_data(dir_path, data_loader, EPOCHS_FOR_PRETRAIN)"
   ],
   "id": "db6699689be81502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 16:27:47.085449 Epoch 1, Actor Loss: 0.0026822865463982646, Critic Loss: 6079348.787957609\n",
      "Epoch 1, Learning rate (Critic): 0.0001\n",
      "Epoch 1, Learning rate (Actor): 0.0001\n",
      "2025-07-15 16:34:47.300607 Epoch 10, Actor Loss: 7.853822999541686e-06, Critic Loss: 516.0803024298177\n",
      "2025-07-15 16:42:58.983043 Epoch 20, Actor Loss: 2.8033056189640774e-06, Critic Loss: 211.43094332109376\n",
      "2025-07-15 16:52:06.261738 Epoch 30, Actor Loss: 1.7116014426205463e-06, Critic Loss: 143.60284401822918\n",
      "2025-07-15 17:01:25.091879 Epoch 40, Actor Loss: 1.2262625033714964e-06, Critic Loss: 115.35663267147623\n",
      "2025-07-15 17:10:46.611564 Epoch 50, Actor Loss: 9.522098025248852e-07, Critic Loss: 97.16969638343099\n",
      "2025-07-15 17:20:05.843347 Epoch 60, Actor Loss: 7.867441409694341e-07, Critic Loss: 81.32849118634441\n",
      "2025-07-15 17:29:24.701614 Epoch 70, Actor Loss: 6.651147892878119e-07, Critic Loss: 70.1532533137736\n",
      "2025-07-15 17:38:44.349470 Epoch 80, Actor Loss: 5.719167145211637e-07, Critic Loss: 67.55745833441568\n",
      "2025-07-15 17:47:40.798146 Epoch 90, Actor Loss: 5.068004056609425e-07, Critic Loss: 60.110138409472654\n",
      "2025-07-15 17:55:51.683728 Epoch 100, Actor Loss: 4.519649278836065e-07, Critic Loss: 56.21893861970215\n",
      "Epoch 100, Learning rate (Critic): 0.0001\n",
      "Epoch 100, Learning rate (Actor): 0.0001\n",
      "2025-07-15 18:04:00.564162 Epoch 110, Actor Loss: 4.074518068668112e-07, Critic Loss: 53.297926872875976\n",
      "2025-07-15 18:12:08.915287 Epoch 120, Actor Loss: 3.72744780603504e-07, Critic Loss: 49.5319633542277\n",
      "2025-07-15 18:20:19.635823 Epoch 130, Actor Loss: 3.4631156562233326e-07, Critic Loss: 48.77344714910482\n",
      "2025-07-15 18:28:28.572348 Epoch 140, Actor Loss: 3.1582710454798264e-07, Critic Loss: 44.97135498436686\n",
      "2025-07-15 18:36:31.113299 Epoch 150, Actor Loss: 2.988441721949736e-07, Critic Loss: 42.41466849114583\n",
      "2025-07-15 18:44:41.804494 Epoch 160, Actor Loss: 2.766715045829187e-07, Critic Loss: 42.08787357109375\n",
      "2025-07-15 18:52:53.827080 Epoch 170, Actor Loss: 2.6601493079976234e-07, Critic Loss: 39.4085356494222\n",
      "2025-07-15 19:01:08.321658 Epoch 180, Actor Loss: 2.523688789190298e-07, Critic Loss: 39.71956260130005\n",
      "2025-07-15 19:09:19.387242 Epoch 190, Actor Loss: 2.3536194030205783e-07, Critic Loss: 37.16487155323893\n",
      "2025-07-15 19:17:33.556230 Epoch 200, Actor Loss: 2.254406211300117e-07, Critic Loss: 36.79282134920858\n",
      "Epoch 200, Learning rate (Critic): 0.0001\n",
      "Epoch 200, Learning rate (Actor): 0.0001\n",
      "2025-07-15 19:25:42.724015 Epoch 210, Actor Loss: 2.1814784250649003e-07, Critic Loss: 36.48027854008993\n",
      "2025-07-15 19:33:54.484152 Epoch 220, Actor Loss: 2.0522482834591452e-07, Critic Loss: 36.175433016634116\n",
      "2025-07-15 19:42:12.967890 Epoch 230, Actor Loss: 1.957535249433325e-07, Critic Loss: 33.46559357347819\n",
      "2025-07-15 19:50:30.710756 Epoch 240, Actor Loss: 1.874973531102114e-07, Critic Loss: 33.820264220979816\n",
      "2025-07-15 19:58:45.961444 Epoch 250, Actor Loss: 1.8305811804848417e-07, Critic Loss: 32.147377037060544\n",
      "2025-07-15 20:06:55.653774 Epoch 260, Actor Loss: 1.7432630381210097e-07, Critic Loss: 32.09669585253499\n",
      "2025-07-15 20:15:09.554479 Epoch 270, Actor Loss: 1.6975833958602683e-07, Critic Loss: 30.016373582047525\n",
      "2025-07-15 20:23:27.179617 Epoch 280, Actor Loss: 1.661358651693263e-07, Critic Loss: 30.65271647335612\n",
      "2025-07-15 20:32:15.064934 Epoch 290, Actor Loss: 1.6000769187345818e-07, Critic Loss: 30.79208079152018\n",
      "2025-07-15 20:40:54.570260 Epoch 300, Actor Loss: 1.539700029747261e-07, Critic Loss: 28.484398625956217\n",
      "Epoch 300, Learning rate (Critic): 0.0001\n",
      "Epoch 300, Learning rate (Actor): 0.0001\n",
      "2025-07-15 20:49:09.696065 Epoch 310, Actor Loss: 1.4944747387651962e-07, Critic Loss: 28.783258829166666\n",
      "2025-07-15 20:57:23.963770 Epoch 320, Actor Loss: 1.428803029022068e-07, Critic Loss: 28.761793696865844\n",
      "2025-07-15 21:05:34.918301 Epoch 330, Actor Loss: 1.3967494589733177e-07, Critic Loss: 28.095428975216674\n",
      "2025-07-15 21:13:49.149458 Epoch 340, Actor Loss: 1.3689089846326776e-07, Critic Loss: 27.681020606622315\n",
      "2025-07-15 21:22:00.504225 Epoch 350, Actor Loss: 1.3514517872489098e-07, Critic Loss: 27.118548398077394\n",
      "2025-07-15 21:30:12.603462 Epoch 360, Actor Loss: 1.2999295714701778e-07, Critic Loss: 26.395138088098143\n",
      "2025-07-15 21:38:34.140949 Epoch 370, Actor Loss: 1.2633558956240448e-07, Critic Loss: 25.91902123811035\n",
      "2025-07-15 21:46:52.860675 Epoch 380, Actor Loss: 1.2373223660864216e-07, Critic Loss: 26.309281503488158\n",
      "2025-07-15 21:55:12.398806 Epoch 390, Actor Loss: 1.2051853210033793e-07, Critic Loss: 25.211681807796225\n",
      "2025-07-15 22:03:33.116525 Epoch 400, Actor Loss: 1.1750908875895523e-07, Critic Loss: 24.207278037776693\n",
      "Epoch 400, Learning rate (Critic): 0.0001\n",
      "Epoch 400, Learning rate (Actor): 0.0001\n",
      "2025-07-15 22:11:51.920312 Epoch 410, Actor Loss: 1.162465553373598e-07, Critic Loss: 24.09629245923462\n",
      "2025-07-15 22:20:14.131256 Epoch 420, Actor Loss: 1.140256338037337e-07, Critic Loss: 24.352788177278647\n",
      "2025-07-15 22:28:32.757441 Epoch 430, Actor Loss: 1.112619952524407e-07, Critic Loss: 23.599215200814818\n",
      "2025-07-15 22:36:55.953334 Epoch 440, Actor Loss: 1.0953128553859036e-07, Critic Loss: 24.367737625665285\n",
      "2025-07-15 22:45:18.493316 Epoch 450, Actor Loss: 1.0908148897863915e-07, Critic Loss: 22.778147703763835\n",
      "2025-07-15 22:53:37.364740 Epoch 460, Actor Loss: 1.0493631551097072e-07, Critic Loss: 22.96459854699707\n",
      "2025-07-15 23:01:54.865611 Epoch 470, Actor Loss: 1.0328459115386674e-07, Critic Loss: 22.78339592471517\n",
      "2025-07-15 23:10:12.973658 Epoch 480, Actor Loss: 1.0107138431631029e-07, Critic Loss: 21.45458804097697\n",
      "2025-07-15 23:18:27.633461 Epoch 490, Actor Loss: 9.853948605493012e-08, Critic Loss: 22.481417947969565\n",
      "2025-07-15 23:26:43.592096 Epoch 500, Actor Loss: 9.830638069620364e-08, Critic Loss: 21.86954762966512\n",
      "Epoch 500, Learning rate (Critic): 0.0001\n",
      "Epoch 500, Learning rate (Actor): 0.0001\n",
      "2025-07-15 23:34:58.507334 Epoch 510, Actor Loss: 9.45526297861458e-08, Critic Loss: 22.594081737202963\n",
      "2025-07-15 23:43:10.764049 Epoch 520, Actor Loss: 9.330487818751862e-08, Critic Loss: 21.046466844995116\n",
      "2025-07-15 23:51:27.078693 Epoch 530, Actor Loss: 9.236523549905844e-08, Critic Loss: 20.82483793042806\n",
      "2025-07-15 23:59:41.290202 Epoch 540, Actor Loss: 9.133474468550654e-08, Critic Loss: 20.67813865049235\n",
      "2025-07-16 00:07:54.174377 Epoch 550, Actor Loss: 8.987071053525141e-08, Critic Loss: 20.855404833528645\n",
      "2025-07-16 00:16:03.140044 Epoch 560, Actor Loss: 8.846414044079816e-08, Critic Loss: 20.679460319910685\n",
      "2025-07-16 00:24:08.058945 Epoch 570, Actor Loss: 8.69213234033244e-08, Critic Loss: 20.39055481204427\n",
      "2025-07-16 00:32:14.534031 Epoch 580, Actor Loss: 8.510218089289007e-08, Critic Loss: 20.375026203344728\n",
      "2025-07-16 00:40:18.217371 Epoch 590, Actor Loss: 8.422710753823897e-08, Critic Loss: 19.552909663430786\n",
      "2025-07-16 00:48:23.108572 Epoch 600, Actor Loss: 8.284355627937808e-08, Critic Loss: 19.651254056315103\n",
      "Epoch 600, Learning rate (Critic): 0.0001\n",
      "Epoch 600, Learning rate (Actor): 0.0001\n",
      "2025-07-16 00:56:29.483007 Epoch 610, Actor Loss: 8.145908050055368e-08, Critic Loss: 19.560594193713378\n",
      "2025-07-16 01:04:31.763416 Epoch 620, Actor Loss: 7.953952597558782e-08, Critic Loss: 18.493318594148764\n",
      "2025-07-16 01:12:38.090124 Epoch 630, Actor Loss: 7.879823994547527e-08, Critic Loss: 18.590329878492227\n",
      "2025-07-16 01:20:45.280079 Epoch 640, Actor Loss: 7.741000214688635e-08, Critic Loss: 18.574021206407675\n",
      "2025-07-16 01:28:53.230095 Epoch 650, Actor Loss: 7.623604169032963e-08, Critic Loss: 18.13275204282837\n",
      "2025-07-16 01:36:56.575399 Epoch 660, Actor Loss: 7.461939306410083e-08, Critic Loss: 18.53237403967692\n",
      "2025-07-16 01:45:05.981878 Epoch 670, Actor Loss: 7.535566867372836e-08, Critic Loss: 18.112503277583823\n",
      "2025-07-16 01:53:19.741645 Epoch 680, Actor Loss: 7.418931646100949e-08, Critic Loss: 17.986127573177082\n",
      "2025-07-16 02:01:34.245247 Epoch 690, Actor Loss: 7.360217103694898e-08, Critic Loss: 17.552841282670084\n",
      "2025-07-16 02:09:48.575764 Epoch 700, Actor Loss: 7.234897500978452e-08, Critic Loss: 16.567280048017373\n",
      "Epoch 700, Learning rate (Critic): 0.0001\n",
      "Epoch 700, Learning rate (Actor): 0.0001\n",
      "2025-07-16 02:17:58.590543 Epoch 710, Actor Loss: 7.293799664154221e-08, Critic Loss: 17.225561844059246\n",
      "2025-07-16 02:26:07.473646 Epoch 720, Actor Loss: 7.238852625647875e-08, Critic Loss: 16.84263689410553\n",
      "2025-07-16 02:34:18.342107 Epoch 730, Actor Loss: 6.968146717953611e-08, Critic Loss: 17.204805176820884\n",
      "2025-07-16 02:42:42.379865 Epoch 740, Actor Loss: 7.000137748370131e-08, Critic Loss: 17.001757945788576\n",
      "2025-07-16 02:51:21.495486 Epoch 750, Actor Loss: 6.940455039054238e-08, Critic Loss: 16.40789869911906\n",
      "2025-07-16 02:59:31.568980 Epoch 760, Actor Loss: 6.806727883437512e-08, Critic Loss: 16.58501404065755\n",
      "2025-07-16 03:07:42.559798 Epoch 770, Actor Loss: 6.716001414585359e-08, Critic Loss: 16.087046554427083\n",
      "2025-07-16 03:15:52.959003 Epoch 780, Actor Loss: 6.686574941062038e-08, Critic Loss: 16.133593392789713\n",
      "2025-07-16 03:24:01.769709 Epoch 790, Actor Loss: 6.552277647303602e-08, Critic Loss: 15.532902637194825\n",
      "2025-07-16 03:32:17.266889 Epoch 800, Actor Loss: 6.46409172092414e-08, Critic Loss: 15.601947974007162\n",
      "Epoch 800, Learning rate (Critic): 0.0001\n",
      "Epoch 800, Learning rate (Actor): 0.0001\n",
      "2025-07-16 03:40:29.436534 Epoch 810, Actor Loss: 6.524095277333496e-08, Critic Loss: 16.27194087035319\n",
      "2025-07-16 03:48:40.688333 Epoch 820, Actor Loss: 6.569498305504263e-08, Critic Loss: 15.658233599206543\n",
      "2025-07-16 03:56:51.356447 Epoch 830, Actor Loss: 6.386977497644845e-08, Critic Loss: 15.68939116145808\n",
      "2025-07-16 04:05:01.726515 Epoch 840, Actor Loss: 6.273713504028061e-08, Critic Loss: 15.206249146311443\n",
      "2025-07-16 04:13:14.667067 Epoch 850, Actor Loss: 6.284173215502354e-08, Critic Loss: 15.299175669848633\n",
      "2025-07-16 04:21:27.175010 Epoch 860, Actor Loss: 6.113842848646982e-08, Critic Loss: 15.738024441170756\n",
      "2025-07-16 04:29:49.896635 Epoch 870, Actor Loss: 6.080428697319273e-08, Critic Loss: 14.91478266899414\n",
      "2025-07-16 04:37:57.545256 Epoch 880, Actor Loss: 6.11857787584692e-08, Critic Loss: 14.437036787478638\n",
      "2025-07-16 04:46:14.720461 Epoch 890, Actor Loss: 6.06276486599351e-08, Critic Loss: 14.586003154182944\n",
      "2025-07-16 04:54:27.404036 Epoch 900, Actor Loss: 6.027773922154059e-08, Critic Loss: 13.814027153006998\n",
      "Epoch 900, Learning rate (Critic): 0.0001\n",
      "Epoch 900, Learning rate (Actor): 0.0001\n",
      "2025-07-16 05:02:49.276681 Epoch 910, Actor Loss: 6.010232316566544e-08, Critic Loss: 14.630017380643718\n",
      "2025-07-16 05:11:04.850719 Epoch 920, Actor Loss: 5.940886367235028e-08, Critic Loss: 14.239612661591085\n",
      "2025-07-16 05:19:19.673567 Epoch 930, Actor Loss: 5.807618916375456e-08, Critic Loss: 14.729745421321615\n",
      "2025-07-16 05:27:33.321792 Epoch 940, Actor Loss: 5.8087440220257727e-08, Critic Loss: 14.078511410302735\n",
      "2025-07-16 05:35:47.672798 Epoch 950, Actor Loss: 5.758776944666503e-08, Critic Loss: 14.176804016163635\n",
      "2025-07-16 05:43:58.518694 Epoch 960, Actor Loss: 5.7587983804478426e-08, Critic Loss: 14.038083231280517\n",
      "2025-07-16 05:52:10.215344 Epoch 970, Actor Loss: 5.6713893283813374e-08, Critic Loss: 13.486261780279033\n",
      "2025-07-16 06:00:34.262748 Epoch 980, Actor Loss: 5.603188029580603e-08, Critic Loss: 14.580086855769856\n",
      "2025-07-16 06:08:50.017504 Epoch 990, Actor Loss: 5.52246453345712e-08, Critic Loss: 13.878143718526204\n",
      "2025-07-16 06:17:08.252296 Epoch 1000, Actor Loss: 5.606166028052636e-08, Critic Loss: 13.177467209611002\n",
      "Epoch 1000, Learning rate (Critic): 0.0001\n",
      "Epoch 1000, Learning rate (Actor): 0.0001\n",
      "2025-07-16 06:25:25.612902 Epoch 1010, Actor Loss: 5.4911702886602144e-08, Critic Loss: 13.630404701975504\n",
      "2025-07-16 06:33:41.438784 Epoch 1020, Actor Loss: 5.450172612383994e-08, Critic Loss: 13.536930668701173\n",
      "2025-07-16 06:41:58.165922 Epoch 1030, Actor Loss: 5.388236328411343e-08, Critic Loss: 12.986388422111002\n",
      "2025-07-16 06:50:16.473747 Epoch 1040, Actor Loss: 5.399278817873589e-08, Critic Loss: 13.420637397802734\n",
      "2025-07-16 06:58:31.042983 Epoch 1050, Actor Loss: 5.41237338247394e-08, Critic Loss: 12.98911420304362\n",
      "2025-07-16 07:06:43.270057 Epoch 1060, Actor Loss: 5.2609486358854456e-08, Critic Loss: 12.932504705436706\n",
      "2025-07-16 07:14:52.638260 Epoch 1070, Actor Loss: 5.281346600639457e-08, Critic Loss: 13.088774932491049\n",
      "2025-07-16 07:23:06.912349 Epoch 1080, Actor Loss: 5.166719004243229e-08, Critic Loss: 12.655954187793478\n",
      "2025-07-16 07:31:17.947075 Epoch 1090, Actor Loss: 5.1984335710487056e-08, Critic Loss: 12.237678450489298\n",
      "2025-07-16 07:39:40.823355 Epoch 1100, Actor Loss: 5.13355205245413e-08, Critic Loss: 12.477784536560058\n",
      "Epoch 1100, Learning rate (Critic): 0.0001\n",
      "Epoch 1100, Learning rate (Actor): 0.0001\n",
      "2025-07-16 07:47:39.879044 Epoch 1110, Actor Loss: 5.1243499000217225e-08, Critic Loss: 12.417466607025656\n",
      "2025-07-16 07:55:41.984859 Epoch 1120, Actor Loss: 5.018900716489346e-08, Critic Loss: 12.501675503114827\n",
      "2025-07-16 08:03:43.765657 Epoch 1130, Actor Loss: 5.021233070579001e-08, Critic Loss: 12.038290220481365\n",
      "2025-07-16 08:11:52.584891 Epoch 1140, Actor Loss: 5.03077382361577e-08, Critic Loss: 12.018954766482544\n",
      "2025-07-16 08:20:07.576151 Epoch 1150, Actor Loss: 4.95526559112174e-08, Critic Loss: 12.056869372669475\n",
      "2025-07-16 08:28:20.723108 Epoch 1160, Actor Loss: 4.947280897129834e-08, Critic Loss: 12.178820974651082\n",
      "2025-07-16 08:36:36.308964 Epoch 1170, Actor Loss: 4.857588917607245e-08, Critic Loss: 11.816157266125996\n",
      "2025-07-16 08:44:47.336236 Epoch 1180, Actor Loss: 4.924888247746821e-08, Critic Loss: 12.15061560023117\n",
      "2025-07-16 08:53:06.147390 Epoch 1190, Actor Loss: 4.7910985211365184e-08, Critic Loss: 11.592032700522614\n",
      "2025-07-16 09:01:49.773316 Epoch 1200, Actor Loss: 4.836129816430912e-08, Critic Loss: 11.985266911116536\n",
      "Epoch 1200, Learning rate (Critic): 0.0001\n",
      "Epoch 1200, Learning rate (Actor): 0.0001\n",
      "2025-07-16 09:10:19.171329 Epoch 1210, Actor Loss: 4.780744101354533e-08, Critic Loss: 11.243653052116139\n",
      "2025-07-16 09:18:34.288847 Epoch 1220, Actor Loss: 4.7379830235735195e-08, Critic Loss: 11.423161006075032\n",
      "2025-07-16 09:26:53.596472 Epoch 1230, Actor Loss: 4.77658838105602e-08, Critic Loss: 11.716683456917318\n",
      "2025-07-16 09:35:13.395863 Epoch 1240, Actor Loss: 4.661411657107237e-08, Critic Loss: 11.179623390470377\n",
      "2025-07-16 09:43:27.024052 Epoch 1250, Actor Loss: 4.6398357625169714e-08, Critic Loss: 11.495456725138347\n",
      "2025-07-16 09:51:46.601114 Epoch 1260, Actor Loss: 4.64219190315968e-08, Critic Loss: 11.479026115283713\n",
      "2025-07-16 09:59:57.732127 Epoch 1270, Actor Loss: 4.591445960040422e-08, Critic Loss: 11.105522912390137\n",
      "2025-07-16 10:08:12.847201 Epoch 1280, Actor Loss: 4.5779006482227184e-08, Critic Loss: 11.17738565463969\n",
      "2025-07-16 10:16:28.983517 Epoch 1290, Actor Loss: 4.539770682038503e-08, Critic Loss: 11.166005105981446\n",
      "2025-07-16 10:24:44.635015 Epoch 1300, Actor Loss: 4.532997383807924e-08, Critic Loss: 10.962578250939941\n",
      "Epoch 1300, Learning rate (Critic): 0.0001\n",
      "Epoch 1300, Learning rate (Actor): 0.0001\n",
      "2025-07-16 10:33:00.810032 Epoch 1310, Actor Loss: 4.5236227281323715e-08, Critic Loss: 10.765537942156982\n",
      "2025-07-16 10:41:16.689576 Epoch 1320, Actor Loss: 4.426862218644298e-08, Critic Loss: 10.829860870507812\n",
      "2025-07-16 10:49:31.101811 Epoch 1330, Actor Loss: 4.413075348020357e-08, Critic Loss: 10.716461442102052\n",
      "2025-07-16 10:57:43.607387 Epoch 1340, Actor Loss: 4.41223171091527e-08, Critic Loss: 10.71642719025065\n",
      "2025-07-16 11:05:56.481436 Epoch 1350, Actor Loss: 4.363736618945495e-08, Critic Loss: 10.467713631219482\n",
      "2025-07-16 11:14:05.067092 Epoch 1360, Actor Loss: 4.292668381317526e-08, Critic Loss: 10.37040594818522\n",
      "2025-07-16 11:22:16.856447 Epoch 1370, Actor Loss: 4.308695883937617e-08, Critic Loss: 10.529636964086913\n",
      "2025-07-16 11:30:30.802995 Epoch 1380, Actor Loss: 4.318558761354628e-08, Critic Loss: 10.582727691377768\n",
      "2025-07-16 11:38:43.610717 Epoch 1390, Actor Loss: 4.294398200096718e-08, Critic Loss: 10.45431374396871\n",
      "2025-07-16 11:47:03.604499 Epoch 1400, Actor Loss: 4.224649028962328e-08, Critic Loss: 10.427947941495514\n",
      "Epoch 1400, Learning rate (Critic): 0.0001\n",
      "Epoch 1400, Learning rate (Actor): 0.0001\n",
      "2025-07-16 11:55:19.062425 Epoch 1410, Actor Loss: 4.218407802115962e-08, Critic Loss: 10.411889444122314\n",
      "2025-07-16 12:03:20.872613 Epoch 1420, Actor Loss: 4.2199795176490324e-08, Critic Loss: 10.700695683717855\n",
      "2025-07-16 12:12:10.873314 Epoch 1430, Actor Loss: 4.1926033849373804e-08, Critic Loss: 10.052024565419643\n",
      "2025-07-16 12:21:01.739433 Epoch 1440, Actor Loss: 4.205689043346865e-08, Critic Loss: 10.506224149356079\n",
      "2025-07-16 12:29:48.340410 Epoch 1450, Actor Loss: 4.1620484254841966e-08, Critic Loss: 10.161344685811361\n",
      "2025-07-16 12:38:08.689512 Epoch 1460, Actor Loss: 4.1211549138445964e-08, Critic Loss: 10.284627600582123\n",
      "2025-07-16 12:46:20.418354 Epoch 1470, Actor Loss: 4.115284753477984e-08, Critic Loss: 9.994615411448414\n",
      "2025-07-16 12:54:35.627486 Epoch 1480, Actor Loss: 4.042622286452418e-08, Critic Loss: 9.871589017626953\n",
      "2025-07-16 13:02:49.361785 Epoch 1490, Actor Loss: 4.044580036299976e-08, Critic Loss: 10.034226835446168\n",
      "2025-07-16 13:10:59.464188 Epoch 1500, Actor Loss: 3.98631769924729e-08, Critic Loss: 10.156725406355795\n",
      "Epoch 1500, Learning rate (Critic): 0.0001\n",
      "Epoch 1500, Learning rate (Actor): 0.0001\n",
      "2025-07-16 13:19:11.100416 Epoch 1510, Actor Loss: 4.025676059804937e-08, Critic Loss: 9.771735295792643\n",
      "2025-07-16 13:27:16.868678 Epoch 1520, Actor Loss: 3.92640481367683e-08, Critic Loss: 9.795688431211344\n",
      "2025-07-16 13:35:25.670344 Epoch 1530, Actor Loss: 3.9136898505216774e-08, Critic Loss: 9.929068567765299\n",
      "2025-07-16 13:43:40.436092 Epoch 1540, Actor Loss: 3.934156347934656e-08, Critic Loss: 9.71799311315918\n",
      "2025-07-16 13:51:50.353475 Epoch 1550, Actor Loss: 3.911204881408897e-08, Critic Loss: 9.571096908390299\n",
      "2025-07-16 13:59:59.916015 Epoch 1560, Actor Loss: 3.9238703963686324e-08, Critic Loss: 9.201554777064514\n",
      "2025-07-16 14:08:05.933807 Epoch 1570, Actor Loss: 3.8576850004665173e-08, Critic Loss: 9.509629855437215\n",
      "2025-07-16 14:16:13.968275 Epoch 1580, Actor Loss: 3.8388910248280205e-08, Critic Loss: 9.580603719877116\n",
      "2025-07-16 14:24:27.031196 Epoch 1590, Actor Loss: 3.824994989026512e-08, Critic Loss: 9.3877252246226\n",
      "2025-07-16 14:32:31.861110 Epoch 1600, Actor Loss: 3.7755633630611856e-08, Critic Loss: 9.351867090995915\n",
      "Epoch 1600, Learning rate (Critic): 0.0001\n",
      "Epoch 1600, Learning rate (Actor): 0.0001\n",
      "2025-07-16 14:40:43.412573 Epoch 1610, Actor Loss: 3.843796144286292e-08, Critic Loss: 9.216249329492188\n",
      "2025-07-16 14:48:54.454701 Epoch 1620, Actor Loss: 3.797253655064878e-08, Critic Loss: 9.084510520865885\n",
      "2025-07-16 14:57:02.660804 Epoch 1630, Actor Loss: 3.752003376008967e-08, Critic Loss: 9.565772708235677\n",
      "2025-07-16 15:05:12.640092 Epoch 1640, Actor Loss: 3.7292725406685655e-08, Critic Loss: 9.460035706103515\n",
      "2025-07-16 15:13:48.061756 Epoch 1650, Actor Loss: 3.722432263472607e-08, Critic Loss: 9.215746995863851\n",
      "2025-07-16 15:22:21.713037 Epoch 1660, Actor Loss: 3.6971201547612506e-08, Critic Loss: 9.241401959398651\n",
      "2025-07-16 15:30:33.639420 Epoch 1670, Actor Loss: 3.706852812602695e-08, Critic Loss: 9.57424496513672\n",
      "2025-07-16 15:38:45.338595 Epoch 1680, Actor Loss: 3.643089556020035e-08, Critic Loss: 9.13829755967102\n",
      "2025-07-16 15:46:54.883321 Epoch 1690, Actor Loss: 3.659571365332492e-08, Critic Loss: 9.214881604735819\n",
      "2025-07-16 15:55:05.195539 Epoch 1700, Actor Loss: 3.620594722904874e-08, Critic Loss: 9.149450111438497\n",
      "Epoch 1700, Learning rate (Critic): 0.0001\n",
      "Epoch 1700, Learning rate (Actor): 0.0001\n",
      "2025-07-16 16:03:16.841727 Epoch 1710, Actor Loss: 3.6032626183600766e-08, Critic Loss: 9.225635029859925\n",
      "2025-07-16 16:11:24.927548 Epoch 1720, Actor Loss: 3.5916259670163225e-08, Critic Loss: 9.254510087209066\n",
      "2025-07-16 16:19:32.280759 Epoch 1730, Actor Loss: 3.6793220060432454e-08, Critic Loss: 9.249948557903036\n",
      "2025-07-16 16:27:34.089499 Epoch 1740, Actor Loss: 3.568673281991626e-08, Critic Loss: 8.812393748621624\n",
      "2025-07-16 16:35:39.745282 Epoch 1750, Actor Loss: 3.527039563496904e-08, Critic Loss: 9.138749083733495\n",
      "2025-07-16 16:43:50.372941 Epoch 1760, Actor Loss: 3.5288349160945624e-08, Critic Loss: 8.908947674865722\n",
      "2025-07-16 16:51:40.363688 Epoch 1770, Actor Loss: 3.5312958851189556e-08, Critic Loss: 8.462321466161091\n",
      "2025-07-16 17:00:06.286295 Epoch 1780, Actor Loss: 3.491778561831325e-08, Critic Loss: 8.769673958247884\n",
      "2025-07-16 17:08:35.827135 Epoch 1790, Actor Loss: 3.495489288062951e-08, Critic Loss: 9.013504375750987\n",
      "2025-07-16 17:16:59.816742 Epoch 1800, Actor Loss: 3.521454135388922e-08, Critic Loss: 8.616682251745605\n",
      "Epoch 1800, Learning rate (Critic): 0.0001\n",
      "Epoch 1800, Learning rate (Actor): 0.0001\n",
      "2025-07-16 17:25:29.570593 Epoch 1810, Actor Loss: 3.4464810685343156e-08, Critic Loss: 8.949887548679607\n",
      "2025-07-16 17:33:56.365883 Epoch 1820, Actor Loss: 3.4432409954661124e-08, Critic Loss: 8.470427932354482\n",
      "2025-07-16 17:42:30.100844 Epoch 1830, Actor Loss: 3.4937313927063467e-08, Critic Loss: 8.696327792665608\n",
      "2025-07-16 17:50:57.940599 Epoch 1840, Actor Loss: 3.466865621057878e-08, Critic Loss: 7.9789050011675515\n",
      "2025-07-16 17:59:22.751261 Epoch 1850, Actor Loss: 3.479099078420707e-08, Critic Loss: 8.41165625044759\n",
      "2025-07-16 18:07:43.211334 Epoch 1860, Actor Loss: 3.3895888762458526e-08, Critic Loss: 8.095132208441163\n",
      "2025-07-16 18:16:06.330118 Epoch 1870, Actor Loss: 3.370661551251336e-08, Critic Loss: 8.607593042258454\n",
      "2025-07-16 18:24:27.504857 Epoch 1880, Actor Loss: 3.3255539976880754e-08, Critic Loss: 8.083252399397786\n",
      "2025-07-16 18:33:01.558608 Epoch 1890, Actor Loss: 3.311314369254698e-08, Critic Loss: 8.470569721900432\n",
      "2025-07-16 18:41:30.989052 Epoch 1900, Actor Loss: 3.336838452696611e-08, Critic Loss: 8.503066810251871\n",
      "Epoch 1900, Learning rate (Critic): 0.0001\n",
      "Epoch 1900, Learning rate (Actor): 0.0001\n",
      "2025-07-16 18:50:05.472468 Epoch 1910, Actor Loss: 3.2907646186458804e-08, Critic Loss: 8.368177334819539\n",
      "2025-07-16 18:58:38.903317 Epoch 1920, Actor Loss: 3.303185373042652e-08, Critic Loss: 7.979225222873942\n",
      "2025-07-16 19:07:18.314703 Epoch 1930, Actor Loss: 3.2871164566745394e-08, Critic Loss: 8.275427235225424\n",
      "2025-07-16 19:15:57.491779 Epoch 1940, Actor Loss: 3.29216553089888e-08, Critic Loss: 7.8462165639404295\n",
      "2025-07-16 19:24:19.015066 Epoch 1950, Actor Loss: 3.236505429512514e-08, Critic Loss: 8.238600384780375\n",
      "2025-07-16 19:32:47.033444 Epoch 1960, Actor Loss: 3.2582325189052124e-08, Critic Loss: 7.993149174501546\n",
      "2025-07-16 19:41:14.471005 Epoch 1970, Actor Loss: 3.2486117035333944e-08, Critic Loss: 8.026446167700831\n",
      "2025-07-16 19:49:37.410318 Epoch 1980, Actor Loss: 3.235408382111018e-08, Critic Loss: 8.16125522125651\n",
      "2025-07-16 19:58:04.031926 Epoch 1990, Actor Loss: 3.1992167934186e-08, Critic Loss: 7.625735444381205\n",
      "2025-07-16 20:06:39.746359 Epoch 2000, Actor Loss: 3.177150625056129e-08, Critic Loss: 7.93198433663915\n",
      "Epoch 2000, Learning rate (Critic): 0.0001\n",
      "Epoch 2000, Learning rate (Actor): 0.0001\n",
      "2025-07-16 20:15:14.045741 Epoch 2010, Actor Loss: 3.19651821622756e-08, Critic Loss: 7.9029144423919675\n",
      "2025-07-16 20:23:43.662013 Epoch 2020, Actor Loss: 3.186568539800116e-08, Critic Loss: 8.036202950670878\n",
      "2025-07-16 20:32:15.845455 Epoch 2030, Actor Loss: 3.19671953454872e-08, Critic Loss: 7.971652123228964\n",
      "2025-07-16 20:40:47.681751 Epoch 2040, Actor Loss: 3.139359798935478e-08, Critic Loss: 8.079938212529246\n",
      "2025-07-16 20:49:13.198479 Epoch 2050, Actor Loss: 3.1274810787878475e-08, Critic Loss: 7.8256916348286945\n",
      "2025-07-16 20:57:41.052760 Epoch 2060, Actor Loss: 3.136041239877159e-08, Critic Loss: 7.820865546633911\n",
      "2025-07-16 21:06:09.419791 Epoch 2070, Actor Loss: 3.12655234851718e-08, Critic Loss: 7.925260847587077\n",
      "2025-07-16 21:14:44.954281 Epoch 2080, Actor Loss: 3.1515641472863836e-08, Critic Loss: 7.987828800008138\n",
      "2025-07-16 21:23:26.506444 Epoch 2090, Actor Loss: 3.0837836619897034e-08, Critic Loss: 7.659354322332637\n",
      "2025-07-16 21:32:27.168964 Epoch 2100, Actor Loss: 3.0773663230032374e-08, Critic Loss: 7.924120797823079\n",
      "Epoch 2100, Learning rate (Critic): 0.0001\n",
      "Epoch 2100, Learning rate (Actor): 0.0001\n",
      "2025-07-16 21:41:00.081541 Epoch 2110, Actor Loss: 3.085090532476897e-08, Critic Loss: 7.6828871240414935\n",
      "2025-07-16 21:49:27.461207 Epoch 2120, Actor Loss: 3.055519869330965e-08, Critic Loss: 7.843249934472656\n",
      "2025-07-16 21:58:00.481453 Epoch 2130, Actor Loss: 3.091713547833024e-08, Critic Loss: 7.8347126453053795\n",
      "2025-07-16 22:06:29.566386 Epoch 2140, Actor Loss: 3.0524202591929375e-08, Critic Loss: 7.647055009602737\n",
      "2025-07-16 22:14:53.792788 Epoch 2150, Actor Loss: 3.032534332810049e-08, Critic Loss: 7.502202813840739\n",
      "2025-07-16 22:23:18.665769 Epoch 2160, Actor Loss: 3.0557598206200965e-08, Critic Loss: 7.623547670296224\n",
      "2025-07-16 22:31:45.700632 Epoch 2170, Actor Loss: 3.001138029394497e-08, Critic Loss: 7.506782042513021\n",
      "2025-07-16 22:40:19.910745 Epoch 2180, Actor Loss: 2.9784607900069203e-08, Critic Loss: 7.600042278576661\n",
      "2025-07-16 22:48:53.424084 Epoch 2190, Actor Loss: 3.0256674226059964e-08, Critic Loss: 7.533338693768311\n",
      "2025-07-16 22:57:22.016024 Epoch 2200, Actor Loss: 2.980536551484268e-08, Critic Loss: 7.548804588972982\n",
      "Epoch 2200, Learning rate (Critic): 0.0001\n",
      "Epoch 2200, Learning rate (Actor): 0.0001\n",
      "2025-07-16 23:05:48.185196 Epoch 2210, Actor Loss: 2.982778971310533e-08, Critic Loss: 7.194243542490641\n",
      "2025-07-16 23:14:16.895670 Epoch 2220, Actor Loss: 2.9500802114982132e-08, Critic Loss: 7.433441100305176\n",
      "2025-07-16 23:22:47.846285 Epoch 2230, Actor Loss: 2.9253988464703677e-08, Critic Loss: 7.17510167126414\n",
      "2025-07-16 23:31:25.989080 Epoch 2240, Actor Loss: 2.956447090865216e-08, Critic Loss: 7.549519672263082\n",
      "2025-07-16 23:40:03.771360 Epoch 2250, Actor Loss: 2.9421815205614622e-08, Critic Loss: 7.177754498405457\n",
      "2025-07-16 23:48:36.290807 Epoch 2260, Actor Loss: 2.9111229591156492e-08, Critic Loss: 7.515673440766652\n",
      "2025-07-16 23:57:08.862437 Epoch 2270, Actor Loss: 2.8879503858881133e-08, Critic Loss: 7.391496783536784\n",
      "2025-07-17 00:05:44.676674 Epoch 2280, Actor Loss: 2.8883412285119903e-08, Critic Loss: 7.367230167448425\n",
      "2025-07-17 00:14:21.542315 Epoch 2290, Actor Loss: 2.8711124318253192e-08, Critic Loss: 7.130989952355957\n",
      "2025-07-17 00:22:50.227158 Epoch 2300, Actor Loss: 2.893833957377865e-08, Critic Loss: 7.3723801458343505\n",
      "Epoch 2300, Learning rate (Critic): 0.0001\n",
      "Epoch 2300, Learning rate (Actor): 0.0001\n",
      "2025-07-17 00:31:16.335100 Epoch 2310, Actor Loss: 2.8995868600977322e-08, Critic Loss: 7.1825838971675875\n",
      "2025-07-17 00:39:40.313200 Epoch 2320, Actor Loss: 2.8452896806709305e-08, Critic Loss: 7.320396864108022\n",
      "2025-07-17 00:48:15.070750 Epoch 2330, Actor Loss: 2.853842949219446e-08, Critic Loss: 7.132880583398437\n",
      "2025-07-17 00:56:48.932327 Epoch 2340, Actor Loss: 2.850580610417334e-08, Critic Loss: 7.377152227189128\n",
      "2025-07-17 01:05:26.920404 Epoch 2350, Actor Loss: 2.8391567094289863e-08, Critic Loss: 7.268821789798991\n",
      "2025-07-17 01:14:02.284107 Epoch 2360, Actor Loss: 2.835840959196503e-08, Critic Loss: 7.421813248295084\n",
      "2025-07-17 01:22:34.780076 Epoch 2370, Actor Loss: 2.8278059399322803e-08, Critic Loss: 7.518047270704651\n",
      "2025-07-17 01:31:07.350077 Epoch 2380, Actor Loss: 2.7865466024400122e-08, Critic Loss: 7.434046889603678\n",
      "2025-07-17 01:39:42.629743 Epoch 2390, Actor Loss: 2.7948885935378107e-08, Critic Loss: 6.935748456928253\n",
      "2025-07-17 01:48:12.196668 Epoch 2400, Actor Loss: 2.776661719177961e-08, Critic Loss: 7.09804752116038\n",
      "Epoch 2400, Learning rate (Critic): 0.0001\n",
      "Epoch 2400, Learning rate (Actor): 0.0001\n",
      "2025-07-17 01:56:43.672119 Epoch 2410, Actor Loss: 2.769660329844328e-08, Critic Loss: 7.018900374398041\n",
      "2025-07-17 02:05:16.502914 Epoch 2420, Actor Loss: 2.7385356416971262e-08, Critic Loss: 7.060138257904053\n",
      "2025-07-17 02:13:41.447228 Epoch 2430, Actor Loss: 2.7579314894516452e-08, Critic Loss: 6.893812981644567\n",
      "2025-07-17 02:22:07.677204 Epoch 2440, Actor Loss: 2.7617915886063807e-08, Critic Loss: 6.785924642769369\n",
      "2025-07-17 02:30:34.964254 Epoch 2450, Actor Loss: 2.76220809819885e-08, Critic Loss: 7.0611154739807125\n",
      "2025-07-17 02:39:00.336056 Epoch 2460, Actor Loss: 2.7347814725908392e-08, Critic Loss: 6.8049472420911155\n",
      "2025-07-17 02:47:30.041471 Epoch 2470, Actor Loss: 2.7301899052755137e-08, Critic Loss: 6.9045546869873045\n",
      "2025-07-17 02:55:57.860856 Epoch 2480, Actor Loss: 2.6840674079790005e-08, Critic Loss: 6.859080449678548\n",
      "2025-07-17 03:04:28.038873 Epoch 2490, Actor Loss: 2.7163522080127223e-08, Critic Loss: 6.811281412102509\n",
      "2025-07-17 03:12:57.727690 Epoch 2500, Actor Loss: 2.703893175746354e-08, Critic Loss: 6.895761136156718\n",
      "Epoch 2500, Learning rate (Critic): 0.0001\n",
      "Epoch 2500, Learning rate (Actor): 0.0001\n",
      "2025-07-17 03:21:28.878575 Epoch 2510, Actor Loss: 2.695532461317877e-08, Critic Loss: 6.9598440659293495\n",
      "2025-07-17 03:29:54.235253 Epoch 2520, Actor Loss: 2.681151574146649e-08, Critic Loss: 7.089680688340759\n",
      "2025-07-17 03:38:32.055138 Epoch 2530, Actor Loss: 2.6755589516981596e-08, Critic Loss: 6.998161522276306\n",
      "2025-07-17 03:47:27.631430 Epoch 2540, Actor Loss: 2.648295026564256e-08, Critic Loss: 6.4909637920283\n",
      "2025-07-17 03:55:57.355785 Epoch 2550, Actor Loss: 2.681791806239744e-08, Critic Loss: 6.75265333719635\n",
      "2025-07-17 04:04:23.130075 Epoch 2560, Actor Loss: 2.670052820910011e-08, Critic Loss: 6.838579989001465\n",
      "2025-07-17 04:12:50.469437 Epoch 2570, Actor Loss: 2.6569035686475217e-08, Critic Loss: 6.8120025438883465\n",
      "2025-07-17 04:21:23.913132 Epoch 2580, Actor Loss: 2.6269826664429274e-08, Critic Loss: 6.7494418305419925\n",
      "2025-07-17 04:29:57.894846 Epoch 2590, Actor Loss: 2.6290433147617403e-08, Critic Loss: 6.7005120761057535\n",
      "2025-07-17 04:38:27.556103 Epoch 2600, Actor Loss: 2.6433796085014667e-08, Critic Loss: 6.817585569753392\n",
      "Epoch 2600, Learning rate (Critic): 0.0001\n",
      "Epoch 2600, Learning rate (Actor): 0.0001\n",
      "2025-07-17 04:46:54.430913 Epoch 2610, Actor Loss: 2.62690238587993e-08, Critic Loss: 6.872366789245605\n",
      "2025-07-17 04:55:22.865101 Epoch 2620, Actor Loss: 2.611762029074119e-08, Critic Loss: 6.68889260055542\n",
      "2025-07-17 05:03:46.895603 Epoch 2630, Actor Loss: 2.579888374004516e-08, Critic Loss: 6.896278277502187\n",
      "2025-07-17 05:12:15.057578 Epoch 2640, Actor Loss: 2.619502866187607e-08, Critic Loss: 6.633384922986221\n",
      "2025-07-17 05:20:39.133241 Epoch 2650, Actor Loss: 2.5799926510167854e-08, Critic Loss: 6.6292078652669275\n",
      "2025-07-17 05:29:04.205726 Epoch 2660, Actor Loss: 2.5731272536442398e-08, Critic Loss: 6.43270943959554\n",
      "2025-07-17 05:37:24.798449 Epoch 2670, Actor Loss: 2.5509388337013662e-08, Critic Loss: 6.672858032017008\n",
      "2025-07-17 05:45:49.230803 Epoch 2680, Actor Loss: 2.5423706047695306e-08, Critic Loss: 6.552419729024251\n",
      "2025-07-17 05:54:11.984887 Epoch 2690, Actor Loss: 2.5446412159120275e-08, Critic Loss: 6.718499710738119\n",
      "2025-07-17 06:02:39.268606 Epoch 2700, Actor Loss: 2.5434957978707946e-08, Critic Loss: 6.373925082145182\n",
      "Epoch 2700, Learning rate (Critic): 0.0001\n",
      "Epoch 2700, Learning rate (Actor): 0.0001\n",
      "2025-07-17 06:11:07.135199 Epoch 2710, Actor Loss: 2.537800073677469e-08, Critic Loss: 6.5978522748689015\n",
      "2025-07-17 06:19:35.235264 Epoch 2720, Actor Loss: 2.4940735804231432e-08, Critic Loss: 6.462963748750814\n",
      "2025-07-17 06:27:58.478579 Epoch 2730, Actor Loss: 2.5238415747518653e-08, Critic Loss: 6.740478412744141\n",
      "2025-07-17 06:36:24.348809 Epoch 2740, Actor Loss: 2.4997454098312727e-08, Critic Loss: 6.823105234143067\n",
      "2025-07-17 06:44:56.266243 Epoch 2750, Actor Loss: 2.524347148248201e-08, Critic Loss: 6.510866245769247\n",
      "2025-07-17 06:53:31.397002 Epoch 2760, Actor Loss: 2.4976597199717313e-08, Critic Loss: 6.465639743043518\n",
      "2025-07-17 07:02:08.414288 Epoch 2770, Actor Loss: 2.5069795426497877e-08, Critic Loss: 6.694067575508626\n",
      "2025-07-17 07:10:39.129347 Epoch 2780, Actor Loss: 2.5155178356484006e-08, Critic Loss: 6.550916225665283\n",
      "2025-07-17 07:19:05.594661 Epoch 2790, Actor Loss: 2.4769009029228074e-08, Critic Loss: 6.5729220138682045\n",
      "2025-07-17 07:27:32.166225 Epoch 2800, Actor Loss: 2.478223696874314e-08, Critic Loss: 6.177345926883952\n",
      "Epoch 2800, Learning rate (Critic): 0.0001\n",
      "Epoch 2800, Learning rate (Actor): 0.0001\n",
      "2025-07-17 07:35:56.344480 Epoch 2810, Actor Loss: 2.4654813061160515e-08, Critic Loss: 6.374229888375854\n",
      "2025-07-17 07:44:37.133819 Epoch 2820, Actor Loss: 2.4408474065694464e-08, Critic Loss: 6.627790000837708\n",
      "2025-07-17 07:53:01.424963 Epoch 2830, Actor Loss: 2.4307811462025104e-08, Critic Loss: 6.337846816294734\n",
      "2025-07-17 08:01:23.756468 Epoch 2840, Actor Loss: 2.4153303561918923e-08, Critic Loss: 6.457969009875234\n",
      "2025-07-17 08:09:49.582418 Epoch 2850, Actor Loss: 2.410602055466787e-08, Critic Loss: 6.587471556912232\n",
      "2025-07-17 08:18:21.829785 Epoch 2860, Actor Loss: 2.443366929772992e-08, Critic Loss: 6.272444255391439\n",
      "2025-07-17 08:26:59.775498 Epoch 2870, Actor Loss: 2.4288445743547982e-08, Critic Loss: 6.375288287365723\n",
      "2025-07-17 08:35:29.891483 Epoch 2880, Actor Loss: 2.4304927119231933e-08, Critic Loss: 6.249221870192973\n",
      "2025-07-17 08:43:59.271062 Epoch 2890, Actor Loss: 2.4232519283759758e-08, Critic Loss: 6.197656642873128\n",
      "2025-07-17 08:52:27.751833 Epoch 2900, Actor Loss: 2.422179427898641e-08, Critic Loss: 6.197378679232788\n",
      "Epoch 2900, Learning rate (Critic): 0.0001\n",
      "Epoch 2900, Learning rate (Actor): 0.0001\n",
      "2025-07-17 09:00:55.535584 Epoch 2910, Actor Loss: 2.405123328068536e-08, Critic Loss: 6.291501123096975\n",
      "2025-07-17 09:09:26.960714 Epoch 2920, Actor Loss: 2.4206542246076878e-08, Critic Loss: 6.389600745236842\n",
      "2025-07-17 09:17:53.458697 Epoch 2930, Actor Loss: 2.3814646365940463e-08, Critic Loss: 6.346547468045044\n",
      "2025-07-17 09:26:21.467171 Epoch 2940, Actor Loss: 2.387751179027191e-08, Critic Loss: 6.181520758119074\n",
      "2025-07-17 09:34:49.689586 Epoch 2950, Actor Loss: 2.3678828873384342e-08, Critic Loss: 6.2565778042991\n",
      "2025-07-17 09:43:15.651851 Epoch 2960, Actor Loss: 2.4020375497349278e-08, Critic Loss: 6.31618140102005\n",
      "2025-07-17 09:52:00.782183 Epoch 2970, Actor Loss: 2.3819382005164395e-08, Critic Loss: 6.147014466233062\n",
      "2025-07-17 10:01:01.105518 Epoch 2980, Actor Loss: 2.368775336304149e-08, Critic Loss: 6.104590562854004\n",
      "2025-07-17 10:09:38.414133 Epoch 2990, Actor Loss: 2.363519915506913e-08, Critic Loss: 6.05821083483785\n",
      "2025-07-17 10:18:04.336805 Epoch 3000, Actor Loss: 2.351400579039667e-08, Critic Loss: 6.136330518611653\n",
      "Epoch 3000, Learning rate (Critic): 0.0001\n",
      "Epoch 3000, Learning rate (Actor): 0.0001\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:53:42.261245800Z",
     "start_time": "2025-06-26T23:43:59.529722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Pretraining\n",
    "# EPOCHS_FOR_PRETRAIN = int(BUFFER_CAPACITY / 5)\n",
    "\n",
    "EPOCHS_FOR_PRETRAIN = 1000000\n",
    "\n",
    "agent.pre_train(dir_path, EPOCHS_FOR_PRETRAIN, BATCH_SIZE, log_interval=10000)"
   ],
   "id": "a4f0b3cb7dd8c5bf",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# # Pretraining\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# EPOCHS_FOR_PRETRAIN = int(BUFFER_CAPACITY / 5)\u001B[39;00m\n\u001B[32m      4\u001B[39m EPOCHS_FOR_PRETRAIN = \u001B[32m1000000\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpre_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdir_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS_FOR_PRETRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 248\u001B[39m, in \u001B[36mAgent.pre_train\u001B[39m\u001B[34m(self, path, batch_size, epochs, log_interval)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;28mself\u001B[39m.critic_optimizer.zero_grad()\n\u001B[32m    247\u001B[39m loss_critic.backward()\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcritic_optimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[38;5;28mself\u001B[39m.critic_losses.append(loss_critic.item())\n\u001B[32m    251\u001B[39m \u001B[38;5;66;03m# 4) Delayed actor update\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001B[39m, in \u001B[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    138\u001B[39m opt = opt_ref()\n\u001B[32m    139\u001B[39m opt._opt_called = \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m140\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__get__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__class__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    488\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    489\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    490\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    491\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m493\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    496\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     89\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     90\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     93\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    232\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    234\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    235\u001B[39m         group,\n\u001B[32m    236\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    241\u001B[39m         state_steps,\n\u001B[32m    242\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m244\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    873\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    874\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m876\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    887\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    889\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    890\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    891\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\optim\\adam.py:619\u001B[39m, in \u001B[36m_multi_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[39m\n\u001B[32m    612\u001B[39m         device_grads = torch._foreach_add(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[32m    613\u001B[39m             device_grads, device_params, alpha=weight_decay\n\u001B[32m    614\u001B[39m         )\n\u001B[32m    616\u001B[39m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[32m    617\u001B[39m \u001B[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001B[39;00m\n\u001B[32m    618\u001B[39m \u001B[38;5;66;03m# tensors are on the same device\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_foreach_lerp_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_grads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_beta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    621\u001B[39m torch._foreach_mul_(device_exp_avg_sqs, beta2)\n\u001B[32m    623\u001B[39m \u001B[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001B[39;00m\n\u001B[32m    624\u001B[39m \u001B[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001B[39;00m\n\u001B[32m    625\u001B[39m \u001B[38;5;66;03m# as a result, separate out the value mul\u001B[39;00m\n\u001B[32m    626\u001B[39m \u001B[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving and loading the agent to make sure the agent has been stored",
   "id": "70b8ccd60f76155f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:27:09.465762Z",
     "start_time": "2025-07-17T14:27:09.403555Z"
    }
   },
   "cell_type": "code",
   "source": "filename_agent = agent.save(dir_path)",
   "id": "3c2a46a6186bb11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved successfully to C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:27:09.847786Z",
     "start_time": "2025-07-17T14:27:09.806117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent\n",
    "agent = Agent(\n",
    "    STATE_DIM,\n",
    "    ACTION_DIM,\n",
    "    ACTOR_LAYER_SIZES,\n",
    "    CRITIC_LAYER_SIZES,\n",
    "    BUFFER_CAPACITY,\n",
    "    ACTOR_LR,\n",
    "    CRITIC_LR,\n",
    "    SMOOTHING_STD,\n",
    "    NOISE_CLIP,\n",
    "    EXPLORATION_NOISE_STD,\n",
    "    GAMMA,\n",
    "    TAU,\n",
    "    MAX_ACTION,\n",
    "    POLICY_DELAY,\n",
    "    DEVICE\n",
    ")"
   ],
   "id": "867000e4048aaee9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:27:10.320493Z",
     "start_time": "2025-07-17T14:27:10.237195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.load(filename_agent)\n",
    "agent.replay_buffer = replay_buffer"
   ],
   "id": "8dedc170773c152e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the accuracy of the agent and compare it to the MPC actions",
   "id": "7dbf22273b26a139"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:27:11.497731Z",
     "start_time": "2025-07-17T14:27:11.489213Z"
    }
   },
   "cell_type": "code",
   "source": "print_accuracy(agent.replay_buffer, agent, n_samples=2, device=DEVICE)",
   "id": "88cd0cc887b9686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent r2 score for the predicted inputs compare to MPC inputs: 0.999998\n",
      "Agent r2 score for the predicted input 1 compare to MPC input 1: 0.999997\n",
      "Agent r2 score for the predicted input 1 compare to MPC input 2: 0.999999\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:27:12.423393Z",
     "start_time": "2025-07-17T14:27:12.420422Z"
    }
   },
   "cell_type": "code",
   "source": "min_max_dict",
   "id": "95b901ffc64f4111",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_max': array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
       "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
       "          6.2029936 ]),\n",
       " 'x_min': array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
       "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
       "          -28.25906582]),\n",
       " 'y_sp_min': array([-4.91766443, -4.61204935]),\n",
       " 'y_sp_max': array([5.00776949, 3.06512771]),\n",
       " 'u_max': array([9.96, 7.3 ]),\n",
       " 'u_min': array([-10. ,  -7.5])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8624775fd0aebdb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
