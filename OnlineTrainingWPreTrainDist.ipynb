{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:37.438006Z",
     "start_time": "2026-01-07T02:33:36.432139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from utils.helpers import *"
   ],
   "id": "833e2155a44bd6c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "8c565965fce1dc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:37.446266Z",
     "start_time": "2026-01-07T02:33:37.443464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "d168509011200b21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:37.453674Z",
     "start_time": "2026-01-07T02:33:37.451030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "ef60c6a882f064d1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:37.546716Z",
     "start_time": "2026-01-07T02:33:37.544532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "e0613ee1ad154d3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:37.866634Z",
     "start_time": "2026-01-07T02:33:37.864560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "aea9d86581d16b25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:38.123440Z",
     "start_time": "2026-01-07T02:33:38.120537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "1a7c69161bd5f8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "85287aecd00bcda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:38.545663Z",
     "start_time": "2026-01-07T02:33:38.543733Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "f8fbe1e84f3bb189",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:38.805278Z",
     "start_time": "2026-01-07T02:33:38.801755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "b12cb95ea94b6f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:39.033239Z",
     "start_time": "2026-01-07T02:33:39.031248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "7758bb35c5218c2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:39.292385Z",
     "start_time": "2026-01-07T02:33:39.290252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "c44dfc980e752980",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:39.546489Z",
     "start_time": "2026-01-07T02:33:39.544039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "88095c1c82c5ad36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:40.271638Z",
     "start_time": "2026-01-07T02:33:40.268433Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "59d7edf141de197",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:40.734582Z",
     "start_time": "2026-01-07T02:33:40.732325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "e7463ce513b8ba31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:41.040435Z",
     "start_time": "2026-01-07T02:33:41.037796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "f15067d59c9f2f52",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:41.446330Z",
     "start_time": "2026-01-07T02:33:41.443491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False, False, False, False]\n",
    "warm_start = 10\n",
    "ACTOR_FREEZE = 10 * set_points_len\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "c062e1a79f80912a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:43.874621Z",
     "start_time": "2026-01-07T02:33:43.857311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "85da68bee773cd7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "1daeca8ba3164a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:45.507137Z",
     "start_time": "2026-01-07T02:33:45.488380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TD3Agent.agent import TD3Agent\n",
    "import torch"
   ],
   "id": "49c428a23b3e48b2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:46.008702Z",
     "start_time": "2026-01-07T02:33:45.991691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "n_outputs = C_aug.shape[0]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 40000\n",
    "ACTOR_LR = 5e-5\n",
    "CRITIC_LR = 5e-4\n",
    "SMOOTHING_STD = 0.005\n",
    "NOISE_CLIP = 0.01\n",
    "# EXPLORATION_NOISE_STD = 0.01\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005 # 0.01\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 256\n",
    "STD_START = 0.02\n",
    "STD_END = 0.001\n",
    "STD_DECAY_RATE = 0.99992\n",
    "STD_DECAY_MODE = \"exp\""
   ],
   "id": "e798424baebbc371",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:48.005246Z",
     "start_time": "2026-01-07T02:33:46.977454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "td3_agent = TD3Agent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    actor_hidden=ACTOR_LAYER_SIZES,\n",
    "    critic_hidden=CRITIC_LAYER_SIZES,\n",
    "    gamma=GAMMA,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    max_action=MAX_ACTION,\n",
    "    tau=TAU,\n",
    "    std_start=STD_START,\n",
    "    std_end=STD_END,\n",
    "    std_decay_rate=STD_DECAY_RATE,\n",
    "    std_decay_mode=STD_DECAY_MODE,\n",
    "    buffer_size=BUFFER_CAPACITY,\n",
    "    device=DEVICE,\n",
    "    actor_freeze=ACTOR_FREEZE,\n",
    "    )"
   ],
   "id": "1d8ae390b2843fca",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:48.899671Z",
     "start_time": "2026-01-07T02:33:48.742693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "td3_agent.load(agent_path)"
   ],
   "id": "5d2d2b610564c69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MPC Initialization",
   "id": "fac3637c0ee4f291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:49.353386Z",
     "start_time": "2026-01-07T02:33:49.350845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1.\n",
    "R2_penalty = 1.\n",
    "Q_penalty = np.array([[Q1_penalty, 0], [0, Q2_penalty]])\n",
    "R_penalty = np.array([[R1_penalty, 0], [0, R2_penalty]])"
   ],
   "id": "211634f6f62d4c5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:49.728363Z",
     "start_time": "2026-01-07T02:33:49.726144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "7bf97648c044ad79",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying RL Agent on the CSTR",
   "id": "65a4fc461d9d0b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:50.622638Z",
     "start_time": "2026-01-07T02:33:50.616402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=5.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward with relative tracking bands.\n",
    "\n",
    "    data_min, data_max : arrays for [u_min..., y_min...], [u_max..., y_max...]\n",
    "    n_inputs           : number of inputs (so outputs start at index n_inputs)\n",
    "    k_rel              : per-output relative tolerance factors (same length as outputs)\n",
    "    band_floor_phys    : per-output minimum band in physical units\n",
    "    Q_diag, R_diag     : quadratic weights (same as before)\n",
    "    \"\"\"\n",
    "\n",
    "    data_min = np.asarray(data_min, float)\n",
    "    data_max = np.asarray(data_max, float)\n",
    "    dy = np.maximum(data_max[n_inputs:] - data_min[n_inputs:], 1e-12)  # phys range for each y\n",
    "\n",
    "    k_rel = np.asarray(k_rel, float)\n",
    "    band_floor_phys = np.asarray(band_floor_phys, float)\n",
    "    Q_diag = np.asarray(Q_diag, float)\n",
    "    R_diag = np.asarray(R_diag, float)\n",
    "\n",
    "    # floor in *scaled* coordinates (used if y_sp_phys is not provided)\n",
    "    band_floor_scaled = band_floor_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "    def _sigmoid(x):\n",
    "        x = np.clip(x, -60.0, 60.0)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _phi(z, kind=bonus_kind, k=bonus_k, p=bonus_p, c=bonus_c):\n",
    "        z = np.clip(z, 0.0, 1.0)\n",
    "        if kind == \"linear\":\n",
    "            return 1.0 - z\n",
    "        if kind == \"quadratic\":\n",
    "            return (1.0 - z) ** 2\n",
    "        if kind == \"exp\":\n",
    "            return (np.exp(-k * z) - np.exp(-k)) / (1.0 - np.exp(-k))\n",
    "        if kind == \"power\":\n",
    "            return 1.0 - np.power(z, p)\n",
    "        if kind == \"log\":\n",
    "            return np.log1p(c * (1.0 - z)) / np.log1p(c)\n",
    "        raise ValueError(\"unknown bonus kind\")\n",
    "\n",
    "    def reward_fn(e_scaled, du_scaled, y_sp_phys=None):\n",
    "        \"\"\"\n",
    "        e_scaled : output error in scaled deviation space  (same as before)\n",
    "        du_scaled: input move in scaled deviation space    (same as before)\n",
    "        y_sp_phys: current setpoint in *physical* units (array len = n_outputs)\n",
    "        \"\"\"\n",
    "\n",
    "        e_scaled = np.asarray(e_scaled, float)\n",
    "        du_scaled = np.asarray(du_scaled, float)\n",
    "\n",
    "        # ----- dynamic band based on setpoint -----\n",
    "        if y_sp_phys is None:\n",
    "            # fallback: just use the floor\n",
    "            band_scaled = band_floor_scaled\n",
    "        else:\n",
    "            y_sp_phys_arr = np.asarray(y_sp_phys, float)\n",
    "            # band_phys_i = max(k_rel_i * |y_sp_i|, band_floor_phys_i)\n",
    "            band_phys = np.maximum(k_rel * np.abs(y_sp_phys_arr), band_floor_phys)\n",
    "            band_scaled = band_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "        tau_scaled = tau_frac * band_scaled\n",
    "\n",
    "        # ----- inside/outside gate -----\n",
    "        abs_e = np.abs(e_scaled)\n",
    "        s_i = _sigmoid((band_scaled - abs_e) / np.maximum(tau_scaled, 1e-12))\n",
    "\n",
    "        if gate == \"prod\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64))\n",
    "        elif gate == \"mean\":\n",
    "            w_in = float(np.mean(s_i))\n",
    "        elif gate == \"geom\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64) ** (1.0 / len(s_i)))\n",
    "        else:\n",
    "            raise ValueError(\"gate must be 'prod'|'mean'|'geom'\")\n",
    "\n",
    "        # ----- core quadratic costs -----\n",
    "        err_quad = np.sum(Q_diag * (e_scaled ** 2))\n",
    "        err_eff = (1.0 - w_in) * err_quad + w_in * (lam_in * err_quad)\n",
    "        move = np.sum(R_diag * (du_scaled ** 2))\n",
    "\n",
    "        # ----- linear penalties around band edge -----\n",
    "        slope_at_edge = 2.0 * Q_diag * band_scaled\n",
    "\n",
    "        overflow = np.maximum(abs_e - band_scaled, 0.0)\n",
    "        lin_out = (1.0 - w_in) * np.sum(gamma_out * slope_at_edge * overflow)\n",
    "\n",
    "        inside_mag = np.minimum(abs_e, band_scaled)\n",
    "        lin_in = w_in * np.sum(gamma_in * slope_at_edge * inside_mag)\n",
    "\n",
    "        # ----- bonus near zero error -----\n",
    "        qb2 = Q_diag * (band_scaled ** 2)\n",
    "        z = abs_e / np.maximum(band_scaled, 1e-12)\n",
    "        phi = _phi(z)\n",
    "        bonus = w_in * beta * np.sum(qb2 * phi)\n",
    "\n",
    "        # ----- total reward -----\n",
    "        return -(err_eff + move + lin_out + lin_in) + bonus\n",
    "\n",
    "    params = dict(\n",
    "        k_rel=k_rel,\n",
    "        band_floor_phys=band_floor_phys,\n",
    "        band_floor_scaled=band_floor_scaled,\n",
    "        Q_diag=Q_diag,\n",
    "        R_diag=R_diag,\n",
    "        tau_frac=tau_frac,\n",
    "        gamma_out=gamma_out,\n",
    "        gamma_in=gamma_in,\n",
    "        beta=beta,\n",
    "        gate=gate,\n",
    "        lam_in=lam_in,\n",
    "        bonus_kind=bonus_kind,\n",
    "        bonus_k=bonus_k,\n",
    "        bonus_p=bonus_p,\n",
    "        bonus_c=bonus_c,\n",
    "    )\n",
    "    return params, reward_fn"
   ],
   "id": "317946f8d424ca6c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reward configuration",
   "id": "29803d0d4ebeff3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:53.822855Z",
     "start_time": "2026-01-07T02:33:53.819180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_inputs = 2\n",
    "\n",
    "dy = data_max[n_inputs:] - data_min[n_inputs:]\n",
    "y_sp_nom = 0.5 * (data_min[n_inputs:] + data_max[n_inputs:])\n",
    "\n",
    "k_rel = np.array([0.003, 0.0003])\n",
    "band_floor_phys = np.array([0.006, 0.07])\n",
    "\n",
    "band_phys = np.maximum(k_rel * np.abs(y_sp_nom), band_floor_phys)\n",
    "\n",
    "scale_factor = 1.0  # use 2.0 for [-1, 1] scaling, 1.0 for [0, 1]\n",
    "band_scaled = scale_factor * band_phys / dy\n",
    "\n",
    "q0 = 1.4\n",
    "Q_diag = q0 / np.maximum(band_scaled ** 2, 1e-12)\n",
    "\n",
    "print(\"dy:\", dy)\n",
    "print(\"y_sp_nom:\", y_sp_nom)\n",
    "print(\"band_phys:\", band_phys)\n",
    "print(\"band_scaled:\", band_scaled)\n",
    "print(\"Q_diag:\", Q_diag)"
   ],
   "id": "85575e2c60b10163",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: [0.22165278 0.78153727]\n",
      "y_sp_nom: [  3.83915067 323.21371982]\n",
      "band_phys: [0.01151745 0.09696412]\n",
      "band_scaled: [0.05196169 0.12406845]\n",
      "Q_diag: [518.51529284  90.95055189]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:54.996829Z",
     "start_time": "2026-01-07T02:33:54.993694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_diag = np.array([518., 90.])          # rounded from the band-based calculation\n",
    "R_diag = np.array([90., 90.])          # move cost for du_scaled ~ 0.02\n",
    "\n",
    "n_inputs = 2\n",
    "\n",
    "print(\"Band scaled are:\")\n",
    "\n",
    "params, reward_fn = make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=7.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    ")\n",
    "print(params)"
   ],
   "id": "599d7fb29af995d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band scaled are:\n",
      "{'k_rel': array([0.003 , 0.0003]), 'band_floor_phys': array([0.006, 0.07 ]), 'band_floor_scaled': array([0.02706937, 0.08956707]), 'Q_diag': array([518.,  90.]), 'R_diag': array([90., 90.]), 'tau_frac': 0.7, 'gamma_out': 0.5, 'gamma_in': 0.5, 'beta': 7.0, 'gate': 'geom', 'lam_in': 1.0, 'bonus_kind': 'exp', 'bonus_k': 12.0, 'bonus_p': 0.6, 'bonus_c': 20.0}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:56.743701Z",
     "start_time": "2026-01-07T02:33:56.741080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "829372b3f310f055",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:33:57.161801Z",
     "start_time": "2026-01-07T02:33:57.154396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rl_train(system, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 test_cycle,\n",
    "                 nominal_qi, nominal_qs, nominal_ha,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"disturb\"):\n",
    "\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "    u_min, u_max = min_max_dict[\"u_min\"], min_max_dict[\"u_max\"]\n",
    "\n",
    "    y_system = np.zeros((nFE + 1, n_outputs))\n",
    "    y_system[0, :] = system.current_output\n",
    "    u_rl = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    # xhatdhat[:, 0] = np.random.uniform(low=min_max_dict[\"x_min\"], high=min_max_dict[\"x_max\"])\n",
    "    rewards = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "\n",
    "    delta_y_storage = []\n",
    "\n",
    "    # ----- helper ------\n",
    "    def map_to_bounds(a, low, high):\n",
    "        return low + ((a + 1.0) / 2.0) * (high - low)\n",
    "\n",
    "    test = False\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # train/test phase\n",
    "        if i in test_train_dict:\n",
    "            test = test_train_dict[i]\n",
    "\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # ---- RL state (scaled) ----\n",
    "        current_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i], y_sp[i, :], scaled_current_input_dev)\n",
    "\n",
    "        # ---- TD3 action ----\n",
    "        if not test:\n",
    "            action = agent.take_action(current_rl_state, explore=(not test))\n",
    "        else:\n",
    "            action = agent.act_eval(current_rl_state)\n",
    "        # Map to bounds\n",
    "        u_scaled = map_to_bounds(action, u_min, u_max)\n",
    "\n",
    "        # scale & step plant\n",
    "        u_rl[i, :] = u_scaled + ss_scaled_inputs\n",
    "        u_plant = reverse_min_max(u_rl[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_rl[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record output\n",
    "        y_system[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_system[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_system[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_rl[i, :] - ss_scaled_inputs)) + np.dot(L, (y_prev_scaled - yhat[:, i])).T\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u, y_sp_phys)\n",
    "\n",
    "        # Record rewards and delta_y\n",
    "        rewards[i] = reward * 0.01\n",
    "        delta_y_storage.append(np.abs(delta_y))\n",
    "\n",
    "        # ----- Next state for TD3 -----\n",
    "        next_u_dev = u_rl[i, :] - ss_scaled_inputs\n",
    "        next_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i+1], y_sp[i, :], next_u_dev)\n",
    "\n",
    "        # Episode boundary (treat each setpoint block as an episode end)\n",
    "        # done = 1.0 if (i + 1) % boundary == 0 else 0.0\n",
    "        done = 0.0\n",
    "\n",
    "        # Buffer + train (skip if in test phase)\n",
    "        if not test:\n",
    "            agent.push(current_rl_state,\n",
    "                       action.astype(np.float32),\n",
    "                       float(reward),\n",
    "                       next_rl_state,\n",
    "                       float(done))\n",
    "            if i >= WARM_START:\n",
    "                _ = agent.train_step()  # returns loss or None\n",
    "\n",
    "        # diagnostics at sub-episode boundary\n",
    "        if i in sub_episodes_changes_dict:\n",
    "            avg_rewards.append(np.mean(rewards[max(0, i - time_in_sub_episodes + 1): i + 1]))\n",
    "            print('Sub_Episode:', sub_episodes_changes_dict[i], '| avg. reward:', avg_rewards[-1])\n",
    "            if hasattr(agent, \"_expl_sigma\"):\n",
    "                print('Exploration noise:', agent._expl_sigma)\n",
    "\n",
    "    # unscale to plant units for plotting\n",
    "    u_rl = reverse_min_max(u_rl, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha"
   ],
   "id": "c0b20eb3706f0e6e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:54:57.299055Z",
     "start_time": "2026-01-07T02:34:07.557980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "    = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 TEST_CYCLE,\n",
    "                 nominal_qi, nominal_qs, nominal_hA,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn)"
   ],
   "id": "eb10ecbf5180672d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 1 | avg. reward: -20.915602920324947\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -26.447110867549572\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -20.707880590916194\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.019523546931467\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.10909622675608\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -25.37481006259389\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.26070243454449\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.105362708554836\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.003397936185756\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.80032570459916\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.87914175136513\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.251931630107656\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -24.851392128436036\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -21.836725167518903\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.062547343638265\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -29.673105549627344\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.244489224964141\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.5508964893736086\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.4172888312401035\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.146516196646192\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.079195532741269\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.7139388881448396\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.495786883449045\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8261770038442517\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8151177890228474\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.831895610167133\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.866058712237097\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.7849276868908617\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7442461677857761\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.7971652198366326\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7888087114584992\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7738762060205449\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.805169104937035\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.812496658999741\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8035166576288477\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8194566372294987\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7978385873192846\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8724830904015233\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.863694054696669\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.830207589258081\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.8272622584409839\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7233884030921294\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7369987239827094\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.781535209384744\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7579428300695465\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.8392793319781184\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7614437811652741\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.796593592435796\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7511933603529446\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.745119395545141\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7928192106165477\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.777350888479981\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7788972463588453\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.806444188510185\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7941491884929877\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8286940451857145\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7949477431402783\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7857929979760507\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7777660876547214\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7830709919501901\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7993731164307725\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.8074281679468676\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8540332729750637\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.8783946760815615\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.9581511056691772\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8911410272678444\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8702048018827266\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8839868926168988\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8478899557361845\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.9026576365484402\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.9368697957908785\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.923081831923356\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.949298452983922\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.9803168716709236\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8656070507768578\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.8918529892677918\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.9168803379902721\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.9377982463392402\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.9183877410149952\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.8700160688713763\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8823785715643402\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.9107117311240986\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.9510104872080785\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.9707292192417765\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.9792427379574626\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.0630198788784893\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.0400208521475425\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -2.1183335006003987\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.9863229399147548\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8155249001436777\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.9066322157260907\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.9253381416039823\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.853239657812257\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.8471892979077718\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8525698510712554\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -2.3098005448978998\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.001184460631028\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.029512236537485\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -2.152769368977521\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -2.1383621493505176\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -2.2461349262088093\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -2.240528011724686\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -2.120319173517294\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -2.1692269755817843\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -2.039866699375351\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -2.104552027296738\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -2.1038443226740724\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -2.0603207128121603\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -2.1110952680216717\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -2.074965040477285\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -2.087648420924852\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -2.0826185929777368\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -2.0765592388004084\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -2.0113076448506364\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.9739640276005463\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.9110714435660787\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.8937616394919894\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.8848080046722897\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.9067137723629595\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.9187404656629732\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.8784570955711792\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8734204287793228\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.879200400679058\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.8792062957509155\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.8754441917927027\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.8766971992779191\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.8716600226935838\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.901135714313296\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.8629474460767295\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.8312210600972094\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.846565317733237\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.8585703560523081\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.8052032653887653\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7816678269723565\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7911029191964334\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8388109411344784\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.83639650738332\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.8300186034876753\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.851567484675828\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.852639078758496\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.8358365091632636\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7682380194428005\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7364539948612774\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7432464328321617\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7498001217217394\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.759538389082382\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7852177521306067\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7561039052469902\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7511479099598193\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7721387781056916\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7388647866007443\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7479380532301565\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.726668539180031\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7053446293453733\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7124620713244325\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.70576750202315\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7054192173483673\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.709420811689634\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.704201476206744\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.71856080093743\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6764751836598706\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7056081560165637\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.687181482576575\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.6937016606860862\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7016048019445256\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.6852085092695561\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6962126938772115\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6939748962627612\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.675455542166885\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6890684097181754\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.6948299175005903\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7250075445243462\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7181721408959298\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.798130769896695\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7744976755962563\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.8234692916177067\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.792455525407427\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.8398173298368743\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.8004036935866394\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.8347214050230614\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.794571605258833\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.8037619408632264\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7696061246055774\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7850474663940432\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7469176350900932\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7613863571684483\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7608102070539566\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7352035453132801\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.73326400858776\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.8973476175960207\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.9337489166290658\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.8310123531862563\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7934989832373085\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7604490818318919\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7971113553677345\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7548764984918257\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.708632889822544\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7208993038920712\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7306784166535794\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.703149304675633\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:59:31.635761Z",
     "start_time": "2026-01-07T02:59:31.617624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=None, prefix_name=\"agent_result\",\n",
    "    agent=None,\n",
    "    delta_y_storage=None,\n",
    "    rewards=None,\n",
    "    dist=None,\n",
    "    start_plot_idx=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation-style plotting (same colors/fonts/no legends).\n",
    "    Saves all figures + input_data.pkl to directory/prefix_name/<timestamp>.\n",
    "    Handles:\n",
    "      dist=None\n",
    "      dist=1D array\n",
    "      dist=dict with keys {\"qi\",\"qs\",\"ha\"}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name):\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, name), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    y_sp_original = np.array(y_sp, copy=True)\n",
    "\n",
    "    actor_losses = getattr(agent, \"actor_losses\", None) if agent is not None else None\n",
    "    critic_losses = getattr(agent, \"critic_losses\", None) if agent is not None else None\n",
    "    dy_arr = np.array(delta_y_storage) if delta_y_storage is not None else None\n",
    "    rewards_arr = np.array(rewards) if rewards is not None else None\n",
    "\n",
    "    input_data = {\n",
    "        \"y_sp\": y_sp_original,\n",
    "        \"steady_states\": steady_states,\n",
    "        \"nFE\": nFE,\n",
    "        \"delta_t\": delta_t,\n",
    "        \"time_in_sub_episodes\": time_in_sub_episodes,\n",
    "        \"y_mpc\": y_mpc,\n",
    "        \"u_mpc\": u_mpc,\n",
    "        \"avg_rewards\": avg_rewards,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"warm_start_plot\": warm_start_plot,\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "        \"delta_y_storage\": dy_arr,\n",
    "        \"rewards\": rewards_arr,\n",
    "        \"dist\": dist,\n",
    "        \"start_plot_idx\": start_plot_idx\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"input_data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(input_data, f)\n",
    "\n",
    "    # Canceling the deviation form (same logic)\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # Distillation-style rcParams (no bold globals; bold comes from \\mathbf in labels)\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    # Colors exactly like distillation code\n",
    "    C_QC = \"tab:green\"\n",
    "    C_QM = \"tab:orange\"\n",
    "    C_RW = \"tab:purple\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    warm_start_plot = np.atleast_1d(warm_start_plot) * delta_t\n",
    "    ws_end = float(warm_start_plot.max()) if warm_start_plot.size > 0 else 0.0\n",
    "\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1: outputs (full) --------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 0], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[0, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{\\eta}$ (L/g)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 1], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[1, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{T}$ (K)\", fontsize=18)\n",
    "    ax.set_xlabel(r\"$\\mathbf{Time}$ (hour)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95, bottom=0.12)\n",
    "    _savefig(\"fig_rl_outputs_full.png\")\n",
    "\n",
    "    # -------- last window --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{time_in_sub_episodes}.png\")\n",
    "\n",
    "    # -------- last 4x window --------\n",
    "    W4 = 4 * time_in_sub_episodes\n",
    "    time_plot_4w = np.linspace(0, W4 * delta_t, W4 + 1)\n",
    "\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[0, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[1, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{W4}.png\")\n",
    "\n",
    "    # -------- Plot 2: inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_QC, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_QM, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(\"fig_rl_inputs_full.png\")\n",
    "\n",
    "    # -------- Plot 3: reward per episode --------\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    xep = np.arange(1, len(avg_rewards) + 1)\n",
    "    plt.plot(xep, avg_rewards, \"o-\", lw=2.2, color=C_RW, zorder=2)\n",
    "    plt.ylabel(\"Avg. Reward\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    _savefig(\"fig_rl_rewards.png\")\n",
    "\n",
    "    # -------- optional losses --------\n",
    "    if actor_losses is not None and len(actor_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(actor_losses, lw=1.8, color=\"tab:blue\")\n",
    "        plt.ylabel(\"Actor Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_actor.png\")\n",
    "\n",
    "    if critic_losses is not None and len(critic_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(critic_losses, lw=1.8, color=\"tab:orange\")\n",
    "        plt.ylabel(\"Critic Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_critic.png\")\n",
    "\n",
    "    # -------- optional delta_y windows (no legend) --------\n",
    "    if dy_arr is not None and dy_arr.ndim == 2 and dy_arr.shape[1] >= 2:\n",
    "        n = dy_arr.shape[0]\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w = dy_arr[i0:n]\n",
    "        if len(w) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w[:, 0], c=\"r\")\n",
    "            plt.plot(w[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_last300.png\")\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w2 = dy_arr[j0:j1]\n",
    "        if len(w2) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w2[:, 0], c=\"r\")\n",
    "            plt.plot(w2[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_700_400.png\")\n",
    "\n",
    "    # -------- optional per-step rewards (no legend) --------\n",
    "    if rewards_arr is not None and rewards_arr.ndim == 1 and rewards_arr.size > 0:\n",
    "        n = rewards_arr.size\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w = rewards_arr[j0:j1]\n",
    "        if w.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w.size), w, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_700_400.png\")\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w2 = rewards_arr[i0:n]\n",
    "        if w2.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w2.size), w2, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_last300.png\")\n",
    "\n",
    "        plt.figure(figsize=(7.6, 4.2))\n",
    "        plt.scatter(range(rewards_arr.size), rewards_arr, s=10)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"rewards_all.png\")\n",
    "\n",
    "    # -------- disturbance (no legend) --------\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, dict) and all(k in dist for k in [\"qi\", \"qs\", \"ha\"]):\n",
    "            qi_arr = np.asarray(dist[\"qi\"]).squeeze()\n",
    "            qs_arr = np.asarray(dist[\"qs\"]).squeeze()\n",
    "            ha_arr = np.asarray(dist[\"ha\"]).squeeze()\n",
    "            n_al = min(nFE, qi_arr.shape[0], qs_arr.shape[0], ha_arr.shape[0])\n",
    "\n",
    "            def _dist_fig(t, q1, q2, hA, suffix):\n",
    "                plt.figure(figsize=(7.6, 6.2))\n",
    "\n",
    "                ax = plt.subplot(3, 1, 1)\n",
    "                ax.plot(t, q1, \"-\", lw=2, color=\"tab:blue\")\n",
    "                ax.set_ylabel(r\"$Q_i$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 2)\n",
    "                ax.plot(t, q2, \"-\", lw=2, color=\"tab:orange\")\n",
    "                ax.set_ylabel(r\"$Q_s$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 3)\n",
    "                ax.plot(t, hA, \"-\", lw=2, color=\"tab:green\")\n",
    "                ax.set_xlabel(\"Time (h)\")\n",
    "                ax.set_ylabel(r\"$h_a$ (J/Kh)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                plt.gcf().subplots_adjust(right=0.95, hspace=0.25)\n",
    "                _savefig(f\"fig_disturbances_{suffix}.png\")\n",
    "\n",
    "            _dist_fig(time_plot[:n_al], qi_arr[:n_al], qs_arr[:n_al], ha_arr[:n_al], suffix=\"full\")\n",
    "\n",
    "            if time_in_sub_episodes > 0:\n",
    "                W = min(time_in_sub_episodes, n_al)\n",
    "                t_lastW = np.linspace(0, W * delta_t, W, endpoint=False)\n",
    "                _dist_fig(\n",
    "                    t_lastW,\n",
    "                    qi_arr[n_al - W:n_al],\n",
    "                    qs_arr[n_al - W:n_al],\n",
    "                    ha_arr[n_al - W:n_al],\n",
    "                    suffix=f\"last{W}\"\n",
    "                )\n",
    "        else:\n",
    "            dist_arr = np.asarray(dist).squeeze()\n",
    "            n_al = min(nFE, dist_arr.shape[0])\n",
    "            plt.figure(figsize=(7.2, 4.2))\n",
    "            plt.plot(time_plot[start_plot_idx:n_al], dist_arr[start_plot_idx:n_al], lw=1.8, color=\"tab:blue\")\n",
    "            plt.ylabel(\"Disturbance\")\n",
    "            plt.xlabel(\"Time (h)\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"disturbance.png\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "f7a5be6d8440239a",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:59:36.887555Z",
     "start_time": "2026-01-07T02:59:33.305660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=dir_path, prefix_name=\"polymer_dist\",\n",
    "    agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "    dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    ")"
   ],
   "id": "9d08421eb40e999f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:18:30.276389Z",
     "start_time": "2026-01-07T04:53:42.700926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    set_points_number = int(C_aug.shape[0])\n",
    "    inputs_number = int(B_aug.shape[1])\n",
    "    STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "    ACTION_DIM = int(B_aug.shape[1])\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    BUFFER_CAPACITY = 40000\n",
    "    ACTOR_LR = 5e-5\n",
    "    CRITIC_LR = 5e-4\n",
    "    SMOOTHING_STD = 0.005\n",
    "    NOISE_CLIP = 0.01\n",
    "    # EXPLORATION_NOISE_STD = 0.01\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.005  # 0.01\n",
    "    MAX_ACTION = 1\n",
    "    POLICY_DELAY = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 256\n",
    "    STD_START = 0.02\n",
    "    STD_END = 0.001\n",
    "    STD_DECAY_RATE = 0.99992\n",
    "    STD_DECAY_MODE = \"exp\"\n",
    "    td3_agent = TD3Agent(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        actor_hidden=ACTOR_LAYER_SIZES,\n",
    "        critic_hidden=CRITIC_LAYER_SIZES,\n",
    "        gamma=GAMMA,\n",
    "        actor_lr=ACTOR_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        policy_delay=POLICY_DELAY,\n",
    "        target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "        noise_clip=NOISE_CLIP,\n",
    "        max_action=MAX_ACTION,\n",
    "        tau=TAU,\n",
    "        std_start=STD_START,\n",
    "        std_end=STD_END,\n",
    "        std_decay_rate=STD_DECAY_RATE,\n",
    "        std_decay_mode=STD_DECAY_MODE,\n",
    "        buffer_size=BUFFER_CAPACITY,\n",
    "        device=DEVICE,\n",
    "        actor_freeze=ACTOR_FREEZE,\n",
    "    )\n",
    "    agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "    td3_agent.load(agent_path)\n",
    "    cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "    y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "        = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                     steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                     L, data_min, data_max, warm_start,\n",
    "                     TEST_CYCLE,\n",
    "                     nominal_qi, nominal_qs, nominal_hA,\n",
    "                     qi_change, qs_change, ha_change,\n",
    "                     reward_fn)\n",
    "    out_dir = plot_rl_results_disturbance(\n",
    "        y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "        y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "        directory=dir_path, prefix_name=\"polymer_dist\",\n",
    "        agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "        dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    "    )"
   ],
   "id": "fdb4470b25c16864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -19.90139743378253\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.829715509777124\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.671085999123015\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.36343396116448\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.154432825655167\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.673948208365086\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -20.388195278753468\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -25.506620208334642\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -22.15842695206463\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.489542881822675\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.134248430029366\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -21.462919904136097\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -21.290941851832073\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.628154200767064\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.548749316237444\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -23.049969221674317\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -5.058384781407603\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.34902548890297\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.27314092219136\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.226773932912252\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.2638379488701257\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.2156993860764804\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9510903366583725\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.9138775181533532\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8110460917783144\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.7912556454083073\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8320307180429285\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.856476729893709\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.865892607079022\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.771482351896883\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8170969255116847\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7967434933552904\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8203521227876522\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.8061509799023134\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7557557786734197\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.766336670492331\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.774640217393727\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8063140580731107\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7833756435990358\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7733141657114506\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7974042041902902\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7819421658781476\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.8245521223994827\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.8140644915388588\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.771912299567403\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7717985529533382\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8054405049961018\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.830122558473887\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.8169881648383586\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.8332262076628332\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.8614870069109741\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8352612882612942\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7832747299377036\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.8363640498563263\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.8066121323373776\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8970664500664713\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8564221950252437\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.87570079513753\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.9270943415689594\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.9526742370581456\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.8888279011311753\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.9931619910008482\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.9016514733309196\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.9702851129081023\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.9463186207930085\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.850071591495274\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.922754874887674\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -2.04757341338635\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.2238092954210544\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -2.2645763975433386\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -2.017528783151919\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.9628708686923992\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.0461809344800255\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.1749421128355984\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.9785619747857965\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.9823120148765856\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.9798135410497542\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.968599221674774\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8673728806875105\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.95911328128046\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8712186497361956\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.9192234546106766\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8293223049059026\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.8546878103204734\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.8693051747275866\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.8055794671845637\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.8653593791700565\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.890607901183954\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.8999321864474945\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8514292711606668\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7997367140958418\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.8092082896176822\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7723466202297453\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7699560350186458\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8003257191766642\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.744641896255273\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7341977507545343\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.751692705093073\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7369096675327487\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.8026017589584904\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.8021838393159606\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7656745679954793\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7847256536281002\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.787445685963241\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.8425983887546282\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8236532241156795\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.8490759254775821\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.8760687540053924\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.8649359304099613\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7883474611140213\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7799948941443169\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.770968230231392\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7428169608383997\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7312253168631309\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7468918765808876\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7410948100278005\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7287988943224477\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7533232463257162\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7615826977014308\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7268367189250688\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7571149829289068\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7336192258878316\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7540678483893464\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7291452256716724\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7452729526284867\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7409962290967798\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7529279940543643\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7540041827976791\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.749658462394708\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7675514843434263\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7410156203260903\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7599665189300322\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.762924026451247\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7651962051359993\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7937915433144849\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7785763107875607\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7509766678647487\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.773639201347085\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7907147633270222\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.75041351971202\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7386927424636485\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7488598413659997\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.8022823117115672\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7567139128414675\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.783739170237053\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7723167942218532\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7756253957476278\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7563482022438917\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7646830327878018\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7325672038894544\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7565861184779026\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.750187667131654\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7313852751847032\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.720900078869622\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7468336025735716\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7449941737179473\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7073340389441694\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7062988135468078\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6923033858828347\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.697540184699758\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6956299265527548\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.6918659113067378\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.672045025394512\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7072480668154109\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7338738270833338\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7308046606315082\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7361507861899594\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7460710689916437\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7285262948866074\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.8678554448848297\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7819057877524056\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.793903580834518\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.8385228370512636\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7813811114016807\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7302888074461817\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7397907211116541\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7109943183645209\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7588782883073453\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7574409185921238\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7642674914554666\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.8288965401615593\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.8032942951800626\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -2.9056035980119894\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -2.0520987128051003\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.948426107280717\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.8931366458707624\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.9229059502069186\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.94697979898569\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.8841245317678754\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.869318241590181\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.8516887499787589\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.8299066044809889\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.790010128552544\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.8120538489952562\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.8461500820884305\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.8670345650857672\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.8540657484553436\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.827407220571776\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7741974222460535\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.761562020596021\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -23.25476172752958\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.820415305020887\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.567424959772435\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.007398216792716\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -26.726118758661396\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.650555959085683\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -21.673476433991492\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.335712702331943\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -22.935768147963273\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.381001963346797\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.51594462183799\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.206368282818243\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.461425758526776\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.075535765401344\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.483624221051972\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -18.57650320744541\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -5.079894022791072\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.8216340071479507\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.5624966058288265\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.1789018449920725\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.201590209475574\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.1299718862249675\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.014598741287929\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.9247820719854283\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.9624169290273137\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.9215081058586858\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.9039669886596684\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8446844444414603\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8378670654286156\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.81482526331459\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8303096295020571\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.831820918913719\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7547478205782654\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.780060047831131\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.809949706784252\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8306812519734166\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.753689214487647\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8241623620417335\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7859068721804447\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7790313851857713\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7406209101882626\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7283486333330693\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7663800259262115\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7677309335775027\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7441225083860044\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7654948231510843\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.739477988395734\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.735754407734054\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7872094667654383\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.750860626813363\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.8237417170188592\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8062252544721253\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.8046948232116091\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.8253411684625214\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.8059915652350917\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7633840018480418\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7601190342814836\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8017461461812758\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7905646815252345\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7878000968727294\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7737915759399965\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7610056018954088\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7769125397301724\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7835670624535822\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8013998664929989\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7614060846718156\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.77127931672604\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7958466782547327\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8321598486275559\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8724423724813033\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.870800221064536\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.9519965464415023\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.027228095385964\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.999227365640333\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.9522334602342892\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.9147163234683209\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.9758757461996759\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.8842740276445078\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.9739969509412367\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.1806751650632425\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.201691016082574\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.443369123047569\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.2107616455435037\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.0801547722910647\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -2.231435690538207\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.1504975136508766\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.495957352348526\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -2.26206318790127\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.197124948165973\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -2.304082159762567\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.019143915770564\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.9254363261729908\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.9903185389473372\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.959584948633354\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8877652688857216\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.8220177277189789\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.8482117584425606\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.8460081907830448\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8054383912546035\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.801386340314832\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.855792653565818\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.8597640112216272\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.789395667188799\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7744718766845404\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7730958301759938\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7990404970637826\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7714240316883922\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.8105046365704311\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.8217727553463925\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -2.069515915679024\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8481258476210802\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.8060401337182457\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.812251891329397\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8135890163710096\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.775123912755673\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7571699470052555\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7570240015195326\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.76749083795483\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7562095213205078\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.8060667428292054\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7945524207686037\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8095074370839546\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.8044236860288398\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7850868864783445\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7873489420722313\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7746860297815932\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7847849857574292\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.8057729782779393\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7735253854586837\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7717786361556478\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7852405700846825\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7751088175809415\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.781519263165818\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7755827335833754\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7787571519576517\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7817751244830107\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.784051507687784\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.775580736314893\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7973102802260406\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7805454914557948\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.8081836108016263\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7959267534819991\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7557771600000558\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7447527407473635\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7770232467625227\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7860221681368955\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7755355954913994\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7900585934528042\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7769060482931747\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7952900868388724\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7977565294244147\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7938858299660325\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7911323830311785\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.8542665834417826\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.8276125642276144\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7929243343599486\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.794306672085122\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.812824418274217\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.8386240035451165\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.8296320295090522\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.792468552115341\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7755566608822635\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7819052495950662\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.8056775737155037\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.8213304309365284\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.8441941940271398\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.835656951341727\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.8598418115732194\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.8465581927985366\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.8233407594742224\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.771503261492116\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7516044014905827\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7595586474484548\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.689721988084165\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7093538153660017\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.717080662349649\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6571971934505945\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6953856738863538\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.697992718502565\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7103394187939143\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7102195362491102\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.720695174744694\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7219608577154324\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7424230102942957\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7002002262551827\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7304918373316633\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.701316748532176\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7207870281523991\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7191715035366604\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7334045294617462\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7886156953891157\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7373932359436817\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7765470196331716\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.8322474823384078\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.77880531782353\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.771188889877363\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.8023199314767615\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.9236906206702071\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.9600742087222642\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.8462639761571265\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -22.28667065255624\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -25.216389408703666\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -21.667177425629475\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.037338241615124\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.4408232367449\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.474355065460983\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.806361168172863\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.256202839325347\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -21.84048138242126\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.49740956737996\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -21.276132935211287\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -21.31301020278871\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -21.962000205229526\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.342427804873942\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -20.78574710429486\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -21.93949745737686\n",
      "Exploration noise: 0.007823673866947569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\system_functions.py:61: RuntimeWarning: invalid value encountered in scalar power\n",
      "  CP = (2 * self.fi * kd * CI / kt) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 17 | avg. reward: -159.385252038862\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.6289173124812315\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.4022529030720787\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.543767170320726\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.0982674831898653\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9830305141443518\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.957608455605889\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.1831674521477273\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.018107999907357\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.103660669146851\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.9466381833901603\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.910692829668125\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.037033703362208\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.319265012846425\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.091589008727565\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -4.961089044656015\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.0367384238532433\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.5265218735197554\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.003545046311759\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.0238843936298414\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.9647343001963844\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8478811266586623\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.9923464287564827\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.8970643724040577\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.3700247085647352\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.1565952190596334\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.1232368709404668\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.2070434912437107\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.241483801331477\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.068323755589852\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.9397825488085807\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.9804014509323673\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.917675570189084\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.874433623158134\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.9223874806497492\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.9755691859254711\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.9306057897787208\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.8356529971565476\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.8002954947984051\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8890517184613322\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8752105606384923\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8386551366632216\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.870726903693271\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.8450769265722777\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7970905842300033\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.8758033025088594\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8279239686666933\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.9185809371018216\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.9383531812905892\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8548716959223146\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8008554571213664\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8861986123129537\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8678114092480078\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8760128408385381\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.9492188069123386\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8746546595174083\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8784684086056513\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.8675075548416915\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8436267933367292\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.9493868674874995\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.8386190294512341\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7928930935531573\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7860723907354379\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.796053657914011\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8416540370846333\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7640688973362593\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8358416049710757\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.8325305774063585\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7822324482151868\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.8051069924153365\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.8602565851589736\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.8185912622385743\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7625238890064931\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7578806608550133\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7194801612096688\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7409480652550928\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7574741395191529\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7186606120777719\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7387747064934052\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7496920299177925\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7513863258425326\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7568635752120574\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7465880593471792\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.69934336989202\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7338952351002308\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.725529485568835\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7353483499392683\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7327884491779708\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7843878202363066\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.744646373945355\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.6984532559748613\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.755930461843781\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7291616356117712\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7062129065164202\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7313736747871808\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.706433847059527\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7287232416696252\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7212830759091753\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7034885411612628\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.6983710767843905\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6920060688857501\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7748729683399336\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.705975627099194\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.6967440923292025\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7167317554901766\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.704855062051638\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7082139680889827\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7041905164107474\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7205614326794985\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7907505428440964\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7192620599906916\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.6987788592159818\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7126797879588023\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7406063786686314\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7428007609992398\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7358744813792766\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7355795725471024\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7172594147444562\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7025229282378966\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7381888882957872\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7254358364632398\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7357895355819801\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7226754919060192\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7066350480214505\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7491824061973091\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7030158004013238\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.74936710185844\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.733464478229575\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7053251193865946\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7196253546754656\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.735603495983313\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6917163046541923\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7028257387051307\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.6851126509725651\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6839658551231584\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.719248876647417\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.6895484755763608\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.6836769111984018\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.6825235494119632\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6892245596117896\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7019758235435847\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6972533337350422\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6940985538785436\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7166244446162024\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6807809073224942\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7801526497620568\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6838948308366708\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.6941450368595923\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.683169737224626\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.6842422337389007\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6989423604655536\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6973563399763134\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.6994187407491501\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6972136092069894\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.696995667789533\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6882823826971742\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.691337270790811\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7072608767969166\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7205587125278772\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6966805161837895\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7349249710082721\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6708290851733727\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6968086353546417\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.6749653120039434\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.667654442929412\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.669868104784082\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6551995813450662\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.6657995520319326\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.667793965093278\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6733220946217853\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6565590028620196\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.667273173729907\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6775340386729118\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.6721086068543303\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.6446931872934807\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.667568141175768\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.6788756296983485\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.675753694835387\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6898160090228367\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.673148798905154\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6631253047161911\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6982912108591564\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6730433553305801\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6634842131959366\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -23.26991809499541\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.025437269026664\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.68833890389268\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.49215930264864\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.214541365452348\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -24.195860163780708\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.718069780177938\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -27.346218060290738\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.32697348226065\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.053113555831274\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.834417437749543\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.445995349276604\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -21.859240677137105\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.294415408289186\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.165027639767732\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -23.824429227741465\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -7.399441525469658\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.6572356300446374\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.2311807329185696\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.158646999419765\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.0403913285590893\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9770965360523995\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9361810685493799\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.9066620352709902\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8792546404093329\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.91028568563661\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.769592932519181\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.7610747759434915\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7347066499925001\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.7465256251287955\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.81063189289398\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.79693597667124\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.810216099061247\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.791960928829284\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.761664155149461\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.807293479229441\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7554396732006794\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.793437087122056\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7831027749122008\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7009728836775806\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7566299956877272\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7135271209154939\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.8180971240580988\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7278588345402808\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7239976678617568\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7446008947831075\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7470292942129821\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7071055779088953\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7202741335626672\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7423705172977404\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7443015271698283\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7469086437270966\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7284899500434718\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7247146575330605\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7617610801780956\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7938675548757905\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7791052975560455\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8165570530634827\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7534615984808806\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.824289660790363\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7814341463892769\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.809254417470885\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7832444596524875\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.819267222489334\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.759578939685951\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7706396598653469\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.876386859426433\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7318021905470107\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.6962270262937154\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7130810248055428\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7295935257720267\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7635275127866066\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7524493604857627\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7394042243646146\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7939391667986806\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7656757801834895\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.8232403543348983\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7649768902086007\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8066270472315915\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.8068426583641404\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7732850977111025\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.8119639799705936\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7877821055381617\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7545098364466059\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7288660195225731\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7038256031561474\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.738063745837321\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.713674340065985\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7482053569682905\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7254259681380062\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.726957249066188\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7307706231311533\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7727891196082408\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.734884371625947\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7697794034276904\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.8402754892732451\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.9555273421402695\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.0451797114885037\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.9434582932778273\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -2.089747366768206\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.846008472958697\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.9633547584646782\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.9067920867217873\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.8615552836603073\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.9058521539991975\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.826788585657823\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.8157166216614946\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7872670986204418\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.8826498573753054\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7983830643075902\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8041535250961738\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.834340208044398\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.8266465393729618\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.905852457807912\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7986258205668018\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7941635510764558\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -2.8120505622398575\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7533140310813433\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7878204009896137\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7181398093734492\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7446246834674985\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7107170254035606\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7319607200059663\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7664507919941326\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.8433161319318139\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7428505373919316\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.732081232422986\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7215124024625774\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7169379379957947\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.6962760502536076\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7976053630116835\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.6694221060318304\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.6827525908764636\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7190012587631924\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.695270239937613\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.6805782181834874\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7150851595608083\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.6819697256546757\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.674010777073442\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.67580105174726\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.6997382684182936\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.69630219648136\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.6597489772277383\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.6766226572228795\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.67653663662082\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.6939605694754603\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.6682331037652176\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6704293767977254\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.673913104508693\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.6791722122563653\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6812067177029348\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.6838270718172081\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.660343145642564\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.6930583635643581\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.6810737228100276\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6936502892273473\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6633776364958925\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6817311397554944\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.673168004835021\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.6824795666709562\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7051050559559717\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.6977051264354577\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7251937813868818\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.707157057342464\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.6808728335789818\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.6794886243643719\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.661568708062656\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6914379212761121\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.662074251699012\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6912336620994488\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.665411973595003\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7092972538579299\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6687387003227212\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6756010025411934\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.881672870503896\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.693185527173887\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7161496644322711\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6816093738121571\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6798949926566127\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.688934810964973\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6668754352187245\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7131202953606357\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6915387392379346\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.8313936761708403\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7355719647679362\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7013744000617281\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7014208563941826\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.70668764442796\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6802143680171147\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7328757162315258\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.723268693785628\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6920729646759418\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.6893479693334053\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6829208469855814\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6750685737580546\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6605131644428786\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.657218083869314\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6594260704107073\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6728387761621013\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6473875829193534\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.64096494475386\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.314455237170787\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.557455392558527\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.17247088774754\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -21.602808872421086\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.956298209665384\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -25.029283516735\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.554180282981534\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.73051684155679\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.823344928871784\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.156241520693214\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -21.672945131505347\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.08559704528288\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -21.458798465565042\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.436298948330386\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -2447.7328225811243\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -14.316144454270956\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.3684212392695696\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.9579607684037668\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.605557888113643\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.6029644787001627\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.1618212627441555\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.2201978743896404\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.158149132822851\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.3304046071031634\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.286610196136116\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.3516717067091286\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.710986226972725\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.485457727792996\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.929328649144497\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.772036415266472\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.434589623367967\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.073051905157689\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.9910840305137436\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.0419251207865927\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.031924817745127\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.0467433632056014\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.9733799496512632\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.8460187015869929\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.9378130170507406\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.854228350986774\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.894317203834763\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.8620224100927272\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7389363054009175\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.8312813623256552\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.8084730307253658\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8892673035684553\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.8223727862275565\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.9945263575215577\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.904011044060947\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.0402814407814365\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.9875002175046494\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.9673763460670188\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.9160284357422592\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.105100455530409\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.137007581632373\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.0142873387524287\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.037014668662048\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.852078962781855\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.154786295971565\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.1229492774740866\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.083689912891565\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -2.150035393375404\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.67362293761734\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -2.2876823260630204\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -2.1185945552909122\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.977141135725632\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -2.1004065816964226\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.1892852019985245\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -2.685477968955774\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -2.364526107992864\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.0425604162917654\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.052493582884678\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.8916313038673294\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.934854246982554\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.981455147185854\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.0485507108819854\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.9495759093396452\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.9162767014446578\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.8462199419990606\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8494911408211647\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.849400484292778\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8192445178864665\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.8449554417127143\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.9106215013692562\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.9007996093710107\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.8554538442515303\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.8212800116487278\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.8492371950303288\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8719341839382815\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.9036268466956603\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.8444608435462246\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7995939829523337\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7865734228572046\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8243545083643569\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.891281065593361\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.203656302620735\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -5.153069689552734\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -2.450079120796227\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -3.164868922821041\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -3.047953069927815\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -2.4784741491975666\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -2.373960819229895\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -2.8280820976369347\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -2.1366108892473217\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -2.2526049957192114\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -2.2849724725969436\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.8014799056794644\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.9298398038888969\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.8456464687432896\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.820915793467016\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.8578377601907283\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.738551730662559\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7952618742787576\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7810553102447375\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.767070627453075\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7674002718102582\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7430715950478584\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7851352584959077\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.8058448659266104\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.8333311366582377\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8360388228368378\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.8761908557768185\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7593720984760393\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -3.4499928402238185\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.9199790080005215\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.8277830200990097\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.8450967519871841\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7577494134421952\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.8226924616008529\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.810888088068492\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7932058689777381\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.8168380920303002\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7915388071050864\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.8018602174611686\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8307585309198857\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.832022473576826\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.8570220590704247\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.866682582682472\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.756041228813087\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.757264908240528\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.718001590457904\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.754840555944777\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7437486803441424\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7235808724037731\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.718891897861893\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7600275799613427\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7479067596012066\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7297051913894617\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7554484177276617\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7418876105644896\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7391076894437902\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7746526470815758\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7834052405813656\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7524551985738999\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7762517891734941\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.8031346769585708\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.8233936372081831\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.846476669112491\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.8824238542202187\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.8963846055305107\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.9308401775182187\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -2.221737576013835\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.8289568711393442\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.8161670745817247\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.8633885743126717\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.895454371868484\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.8456235814106037\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.8291998567242547\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.9000821710490292\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.9721485879994969\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.8463594592730812\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.8286202507769105\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.8037690884799582\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.8299033110792908\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.8254189591406476\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7973807054132456\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7763719549953811\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.873771628941175\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7416335307175899\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7435113746614197\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7590709928672164\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6953618330972744\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.730437359157707\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7264603922197006\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7407153431648477\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.760848532596209\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7879591666133134\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.955205979101089\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.716965510947996\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7091535461373126\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7282887675541467\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7323825817033605\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.810182619278672\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7599173169007924\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.728495589419512\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7872388924916935\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7741828811627653\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7159826642721157\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.717321957641156\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.054949399190814\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.6631193102141\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.186602142643302\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -20.760181371054195\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.57025955314559\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -25.173691488866876\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.632883496013548\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -20.95678681563587\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -21.669986254804904\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -21.276949223713242\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.29605588327995\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -21.82022344934173\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -19.964239959053753\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.028585699716\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.898280873919283\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -3277.5632889856706\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2524.12368614226\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -962.4171216177483\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1103.7539033168528\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -1268.555622871506\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1034.054265842082\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1083.6070246407294\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1003.4737839992723\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -908.6233651454481\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -934.149598749646\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -847.0163306392617\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -802.8409545309202\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -29.247804491828656\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -18.473637044212662\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -26.11009922860334\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -6.371846022909527\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -18.244671130186916\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -11.33757006444778\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -7.0794732394219775\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -6.127255947872414\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -3.7324165114238395\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.835613669670512\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -3.2743048612474093\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -4.582211962189876\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -3.4124060804664658\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -4.134227757424117\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -3.898525960457553\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -3.6093337160482566\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -3.7488680480391565\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -3.545886980094215\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -3.1882222676442313\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.931101195365554\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.5140820663780454\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.425362430906424\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -2.491921765437428\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.750840611687267\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -2.594695836648145\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.5757043343549526\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -2.328306483826543\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.422275684980808\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.5805852938469216\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.69425264625935\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.9814943465337986\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.7486183356973295\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.8708057724820413\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.841549186539106\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.949169401968285\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -3.206423806924852\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -3.122356176435121\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -3.351940730869524\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -3.6666568288283417\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -3.4786813868461217\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -3.6307047600687383\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -3.9565364101930474\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -3.9237567505585447\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -3.9945026786823776\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -3.5877906390871654\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -4.036811895247563\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -3.756727842979833\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -3.704983572381543\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -3.512559389396658\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -3.387702024665498\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.969817110038838\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -3.126957447395978\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.4846533738288827\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.142420597544953\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.0254050043013176\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.076149456174785\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.957206023480594\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.9838146155292813\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.9287219664406803\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.0158546704176983\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.9371879790551245\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.019533776035874\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -2.085853306597206\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.057624768044414\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.0816828028404633\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -2.1155095649962403\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -2.07701488230747\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -2.1908411150409597\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -2.238086702536594\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.107244197672426\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.0411262529327194\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -2.002878405498266\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.9421283502353808\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -2.093078753141275\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.9928332106612279\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -2.002280076970686\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -2.072165515701174\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.999526409177411\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -2.019506345596458\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -2.0128321570413577\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -2.061014215185697\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -2.0297543424086526\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -2.0135701409158586\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -2.0423141004942584\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -2.0028452479812473\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -2.016229556680201\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -2.0664270971294223\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -2.0315367418771353\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -2.049219396757111\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -2.0252559425832777\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -2.048554697181398\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -2.0558075390183275\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -2.0871138991515763\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -2.052096712638558\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -2.1243468552196223\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -2.0587317590291705\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -2.0561636306525113\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -2.1086360838044134\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -2.0644038391564528\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -2.0363129267807483\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.9920454565066672\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.9747470918198065\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.9413583729319202\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.9489221976091893\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.920563733722466\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.987429170334621\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.8836317011619064\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.9218587298350187\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8876771481782202\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.8827338380096978\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.8951970579294144\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.9136891618511636\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.9479795098308528\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -2.0175970881393837\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -3.0891254937028765\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -2.0222569552913554\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.936622001586552\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.897228982289476\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.9241077155785604\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.8665066391699694\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.8349102500925398\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.8476339211157766\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.8151274952275411\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.8032084252103595\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.8181180426477568\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.870819737632912\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.8640053791433469\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7761359968006227\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.824063618127085\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7951324122924615\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7969317428312792\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7980103040031359\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7766546174733915\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.778328719913845\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.8045523497511207\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.797047990593685\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.8121036989847403\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.8162091732763852\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.8533065847844663\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.8321776861479133\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.8550022044133179\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.824275686904379\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7334537873325189\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.77396831447232\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7825918539688508\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7588159872058833\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7627239572952227\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7649863836951647\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.766550141137987\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7290428713972148\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.8038860303722988\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7840634297693259\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.774150349338472\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7526746026329234\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7843883258685396\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7283156233333345\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7569977749881014\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.769215344891224\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7755887450405066\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.760718125699584\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7787346989632695\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7450059349121643\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7420437327230849\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7529116741254969\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7575930491583796\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.8481688209481695\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7989240738035008\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.8834087234682226\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.9274746722371177\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.9574347143485074\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.862138087411932\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.8706028976533946\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.8507063929158551\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -17.149280422516412\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -25.145330385672555\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -21.315016571289426\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.598087411810862\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -26.04646487778034\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -24.197991604192133\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -21.504085217461235\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.77326758114147\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.27814573504187\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -24.87959905247224\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.324500203704865\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.980472955157037\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.559045479745144\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.404112548476228\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -24.06991506606606\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -24.424991021160903\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.719155561637734\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.376522040272912\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.21456217806419\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.1221873200433055\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.0177259674849055\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.065569752161739\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.091969338587848\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8583063553970811\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.7989280670180734\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8029574561389534\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.772644514344633\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8011030971546145\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7904447584924583\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8074626268799774\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.830158155968203\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.847960451266287\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8309336612322433\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7687427704873802\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8311542354934347\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8434809159582364\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.8451126565949334\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7878446594354778\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7571802888707062\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7950297268928592\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.8378940880480366\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7978541138350261\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.8513858948571555\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.8144553542482273\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.8017062653182387\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.8227815022156957\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.766197583713161\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7777355936102077\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.8220986194280335\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.811151748518646\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.8488480958538165\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8135899777015947\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7982501546394958\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.8523226600952831\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7766487527235983\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7992578401767025\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7645797695953014\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7845808643454901\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7510116690679647\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.80646162770648\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7945009362963802\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7612857908411466\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7736505669346536\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7300911231261844\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7566570851603662\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7978143344562563\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.6996760089434275\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7731399763458295\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7389291376600988\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.758013144577086\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7690164594935716\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7660683159157207\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7530861290407376\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7649381243580273\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.807032503210461\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.8231869348930037\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.8540086914759764\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.8122257819137007\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.794921691077118\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7145579429637245\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7408144212061396\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.764596488722553\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7110990935088355\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7372588610474786\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7730206298478697\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7316552355216543\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7185817308637952\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.774860320280092\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7433469512356397\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7330277926836124\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7885026658467411\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7555568629988136\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7259074043814604\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.748029685865256\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.738784072219623\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7611566483146828\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7432109853801472\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7251917182393013\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7629522181425035\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7618896982512517\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7630111996897324\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7384085263905618\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.686126253842379\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7191973745823068\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.6901582874928807\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.716471862012929\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7267207050628794\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.6906693623653835\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7089350334521174\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.6797767096273661\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.706749319548577\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.6987720082354476\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.6941594173363095\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7022854653976511\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.6772745672284497\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7114131761843459\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6849133035036266\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7056075756507116\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7023859566902808\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.6848685072252283\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.730787951927493\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.6970038258713596\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.6975310778636288\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.6689803532575473\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6766578004853097\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.6624636745410317\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6971843469916883\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.6840309386859207\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7178878805295352\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -2.5743172629606925\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7158151928341687\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7153385095669602\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7062992385635396\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7221341999597883\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7326152015419347\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7191054321525012\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.9728505901693465\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7174208418985137\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.6957563151703625\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.6833329452437067\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7105311779889172\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.6833761553064488\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7500077452375422\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7129416943578577\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.675146215080348\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7158321980765283\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.730170253230018\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7582133819252648\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7022775468873013\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7214392839284887\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.696724810153426\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.6759767574782447\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.687864678432337\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7092095480212448\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7104361660601297\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6830270056606873\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6969569560352844\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6880268470701796\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7004376937785906\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.6903237791058963\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7134586084817056\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7117400863937124\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6986832886001486\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.6707391426256128\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.709248084812808\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.672958970114077\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6856697060073749\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7302988832985284\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.697120176143165\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6973244478100695\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.6813280790479224\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6799573582942662\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6740249795904043\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7045287834269334\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.6980900993433021\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6774006568603739\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6714146241628982\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.681934112221887\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6898818320823294\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.6922166785785522\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.682049329916709\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.683207729346945\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6934115657540725\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7068130207349748\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7028508126138049\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6904943544433888\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6939453384744423\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6790283159832586\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6719649699073253\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.6782300122561902\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.668293698524987\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.662253370328152\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.6881711476877037\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6585928334034057\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6578473927698456\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6594781923506985\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.62838240243724\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.655899533271544\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6591932543267398\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6136522238872897\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -26.271323416397617\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.462642431474954\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -22.081828149954443\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.01834733451191\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.196377780564447\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.821202423766263\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.30998302879682\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.47136393089734\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -20.91950094442773\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -21.478677715700933\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.32667555476498\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.69545522607023\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.837649099558075\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.406793574719664\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.446781255629475\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -21.468366055289042\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.9997941093030476\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.0064295371039047\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.7570323108915638\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.6381053807242836\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.4452337208183716\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.243934158364299\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.2532087300705457\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.2676387201057633\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.338226000505119\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.200206790222466\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.2784896772938223\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.2396911867425437\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.0618447161677627\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.9367881785131749\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.9720654159313529\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.2017846386954933\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.0656169652389837\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.00153325389071\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.9200107911162683\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.011354033075905\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.8985116594297429\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.9251643706117563\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.822024725961349\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.779462489361764\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7921831457632893\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7957427332060139\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.835156650150883\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.785949845414608\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.83163720044373\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.8150061785343086\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8649462295565922\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.8519218436208866\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.8104227747772184\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.850096137897043\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.819028929217014\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7786859234107062\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7212263036983046\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7723518572346457\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7764637172620792\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7641359567522874\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7480447627360156\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7357243566104756\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.790421253883975\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.759970013546996\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.8077066685502994\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.8794975534933471\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8415057523954637\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.91526131750857\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8522878836204626\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8307801738146776\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.913257628546397\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8527937073145313\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.9353191939464938\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.9202272872206776\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8866406000959646\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.9522226808170893\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.9509918315233716\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.9261019080347226\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.9135989680753476\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.899674695895706\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.0310196710286736\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.2339088456512655\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -2.1214223525852436\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.992998924708973\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.9780817496852818\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.057591623248935\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.0460247897890276\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.037238048416008\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.8919097374501672\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.879860226059481\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.8484756773652509\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.8456618548550865\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7505990836087717\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7901417787056715\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.8489548446323654\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7486215728107128\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7723190091634842\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7982680526977097\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7813408342907433\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.9148136559857452\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.8481930167474732\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.8534909745451966\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8766578955800117\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.90171213284334\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.8838985249599682\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.8938421154925413\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.8745141506310525\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.8379830393914525\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.8792037372744113\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8670486310309884\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.8880979867899657\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.9057684567827409\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.8321373000959253\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7976034793799578\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8228720896911488\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.8295177623420218\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.831225099313193\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8317132444947588\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.819198575715251\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.8504305236996337\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.8362926496906005\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.8501672017300077\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.8351992974252613\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.797532048790734\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.805220744663907\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8183781206014986\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.8071261215738184\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.786842612523722\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.804512091556428\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7936412813319271\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.804258412975214\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.8273607388362898\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.8014706089529733\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.812637727273173\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.766744502481584\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7591869993219273\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7437880958146765\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.749999595661687\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7162969976653237\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7103081347506235\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.735318911533946\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.778428512055928\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7431197447270321\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7730735488152976\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7534813731926409\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7613386357345096\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.752593327865457\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7604085949279067\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7242192331892128\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.719938353165581\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7017192279986337\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6835613598373644\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7030691716226398\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7302253794894489\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6954856905762983\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7353927993295142\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7645723092255556\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7278552771958358\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.6885862891392585\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6923611860514796\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6929440498175674\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7072294404065593\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6752268503733487\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.718710958479221\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.70097387799468\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.687671523551034\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6797174198878144\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7002234091546133\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.683430639001502\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7179079240204975\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6752097959985945\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6914683030270965\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.6817320806997207\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.702056828112414\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.6716755174381768\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6640256626475385\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6747720484193969\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6812198314142224\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.694907707493111\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.677150216083887\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6699362800437967\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6797941590217746\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6787645331468408\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7049611353316154\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6685890654156197\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.6804574180496552\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6909740987143627\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.6814648132566827\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.6752076023621743\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7688984652958684\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6924096020829291\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6936279724205123\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6942587474911186\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.6949911748802153\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7042182154438366\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7078111549562198\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7033039633317815\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6946660195012655\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7063177853200022\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6901875221971971\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7063497806188044\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7445425257990963\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6853569339250958\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.634305555772217\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.16451578861706\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.94667839933587\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.564687118095854\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.920460160754327\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.379023718583284\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -26.370987946368086\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.708067190547236\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.60675111058629\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -22.639196498684576\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.585831970345122\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.479998847568694\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.082268824917765\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -21.801443532908014\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.150960468796885\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.509523699181035\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -27.69453923897774\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.7819705337480656\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.3888141009348747\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.4952887252255254\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.4726927651869355\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.3059275920012414\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.7722834489262869\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.8395629150745927\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.853455992323645\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8458944946374027\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.848391365218455\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8274862982094118\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8887466893920501\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8695798864864015\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8193732721095028\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8077208481508007\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7957218045211143\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7749754066126164\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.848404081440569\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8846010194472103\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8779342270488961\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.821981098284927\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8039644153926528\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.8135233545309088\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7040735417009083\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.801418751575253\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7801773924245594\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7878924129273543\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7902058572986386\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7986049249011637\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.82188292541166\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.788309435691084\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.8002903153767806\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.8140612219369268\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.77635329869185\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7697130764167253\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7640145230428186\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7685130302152259\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7354726900253343\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7900690744322947\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7835234685721275\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7851316299943596\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.775129288731631\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7504390540538228\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7791697404366271\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.8683243454364244\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.800308206126311\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.759761772869902\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7637076859966203\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8610233185445537\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8096577304220813\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7736482306497217\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7806494903777048\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8136574808814985\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8201205710759303\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.781614140757179\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8717814636777248\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7728358659094845\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.87771496572611\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8358034634776812\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.8139228651225687\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.756468971984583\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.813260208200257\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8146262910872384\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.8262069817646969\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8227339623590726\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.8362891785576898\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8110018166124795\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7824279540523427\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.8170219347957164\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7062927328330146\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7009194694379943\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.6977450220900547\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.711223118741173\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7538155384019445\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7426256300779495\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7402746009073309\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7282010505195626\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.722504097361084\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7540125962808224\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.8076232519686513\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7604647135350555\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.818790852610512\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.861563901066798\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.9697750223929558\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.766230475884367\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7383133526974104\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7076256888934092\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7267739787873466\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.716264192506992\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7206302527312964\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7357686517691246\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7511257368590003\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.715285529979934\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7668952326240037\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7459418978483614\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7468032944999896\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7895673099334344\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8004347958740436\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.80013174342559\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.744261260394611\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.739478521395741\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.712853441490177\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7279051536070904\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7224451520439814\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7057141116311443\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7113514038288187\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7031484489847701\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7109831980362957\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.732510747802883\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.73497963381733\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.748639907354815\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7618175413187962\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7618646525531334\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7575611147799912\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7550940041006067\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7761267026116465\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7696905885708258\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7760382036067466\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.790350581843084\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7652127426797295\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7786654793276258\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7812953719565199\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7779177540413094\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.8322341596570937\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.844152032513038\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.833347009296657\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.8464290844604556\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.8770123429144334\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.8811323637026058\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.9750928643525756\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -2.1127041549717624\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.9470838279516574\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.9712104093321847\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.9658585881307673\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -2.038489520051328\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -2.051227328043042\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -2.0116288058031806\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -2.088112046277635\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -2.000803936336476\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -2.05784051825965\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -2.144119739503339\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -2.097703100371477\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -2.0699975468074387\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.9408250931827349\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.9103709806124443\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.8643235238378384\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.8604067056030045\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.8371225113425202\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.793054821210228\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.8108413059124115\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.8206899899875624\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.8301580508687851\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.841905071314664\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.8704322805669529\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.826418250137935\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.8059851797429973\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.8000856679164412\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.8295891087768716\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7711952828853308\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.9739504761766822\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.8758216999035813\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -2.3006059030637767\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.8402363500028962\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.992809308287616\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.9962384330857652\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.983125006692062\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.8523185912027287\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.8909249097267098\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.8853801399780912\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.851094268700042\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7733345028827017\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.8348047650747819\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7996372228417326\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7868017871594315\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7931095408835056\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7894382555160206\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.776842166784313\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7697269845680534\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7515774032438356\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.747767206835635\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7102569980704527\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7264358329040856\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7041306892956476\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6992402416049197\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -24.7946280165582\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -19.60965910789215\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.767370562117723\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.69036258729264\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.641360177828883\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.905246287480786\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -25.265990763693402\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.508094041683485\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -20.252213507653995\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.765787186618855\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.710342434002996\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.0158341377557\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.890513295539787\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.06155476046886\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.442017560006207\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -3707.891902811405\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -894.1647181085032\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.8743402407287313\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -8.738004802260305\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -4.599937768267024\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -3.461583933433707\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -3.4335535142170137\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -4.025895048359925\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -3.2242905251425054\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -3.215711117676404\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.9690786826067908\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.773000225518262\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.628013223539995\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.2764865847791045\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.288253560724606\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.241543111672336\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -4.377314150728094\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8739429801220973\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.0906004279966224\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.1220853346425357\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.044184440316542\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.3483009929959353\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.072178728822857\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.2027878759842094\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.3568993117566235\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.9166460676717678\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.9105698180565596\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.871742142727353\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.0234432867043677\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.9813286060755408\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.117309781106893\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.947700682159467\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.164584579467319\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.0786346185519506\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.8456506878472982\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.033428940298479\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.9554219388061416\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.9551158947487368\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.9657008320556537\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.9573065098423994\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.9598613315299065\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8829401157740733\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.018711079026202\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8339618786142264\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.9756612141004777\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.0307393000586256\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.031384365395089\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.9949120446480242\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.9215611837623685\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.975480976763389\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -2.058856826237002\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -2.171749949342711\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.9609811679237146\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.218271095265347\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -2.2232176342657417\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -2.217602608405873\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.0160851675804\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.230411921357446\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.1695109039225606\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -2.473636928633137\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -2.087788021329352\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.0692062932563418\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.9819569017558936\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -2.0295998174575227\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.1979116561498273\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.01162929593183\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.033546805114735\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.9958475522206862\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.92442095122875\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.9925307220034008\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.905999303938814\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.9548655686932124\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.8644790600248542\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.8203702184077366\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8294735697440985\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7978919450647062\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.842870069863163\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.9369533545302784\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.9437807205728779\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -2.027238780703572\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -2.0071028816343413\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.004322467303165\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.025610440809318\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.945640425321796\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.9621765705211502\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.9720656162084111\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.9032170823646646\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.8973326368036003\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.9396619944059321\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.8160633189201498\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8102271823214295\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.867481345200747\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.8745870169012204\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.8694737025972035\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.9079115694170685\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8471543329217246\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.834651237401626\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.8591657018626728\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8449112821382179\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.9493937227642977\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.8932993898024073\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.9742420225533561\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -2.7792816082273557\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.8233808749171607\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7682779243244664\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7261976109199386\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7115323590513907\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7041429672375583\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.708563389304601\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7040346743455452\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7216582806542204\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.728347490039293\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7712881795979554\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.771344490875954\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7372585433668124\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7806054463245204\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7517583642430252\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7372621044024215\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7553840509490066\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7394901488702077\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.850186294005291\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.84019029854631\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7472005465310627\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7413449949157374\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.724899397326822\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7207672600646493\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7496011038852197\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7386854794170465\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.729536608926366\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7035351907491603\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7007736320699942\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.8037969106504794\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6908987002444724\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.6915144083301772\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.6949109676419492\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7133307321112223\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7139074194067938\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7112129023067397\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7447215179553268\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.711672688404636\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7161886760651972\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7159826152735058\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7162276745581182\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7075556907925273\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7291091844847828\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.735051959855249\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.719246272759242\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.793418359576465\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7495382575242393\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7804493105334223\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7271551158249863\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.723196269000277\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7089524691988998\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7221195426560547\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7229303634686906\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7089558798994455\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7324274283125356\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.719554580527202\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.69272155968346\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7331170276525614\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7263818900612784\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6980726538227366\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.741001609361411\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7233974262012401\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7108811365491352\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6892384497387738\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.6826008310490375\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6617999344449663\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.6842128191916863\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.6904608189426198\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.700966128366893\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6914251201738242\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6721140076929208\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6656743209789044\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7042201064567069\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.697112305857601\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6884780192095212\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7001337187315153\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7161214181230173\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7001722866289288\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.686096780940259\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6736329714130789\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6803786703829453\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.672932068169236\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6452266803134388\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9da9d7eba75b6d78"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
