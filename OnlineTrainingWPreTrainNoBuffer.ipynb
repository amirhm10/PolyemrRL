{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.765113Z",
     "start_time": "2026-01-07T04:57:16.607146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from utils.helpers import *"
   ],
   "id": "833e2155a44bd6c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "8c565965fce1dc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.774091Z",
     "start_time": "2026-01-07T04:57:17.770214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "d168509011200b21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.784638Z",
     "start_time": "2026-01-07T04:57:17.781690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "ef60c6a882f064d1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.792869Z",
     "start_time": "2026-01-07T04:57:17.788831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "e0613ee1ad154d3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.803839Z",
     "start_time": "2026-01-07T04:57:17.801147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "aea9d86581d16b25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.811856Z",
     "start_time": "2026-01-07T04:57:17.809158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "1a7c69161bd5f8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "85287aecd00bcda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.818843Z",
     "start_time": "2026-01-07T04:57:17.816344Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "f8fbe1e84f3bb189",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.827783Z",
     "start_time": "2026-01-07T04:57:17.823247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "b12cb95ea94b6f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:17.852328Z",
     "start_time": "2026-01-07T04:57:17.849279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "7758bb35c5218c2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:18.100300Z",
     "start_time": "2026-01-07T04:57:18.092959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "c44dfc980e752980",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "88095c1c82c5ad36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:18.656238Z",
     "start_time": "2026-01-07T04:57:18.653413Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "59d7edf141de197",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:19.058510Z",
     "start_time": "2026-01-07T04:57:19.056460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "e7463ce513b8ba31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:19.433839Z",
     "start_time": "2026-01-07T04:57:19.431263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "f15067d59c9f2f52",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:19.794071Z",
     "start_time": "2026-01-07T04:57:19.789969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False, False, False, False]\n",
    "warm_start = 10\n",
    "ACTOR_FREEZE = 10 * set_points_len\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "c062e1a79f80912a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:20.246085Z",
     "start_time": "2026-01-07T04:57:20.229927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "85da68bee773cd7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "1daeca8ba3164a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:22.652738Z",
     "start_time": "2026-01-07T04:57:21.041921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TD3Agent.agent import TD3Agent\n",
    "import torch"
   ],
   "id": "49c428a23b3e48b2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:22.768736Z",
     "start_time": "2026-01-07T04:57:22.749493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "n_outputs = C_aug.shape[0]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 40000\n",
    "ACTOR_LR = 5e-5\n",
    "CRITIC_LR = 5e-4\n",
    "SMOOTHING_STD = 0.005\n",
    "NOISE_CLIP = 0.01\n",
    "# EXPLORATION_NOISE_STD = 0.01\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005 # 0.01\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 256\n",
    "STD_START = 0.02\n",
    "STD_END = 0.001\n",
    "STD_DECAY_RATE = 0.99992\n",
    "STD_DECAY_MODE = \"exp\""
   ],
   "id": "e798424baebbc371",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:24.012785Z",
     "start_time": "2026-01-07T04:57:22.794578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "td3_agent = TD3Agent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    actor_hidden=ACTOR_LAYER_SIZES,\n",
    "    critic_hidden=CRITIC_LAYER_SIZES,\n",
    "    gamma=GAMMA,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    max_action=MAX_ACTION,\n",
    "    tau=TAU,\n",
    "    std_start=STD_START,\n",
    "    std_end=STD_END,\n",
    "    std_decay_rate=STD_DECAY_RATE,\n",
    "    std_decay_mode=STD_DECAY_MODE,\n",
    "    buffer_size=BUFFER_CAPACITY,\n",
    "    device=DEVICE,\n",
    "    actor_freeze=ACTOR_FREEZE,\n",
    "    )"
   ],
   "id": "1d8ae390b2843fca",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:24.397114Z",
     "start_time": "2026-01-07T04:57:24.100444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "td3_agent.load(agent_path)"
   ],
   "id": "5d2d2b610564c69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MPC Initialization",
   "id": "fac3637c0ee4f291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:24.419077Z",
     "start_time": "2026-01-07T04:57:24.415710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1.\n",
    "R2_penalty = 1.\n",
    "Q_penalty = np.array([[Q1_penalty, 0], [0, Q2_penalty]])\n",
    "R_penalty = np.array([[R1_penalty, 0], [0, R2_penalty]])"
   ],
   "id": "211634f6f62d4c5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:24.427792Z",
     "start_time": "2026-01-07T04:57:24.425539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "7bf97648c044ad79",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying RL Agent on the CSTR",
   "id": "65a4fc461d9d0b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:25.028510Z",
     "start_time": "2026-01-07T04:57:25.021130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=5.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward with relative tracking bands.\n",
    "\n",
    "    data_min, data_max : arrays for [u_min..., y_min...], [u_max..., y_max...]\n",
    "    n_inputs           : number of inputs (so outputs start at index n_inputs)\n",
    "    k_rel              : per-output relative tolerance factors (same length as outputs)\n",
    "    band_floor_phys    : per-output minimum band in physical units\n",
    "    Q_diag, R_diag     : quadratic weights (same as before)\n",
    "    \"\"\"\n",
    "\n",
    "    data_min = np.asarray(data_min, float)\n",
    "    data_max = np.asarray(data_max, float)\n",
    "    dy = np.maximum(data_max[n_inputs:] - data_min[n_inputs:], 1e-12)  # phys range for each y\n",
    "\n",
    "    k_rel = np.asarray(k_rel, float)\n",
    "    band_floor_phys = np.asarray(band_floor_phys, float)\n",
    "    Q_diag = np.asarray(Q_diag, float)\n",
    "    R_diag = np.asarray(R_diag, float)\n",
    "\n",
    "    # floor in *scaled* coordinates (used if y_sp_phys is not provided)\n",
    "    band_floor_scaled = band_floor_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "    def _sigmoid(x):\n",
    "        x = np.clip(x, -60.0, 60.0)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _phi(z, kind=bonus_kind, k=bonus_k, p=bonus_p, c=bonus_c):\n",
    "        z = np.clip(z, 0.0, 1.0)\n",
    "        if kind == \"linear\":\n",
    "            return 1.0 - z\n",
    "        if kind == \"quadratic\":\n",
    "            return (1.0 - z) ** 2\n",
    "        if kind == \"exp\":\n",
    "            return (np.exp(-k * z) - np.exp(-k)) / (1.0 - np.exp(-k))\n",
    "        if kind == \"power\":\n",
    "            return 1.0 - np.power(z, p)\n",
    "        if kind == \"log\":\n",
    "            return np.log1p(c * (1.0 - z)) / np.log1p(c)\n",
    "        raise ValueError(\"unknown bonus kind\")\n",
    "\n",
    "    def reward_fn(e_scaled, du_scaled, y_sp_phys=None):\n",
    "        \"\"\"\n",
    "        e_scaled : output error in scaled deviation space  (same as before)\n",
    "        du_scaled: input move in scaled deviation space    (same as before)\n",
    "        y_sp_phys: current setpoint in *physical* units (array len = n_outputs)\n",
    "        \"\"\"\n",
    "\n",
    "        e_scaled = np.asarray(e_scaled, float)\n",
    "        du_scaled = np.asarray(du_scaled, float)\n",
    "\n",
    "        # ----- dynamic band based on setpoint -----\n",
    "        if y_sp_phys is None:\n",
    "            # fallback: just use the floor\n",
    "            band_scaled = band_floor_scaled\n",
    "        else:\n",
    "            y_sp_phys_arr = np.asarray(y_sp_phys, float)\n",
    "            # band_phys_i = max(k_rel_i * |y_sp_i|, band_floor_phys_i)\n",
    "            band_phys = np.maximum(k_rel * np.abs(y_sp_phys_arr), band_floor_phys)\n",
    "            band_scaled = band_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "        tau_scaled = tau_frac * band_scaled\n",
    "\n",
    "        # ----- inside/outside gate -----\n",
    "        abs_e = np.abs(e_scaled)\n",
    "        s_i = _sigmoid((band_scaled - abs_e) / np.maximum(tau_scaled, 1e-12))\n",
    "\n",
    "        if gate == \"prod\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64))\n",
    "        elif gate == \"mean\":\n",
    "            w_in = float(np.mean(s_i))\n",
    "        elif gate == \"geom\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64) ** (1.0 / len(s_i)))\n",
    "        else:\n",
    "            raise ValueError(\"gate must be 'prod'|'mean'|'geom'\")\n",
    "\n",
    "        # ----- core quadratic costs -----\n",
    "        err_quad = np.sum(Q_diag * (e_scaled ** 2))\n",
    "        err_eff = (1.0 - w_in) * err_quad + w_in * (lam_in * err_quad)\n",
    "        move = np.sum(R_diag * (du_scaled ** 2))\n",
    "\n",
    "        # ----- linear penalties around band edge -----\n",
    "        slope_at_edge = 2.0 * Q_diag * band_scaled\n",
    "\n",
    "        overflow = np.maximum(abs_e - band_scaled, 0.0)\n",
    "        lin_out = (1.0 - w_in) * np.sum(gamma_out * slope_at_edge * overflow)\n",
    "\n",
    "        inside_mag = np.minimum(abs_e, band_scaled)\n",
    "        lin_in = w_in * np.sum(gamma_in * slope_at_edge * inside_mag)\n",
    "\n",
    "        # ----- bonus near zero error -----\n",
    "        qb2 = Q_diag * (band_scaled ** 2)\n",
    "        z = abs_e / np.maximum(band_scaled, 1e-12)\n",
    "        phi = _phi(z)\n",
    "        bonus = w_in * beta * np.sum(qb2 * phi)\n",
    "\n",
    "        # ----- total reward -----\n",
    "        return -(err_eff + move + lin_out + lin_in) + bonus\n",
    "\n",
    "    params = dict(\n",
    "        k_rel=k_rel,\n",
    "        band_floor_phys=band_floor_phys,\n",
    "        band_floor_scaled=band_floor_scaled,\n",
    "        Q_diag=Q_diag,\n",
    "        R_diag=R_diag,\n",
    "        tau_frac=tau_frac,\n",
    "        gamma_out=gamma_out,\n",
    "        gamma_in=gamma_in,\n",
    "        beta=beta,\n",
    "        gate=gate,\n",
    "        lam_in=lam_in,\n",
    "        bonus_kind=bonus_kind,\n",
    "        bonus_k=bonus_k,\n",
    "        bonus_p=bonus_p,\n",
    "        bonus_c=bonus_c,\n",
    "    )\n",
    "    return params, reward_fn"
   ],
   "id": "317946f8d424ca6c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reward configuration",
   "id": "29803d0d4ebeff3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:26.276976Z",
     "start_time": "2026-01-07T04:57:26.272525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_inputs = 2\n",
    "\n",
    "dy = data_max[n_inputs:] - data_min[n_inputs:]\n",
    "y_sp_nom = 0.5 * (data_min[n_inputs:] + data_max[n_inputs:])\n",
    "\n",
    "k_rel = np.array([0.003, 0.0003])\n",
    "band_floor_phys = np.array([0.006, 0.07])\n",
    "\n",
    "band_phys = np.maximum(k_rel * np.abs(y_sp_nom), band_floor_phys)\n",
    "\n",
    "scale_factor = 1.0  # use 2.0 for [-1, 1] scaling, 1.0 for [0, 1]\n",
    "band_scaled = scale_factor * band_phys / dy\n",
    "\n",
    "q0 = 1.4\n",
    "Q_diag = q0 / np.maximum(band_scaled ** 2, 1e-12)\n",
    "\n",
    "print(\"dy:\", dy)\n",
    "print(\"y_sp_nom:\", y_sp_nom)\n",
    "print(\"band_phys:\", band_phys)\n",
    "print(\"band_scaled:\", band_scaled)\n",
    "print(\"Q_diag:\", Q_diag)"
   ],
   "id": "85575e2c60b10163",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: [0.22165278 0.78153727]\n",
      "y_sp_nom: [  3.83915067 323.21371982]\n",
      "band_phys: [0.01151745 0.09696412]\n",
      "band_scaled: [0.05196169 0.12406845]\n",
      "Q_diag: [518.51529284  90.95055189]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:26.829685Z",
     "start_time": "2026-01-07T04:57:26.826082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_diag = np.array([518., 90.])          # rounded from the band-based calculation\n",
    "R_diag = np.array([90., 90.])          # move cost for du_scaled ~ 0.02\n",
    "\n",
    "n_inputs = 2\n",
    "\n",
    "print(\"Band scaled are:\")\n",
    "\n",
    "params, reward_fn = make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=7.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    ")\n",
    "print(params)"
   ],
   "id": "599d7fb29af995d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band scaled are:\n",
      "{'k_rel': array([0.003 , 0.0003]), 'band_floor_phys': array([0.006, 0.07 ]), 'band_floor_scaled': array([0.02706937, 0.08956707]), 'Q_diag': array([518.,  90.]), 'R_diag': array([90., 90.]), 'tau_frac': 0.7, 'gamma_out': 0.5, 'gamma_in': 0.5, 'beta': 7.0, 'gate': 'geom', 'lam_in': 1.0, 'bonus_kind': 'exp', 'bonus_k': 12.0, 'bonus_p': 0.6, 'bonus_c': 20.0}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:28.034462Z",
     "start_time": "2026-01-07T04:57:28.030841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "829372b3f310f055",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:28.421950Z",
     "start_time": "2026-01-07T04:57:28.414763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rl_train(system, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 test_cycle,\n",
    "                 nominal_qi, nominal_qs, nominal_ha,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"disturb\"):\n",
    "\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "    u_min, u_max = min_max_dict[\"u_min\"], min_max_dict[\"u_max\"]\n",
    "\n",
    "    y_system = np.zeros((nFE + 1, n_outputs))\n",
    "    y_system[0, :] = system.current_output\n",
    "    u_rl = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    # xhatdhat[:, 0] = np.random.uniform(low=min_max_dict[\"x_min\"], high=min_max_dict[\"x_max\"])\n",
    "    rewards = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "\n",
    "    delta_y_storage = []\n",
    "\n",
    "    # ----- helper ------\n",
    "    def map_to_bounds(a, low, high):\n",
    "        return low + ((a + 1.0) / 2.0) * (high - low)\n",
    "\n",
    "    test = False\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # train/test phase\n",
    "        if i in test_train_dict:\n",
    "            test = test_train_dict[i]\n",
    "\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # ---- RL state (scaled) ----\n",
    "        current_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i], y_sp[i, :], scaled_current_input_dev)\n",
    "\n",
    "        # ---- TD3 action ----\n",
    "        if not test:\n",
    "            action = agent.take_action(current_rl_state, explore=(not test))\n",
    "        else:\n",
    "            action = agent.act_eval(current_rl_state)\n",
    "        # Map to bounds\n",
    "        u_scaled = map_to_bounds(action, u_min, u_max)\n",
    "\n",
    "        # scale & step plant\n",
    "        u_rl[i, :] = u_scaled + ss_scaled_inputs\n",
    "        u_plant = reverse_min_max(u_rl[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_rl[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record output\n",
    "        y_system[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_system[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_system[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_rl[i, :] - ss_scaled_inputs)) + np.dot(L, (y_prev_scaled - yhat[:, i])).T\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u, y_sp_phys)\n",
    "\n",
    "        # Record rewards and delta_y\n",
    "        rewards[i] = reward * 0.01\n",
    "        delta_y_storage.append(np.abs(delta_y))\n",
    "\n",
    "        # ----- Next state for TD3 -----\n",
    "        next_u_dev = u_rl[i, :] - ss_scaled_inputs\n",
    "        next_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i+1], y_sp[i, :], next_u_dev)\n",
    "\n",
    "        # Episode boundary (treat each setpoint block as an episode end)\n",
    "        # done = 1.0 if (i + 1) % boundary == 0 else 0.0\n",
    "        done = 0.0\n",
    "\n",
    "        # Buffer + train (skip if in test phase)\n",
    "        if not test:\n",
    "            agent.push(current_rl_state,\n",
    "                       action.astype(np.float32),\n",
    "                       float(reward),\n",
    "                       next_rl_state,\n",
    "                       float(done))\n",
    "            if i >= WARM_START:\n",
    "                _ = agent.train_step()  # returns loss or None\n",
    "\n",
    "        # diagnostics at sub-episode boundary\n",
    "        if i in sub_episodes_changes_dict:\n",
    "            avg_rewards.append(np.mean(rewards[max(0, i - time_in_sub_episodes + 1): i + 1]))\n",
    "            print('Sub_Episode:', sub_episodes_changes_dict[i], '| avg. reward:', avg_rewards[-1])\n",
    "            if hasattr(agent, \"_expl_sigma\"):\n",
    "                print('Exploration noise:', agent._expl_sigma)\n",
    "\n",
    "    # unscale to plant units for plotting\n",
    "    u_rl = reverse_min_max(u_rl, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha"
   ],
   "id": "c0b20eb3706f0e6e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:48:44.499Z",
     "start_time": "2026-01-07T02:27:47.299316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "    = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 TEST_CYCLE,\n",
    "                 nominal_qi, nominal_qs, nominal_hA,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"nominal\")"
   ],
   "id": "eb10ecbf5180672d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 1 | avg. reward: -19.19575890913601\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.062771369703952\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.30397748062933\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.39185043849061\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.028003561880112\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.915260782907495\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.50976340458148\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.814389234774485\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.60950024509586\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.04995959142853\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.334067261220326\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.28894534848662\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.466078373812437\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.387789648685356\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.366440109033583\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -26.1925490170738\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.9873168616474444\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.5530690636039144\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.3811303985869343\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.3589011120479575\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.278939159837328\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9320990866575414\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9114179571698893\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8210353970136355\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8117492620918114\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8026353839525369\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8124254857499427\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.7965358755511573\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8004978836863381\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.7805261284898635\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.77469200231368\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.748236187198425\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7873109964239418\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7853137504343015\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.77079850016777\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7527542116923736\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.796062819733436\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7313123984173047\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7773940478475865\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7761192725930715\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7576869370558221\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.794870885685508\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7401595975145148\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.777374373815406\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.76955506918277\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7340706396865415\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8004494056548959\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.771403366076963\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.753714232317436\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7884561828421313\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7575561800346549\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8034927929333515\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7726407129342687\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7629815204993735\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7935329668731201\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8050954533208694\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8104844502481774\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7877288498765536\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7809998115843046\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.780146107948571\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7722953004352493\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7596128823663082\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.762528837702751\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7749966947578566\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7289494590232437\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7923113242531488\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8033280533049043\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8073027103069093\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8036360000858986\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.75776151270823\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8063338794756825\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8174218255238728\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7389618031631557\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.726641116504046\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7636116743163612\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7301643806106222\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7229736555543684\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.777740549133946\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8077904274026793\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.737071896464737\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7219470557975667\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.736516092185974\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7289117299587695\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7290301296889448\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7314562681010914\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7746775092305376\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7556368632132333\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7303160088102791\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7413001710024927\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7503555377206368\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7447754078153093\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7301502246538218\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7362452659170213\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7263406557808736\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7446814518068805\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.717751443821195\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7246815254924477\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.773558415334945\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.759531667575477\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7389291949880872\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7070154247269154\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.718645896589765\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.6863592176891198\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7253193792978743\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7062130817696248\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.6797071133314057\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.6738990866744223\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7109163586950011\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.6929672697630962\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.713553045312942\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.6749488870026987\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7026958392785059\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7053897179227056\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.6956372389504526\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7079560568740022\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.6717845271579386\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6952560882571412\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.70956690530225\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.6815458371810286\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.696706802333906\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.6938993959958384\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.6854625455965833\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.675886171056063\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.6742005366114023\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6831255245595602\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7028522363518197\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6906667083239204\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.710719504865725\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.6913556214607937\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.6924692209257888\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.704499957088068\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7062261171711037\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7193589905183198\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.6970691794237844\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7072575995207193\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7229546786482433\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7217879429903338\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7489466724120166\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.71592672578593\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7191133374724046\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7453993831745778\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7615841813005733\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7204261938615855\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7089865105692805\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.6923444161748022\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.716964255337145\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7195616547283117\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7119366887613034\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7588883849046686\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7053830691083263\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7123734424294212\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7353578982551654\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7010121050460105\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.6974880230609302\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7181786217288972\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7671572068838504\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.719886014202159\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.742723282162096\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7034811903023461\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7400636201339652\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7000580610370701\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7086784332290648\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7071778507162207\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7123135253097144\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7175224646443474\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7129774601106604\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7369107955632563\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7293700606723055\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7074563167240866\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7017076861176943\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.720803919983835\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6790991866835498\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6944393158133984\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7017399605738133\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.708398785688106\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.696421591091232\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7105455862191896\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7018960363807876\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.714306172765624\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7067009461751397\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.714303847638089\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.689415193806886\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7230514093530835\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.705582414298664\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7137775398842359\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6985167435176212\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7038937447365072\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7260205954658097\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7358612923272556\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.713561567430367\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.732357410322528\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7378705050500551\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7080724310531787\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6950897724590102\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7055582182221372\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7311033865153547\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7025902938615474\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6875069554001192\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6971141581593612\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6629883767675921\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:57:30.615032Z",
     "start_time": "2026-01-07T04:57:30.596366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=None, prefix_name=\"agent_result\",\n",
    "    agent=None,\n",
    "    delta_y_storage=None,\n",
    "    rewards=None,\n",
    "    dist=None,\n",
    "    start_plot_idx=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation-style plotting (same colors/fonts/no legends).\n",
    "    Saves all figures + input_data.pkl to directory/prefix_name/<timestamp>.\n",
    "    Handles:\n",
    "      dist=None\n",
    "      dist=1D array\n",
    "      dist=dict with keys {\"qi\",\"qs\",\"ha\"}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name):\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, name), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    y_sp_original = np.array(y_sp, copy=True)\n",
    "\n",
    "    actor_losses = getattr(agent, \"actor_losses\", None) if agent is not None else None\n",
    "    critic_losses = getattr(agent, \"critic_losses\", None) if agent is not None else None\n",
    "    dy_arr = np.array(delta_y_storage) if delta_y_storage is not None else None\n",
    "    rewards_arr = np.array(rewards) if rewards is not None else None\n",
    "\n",
    "    input_data = {\n",
    "        \"y_sp\": y_sp_original,\n",
    "        \"steady_states\": steady_states,\n",
    "        \"nFE\": nFE,\n",
    "        \"delta_t\": delta_t,\n",
    "        \"time_in_sub_episodes\": time_in_sub_episodes,\n",
    "        \"y_mpc\": y_mpc,\n",
    "        \"u_mpc\": u_mpc,\n",
    "        \"avg_rewards\": avg_rewards,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"warm_start_plot\": warm_start_plot,\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "        \"delta_y_storage\": dy_arr,\n",
    "        \"rewards\": rewards_arr,\n",
    "        \"dist\": dist,\n",
    "        \"start_plot_idx\": start_plot_idx\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"input_data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(input_data, f)\n",
    "\n",
    "    # Canceling the deviation form (same logic)\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # Distillation-style rcParams (no bold globals; bold comes from \\mathbf in labels)\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    # Colors exactly like distillation code\n",
    "    C_QC = \"tab:green\"\n",
    "    C_QM = \"tab:orange\"\n",
    "    C_RW = \"tab:purple\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    warm_start_plot = np.atleast_1d(warm_start_plot) * delta_t\n",
    "    ws_end = float(warm_start_plot.max()) if warm_start_plot.size > 0 else 0.0\n",
    "\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1: outputs (full) --------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 0], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[0, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{\\eta}$ (L/g)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 1], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[1, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{T}$ (K)\", fontsize=18)\n",
    "    ax.set_xlabel(r\"$\\mathbf{Time}$ (hour)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95, bottom=0.12)\n",
    "    _savefig(\"fig_rl_outputs_full.png\")\n",
    "\n",
    "    # -------- last window --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{time_in_sub_episodes}.png\")\n",
    "\n",
    "    # -------- last 4x window --------\n",
    "    W4 = 4 * time_in_sub_episodes\n",
    "    time_plot_4w = np.linspace(0, W4 * delta_t, W4 + 1)\n",
    "\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[0, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[1, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{W4}.png\")\n",
    "\n",
    "    # -------- Plot 2: inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_QC, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_QM, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(\"fig_rl_inputs_full.png\")\n",
    "\n",
    "    # -------- Plot 3: reward per episode --------\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    xep = np.arange(1, len(avg_rewards) + 1)\n",
    "    plt.plot(xep, avg_rewards, \"o-\", lw=2.2, color=C_RW, zorder=2)\n",
    "    plt.ylabel(\"Avg. Reward\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    _savefig(\"fig_rl_rewards.png\")\n",
    "\n",
    "    # -------- optional losses --------\n",
    "    if actor_losses is not None and len(actor_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(actor_losses, lw=1.8, color=\"tab:blue\")\n",
    "        plt.ylabel(\"Actor Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_actor.png\")\n",
    "\n",
    "    if critic_losses is not None and len(critic_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(critic_losses, lw=1.8, color=\"tab:orange\")\n",
    "        plt.ylabel(\"Critic Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_critic.png\")\n",
    "\n",
    "    # -------- optional delta_y windows (no legend) --------\n",
    "    if dy_arr is not None and dy_arr.ndim == 2 and dy_arr.shape[1] >= 2:\n",
    "        n = dy_arr.shape[0]\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w = dy_arr[i0:n]\n",
    "        if len(w) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w[:, 0], c=\"r\")\n",
    "            plt.plot(w[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_last300.png\")\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w2 = dy_arr[j0:j1]\n",
    "        if len(w2) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w2[:, 0], c=\"r\")\n",
    "            plt.plot(w2[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_700_400.png\")\n",
    "\n",
    "    # -------- optional per-step rewards (no legend) --------\n",
    "    if rewards_arr is not None and rewards_arr.ndim == 1 and rewards_arr.size > 0:\n",
    "        n = rewards_arr.size\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w = rewards_arr[j0:j1]\n",
    "        if w.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w.size), w, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_700_400.png\")\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w2 = rewards_arr[i0:n]\n",
    "        if w2.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w2.size), w2, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_last300.png\")\n",
    "\n",
    "        plt.figure(figsize=(7.6, 4.2))\n",
    "        plt.scatter(range(rewards_arr.size), rewards_arr, s=10)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"rewards_all.png\")\n",
    "\n",
    "    # -------- disturbance (no legend) --------\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, dict) and all(k in dist for k in [\"qi\", \"qs\", \"ha\"]):\n",
    "            qi_arr = np.asarray(dist[\"qi\"]).squeeze()\n",
    "            qs_arr = np.asarray(dist[\"qs\"]).squeeze()\n",
    "            ha_arr = np.asarray(dist[\"ha\"]).squeeze()\n",
    "            n_al = min(nFE, qi_arr.shape[0], qs_arr.shape[0], ha_arr.shape[0])\n",
    "\n",
    "            def _dist_fig(t, q1, q2, hA, suffix):\n",
    "                plt.figure(figsize=(7.6, 6.2))\n",
    "\n",
    "                ax = plt.subplot(3, 1, 1)\n",
    "                ax.plot(t, q1, \"-\", lw=2, color=\"tab:blue\")\n",
    "                ax.set_ylabel(r\"$Q_i$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 2)\n",
    "                ax.plot(t, q2, \"-\", lw=2, color=\"tab:orange\")\n",
    "                ax.set_ylabel(r\"$Q_s$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 3)\n",
    "                ax.plot(t, hA, \"-\", lw=2, color=\"tab:green\")\n",
    "                ax.set_xlabel(\"Time (h)\")\n",
    "                ax.set_ylabel(r\"$h_a$ (J/Kh)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                plt.gcf().subplots_adjust(right=0.95, hspace=0.25)\n",
    "                _savefig(f\"fig_disturbances_{suffix}.png\")\n",
    "\n",
    "            _dist_fig(time_plot[:n_al], qi_arr[:n_al], qs_arr[:n_al], ha_arr[:n_al], suffix=\"full\")\n",
    "\n",
    "            if time_in_sub_episodes > 0:\n",
    "                W = min(time_in_sub_episodes, n_al)\n",
    "                t_lastW = np.linspace(0, W * delta_t, W, endpoint=False)\n",
    "                _dist_fig(\n",
    "                    t_lastW,\n",
    "                    qi_arr[n_al - W:n_al],\n",
    "                    qs_arr[n_al - W:n_al],\n",
    "                    ha_arr[n_al - W:n_al],\n",
    "                    suffix=f\"last{W}\"\n",
    "                )\n",
    "        else:\n",
    "            dist_arr = np.asarray(dist).squeeze()\n",
    "            n_al = min(nFE, dist_arr.shape[0])\n",
    "            plt.figure(figsize=(7.2, 4.2))\n",
    "            plt.plot(time_plot[start_plot_idx:n_al], dist_arr[start_plot_idx:n_al], lw=1.8, color=\"tab:blue\")\n",
    "            plt.ylabel(\"Disturbance\")\n",
    "            plt.xlabel(\"Time (h)\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"disturbance.png\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "f7a5be6d8440239a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:50:07.083182Z",
     "start_time": "2026-01-07T02:50:03.563883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=dir_path, prefix_name=\"polymer_nominal\",\n",
    "    agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "    dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    ")"
   ],
   "id": "9d08421eb40e999f",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T15:43:44.686004Z",
     "start_time": "2026-01-07T04:57:57.849042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    set_points_number = int(C_aug.shape[0])\n",
    "    inputs_number = int(B_aug.shape[1])\n",
    "    STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "    ACTION_DIM = int(B_aug.shape[1])\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    BUFFER_CAPACITY = 40000\n",
    "    ACTOR_LR = 5e-5\n",
    "    CRITIC_LR = 5e-4\n",
    "    SMOOTHING_STD = 0.005\n",
    "    NOISE_CLIP = 0.01\n",
    "    # EXPLORATION_NOISE_STD = 0.01\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.005  # 0.01\n",
    "    MAX_ACTION = 1\n",
    "    POLICY_DELAY = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 256\n",
    "    STD_START = 0.02\n",
    "    STD_END = 0.001\n",
    "    STD_DECAY_RATE = 0.99992\n",
    "    STD_DECAY_MODE = \"exp\"\n",
    "    td3_agent = TD3Agent(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        actor_hidden=ACTOR_LAYER_SIZES,\n",
    "        critic_hidden=CRITIC_LAYER_SIZES,\n",
    "        gamma=GAMMA,\n",
    "        actor_lr=ACTOR_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        policy_delay=POLICY_DELAY,\n",
    "        target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "        noise_clip=NOISE_CLIP,\n",
    "        max_action=MAX_ACTION,\n",
    "        tau=TAU,\n",
    "        std_start=STD_START,\n",
    "        std_end=STD_END,\n",
    "        std_decay_rate=STD_DECAY_RATE,\n",
    "        std_decay_mode=STD_DECAY_MODE,\n",
    "        buffer_size=BUFFER_CAPACITY,\n",
    "        device=DEVICE,\n",
    "        actor_freeze=ACTOR_FREEZE,\n",
    "        mode=\"mpc\"\n",
    "    )\n",
    "    agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "    td3_agent.load(agent_path)\n",
    "    cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "    y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "        = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                     steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                     L, data_min, data_max, warm_start,\n",
    "                     TEST_CYCLE,\n",
    "                     nominal_qi, nominal_qs, nominal_hA,\n",
    "                     qi_change, qs_change, ha_change,\n",
    "                     reward_fn, mode=\"nominal\")\n",
    "    out_dir = plot_rl_results_disturbance(\n",
    "        y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "        y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "        directory=dir_path, prefix_name=\"polymer_nominal_no_buffer_new_reward\",\n",
    "        agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "        dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    "    )"
   ],
   "id": "fdb4470b25c16864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -24.63238248174676\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -20.86557503971867\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.967425391340768\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.784176062650108\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -25.457865740377965\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.30146718259024\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.959965274901087\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.406052695716852\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.582718388929212\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -25.17636949203222\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.753584029084198\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.082318487885708\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -24.144882017436093\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.999941892251154\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -24.475080743749967\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -44.6528445247729\n",
      "Exploration noise: 0.007823673866947569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\system_functions.py:61: RuntimeWarning: invalid value encountered in scalar power\n",
      "  CP = (2 * self.fi * kd * CI / kt) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 17 | avg. reward: -171.42138421948522\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.461599879850059\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.002928058800451\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.521161178368048\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.9635353846671142\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9914394969962612\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9092968922305784\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8493772654572203\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.978428052846258\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.08191030771391\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8604365567502157\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8607826150755415\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.9078721684257873\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.779428983506005\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8456328579727097\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8223286523033821\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8045686124276288\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7705470839235966\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8114837478275296\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.831635378957763\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.9192321347180337\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8090722007283977\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7834836107345096\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7971425841061488\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7833411812265438\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.8012864813304832\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.813996259027273\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.8258054301686104\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.9755272139509998\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.9374504359825624\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.9374459728465854\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.9322940125014474\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.8659917986148384\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.8480242040005732\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7902678757505435\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.794013052669909\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7322605359084213\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.779942000365078\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7454175175201008\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7883416597713064\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7820565789103484\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8054880254967736\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8436065720445374\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7707456723528838\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7769922458578467\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.804197142263405\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7339237914745527\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7876182901634186\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7547282186285946\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7776476413347724\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7785805483471746\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7673681110995592\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.764496394083098\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.734198815164798\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7215149017135802\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7404486445215304\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7293269921310466\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7471661122684572\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.741065570606006\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7456896186646675\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7335214494396398\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7495108576242309\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7525156753057385\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.734291371084661\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7516115612123035\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.734388052021846\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7711817824874794\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7483615254329297\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7262778173482338\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7429840986885552\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7378685679257446\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7381235325641087\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7326335647422269\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7700537744261016\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7359309989256628\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7560222137230588\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.8152498795548906\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7355113563803448\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7423994238234997\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7528221897921967\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7403086596372404\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.727434814825404\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7355966796322126\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7625796495205734\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7329888250295427\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7382676883180972\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7662047929406512\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7856134778521024\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.800860751152397\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7540792786419446\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.720505265139377\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7169837339055476\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.742152101134381\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7192935145565815\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7161862246533264\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7537406455088669\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.725021926404893\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7488866596556492\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7389365666478707\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7426267714076473\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7998233437429079\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7773313577294454\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -2.3690760890092286\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.8519354331524363\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.8360026690104063\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8500191390922083\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -2.057130862626898\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.904479320204805\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.9623379688930236\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7200964202130131\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7174001106724301\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.6919857080768819\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7223736345936878\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7814183867589202\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -2.3333445800182\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.8418106627068516\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.8804446741543632\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -2.271340289380414\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.915012624541992\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8060055001098458\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.8279632760222995\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.720019679964965\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7282945292610299\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7758965694066577\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7252823795194996\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7607428774759597\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7473287926137704\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7193488606998568\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7008836797462765\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.6780676644475243\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7067419249152829\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6909376364533433\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.6851493026176638\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.6843666898560872\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6823207391341282\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.6790822862095103\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.672039801616091\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.679721045316129\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.6861232905323402\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6732712388441666\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6849491585516156\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6883008800142454\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6875137465986148\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.6889704188134433\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7054046373103393\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.6951848637621902\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6752212531938069\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.686785741635906\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.6724843314206437\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.693767165302392\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6787938963848932\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6816784353346588\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.6834763485485191\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6690191500343587\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.6901421036225373\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6868741144476695\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6603478773855866\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6841926748790703\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.6810334611518098\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6851554984202466\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6947962148367777\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7338175632211659\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7117902732929091\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.68681524808201\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6850920308683086\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7012161638108643\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6919707850041754\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.701160116211848\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.693750379042025\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.694489151183928\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6890209790205035\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6972085485401514\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6882608243071644\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7013775532618791\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.6800424790152733\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7674289323528996\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7551149086491586\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.801875815089686\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6990655002985369\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7515542232984893\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7440132721760833\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7168520478822307\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.727927286468991\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.717981959329657\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.009890969777626\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.77449936770086\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.5876840148629\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -21.708461428513985\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.60860049530077\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.444423047796818\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.738316482441004\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.26274995933604\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.99119072271839\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -25.525012431378226\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -21.64555736883428\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.948395611041633\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.37058687071281\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -21.72948603692084\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.204113947613404\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -28.74049561335115\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -7.612766223106174\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.5661071809346363\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.1963401910824056\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.0658792401359425\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.9620593911250657\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.871396786328732\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.868463277978405\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.876816353778604\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8513073174709909\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8089355713077842\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8143300064095103\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8130716404761182\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.787190950624813\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.7582824365372838\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.791095025395999\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7678737477150015\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7813527287875603\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7977155123291972\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7523834201762731\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8048429460679032\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.748485441667645\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8107542136802248\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.788736616463544\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7610592637065856\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7537891511784187\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.6939319711289327\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7615315114936498\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.711073381820612\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7267395543312727\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7231106456024299\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.733524477200439\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7601946770146468\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7424527396893945\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7156818158550433\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7461908854357437\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7384132820161449\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7455665336108879\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7247801268602205\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7273531564938616\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7243269255739944\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.73754023464917\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.733065029322832\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7501381103476854\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7417248875998694\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7306690954990873\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.711039516186857\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7367557524153654\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7283589215702306\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7362382424552327\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7457960409829594\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7299113878535763\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7275716049910748\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7038237194925756\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.780769725449662\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.761923578564756\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8029361799461343\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8619240164716342\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.885675334977401\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8342366311134566\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.8587975563902859\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.2635115206088003\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7794190046511957\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7637976263823671\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.750844157905122\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.77820678614149\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7537814185738683\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7504479984056673\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7789419653154448\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7873457806802724\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7584006395411418\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7882422377023477\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7963010195998317\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7676719557610818\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7465092381163334\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7726591355925498\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.762810218951815\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7538127519456328\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7346818851502792\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7369080742542276\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7132786234280148\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7187991900085025\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7069824598179562\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7087565830724327\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7359129194652296\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.717326060050547\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.729013109367899\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7024435648205614\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7528797232770972\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7582544185335731\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7449063088723145\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7515556044566971\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7340283902858828\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7308765341900552\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7315761833030865\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7505710088742796\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7485334980942366\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7038188926894338\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7177386818695901\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.777260925195996\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7235840992416132\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7337274570983758\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7154963691961909\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7319018041109657\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.719337566276815\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7553966712344578\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7547498763708729\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7427636193028342\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.75153257634888\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7324582999669544\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7443161106670573\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7187166484507843\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.745667917813233\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7284454647722136\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7339242079240498\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7324252160527767\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.74975927350147\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.744035473094024\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.77297781670618\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.8163814577077964\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8783880489919853\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.8527950058961136\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7843347786921948\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.8233858514670067\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -4.280536441831728\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.814193950804425\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7806846393697953\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.870508930686599\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.868537936852323\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.948559706098029\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.916344429561288\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.9377524015943999\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.9255140665850468\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.8017694394682888\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7564315342709773\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.8056204631893644\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.8040634715655077\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.8291505102244339\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7632805328711312\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.854629673651941\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.8073864747757087\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7826020171306884\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7420251076229198\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7182021213415277\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.737798829262781\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7310732371366533\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7388309568723157\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7325269164972508\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7515646214610803\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7877238553469323\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7201295592586958\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7337578809264371\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.725009012466565\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7180320104364026\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7313196129464885\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7285609640974464\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7327977622580784\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7484183338835846\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7158418875129353\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.717244159814603\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7142982530105768\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7291475628007467\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7271433038338386\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7249737858923118\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7296171876062982\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7421101602956082\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.746664612006412\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7305679719428428\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7356970428314549\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.717121995831588\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7302371045188585\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7194775913656213\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.724290657547687\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.74472481995787\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7256470645212831\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7397037802372823\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7122888960841112\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7229856802580943\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7333056246954834\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7867642512534285\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7949593801723445\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.8014771977164445\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.8853553831068808\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.8840460595070618\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.898474151542159\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -20.983002342347824\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -19.14321469043226\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.960588606451083\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.47867955074219\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -21.86504378510416\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.941029920707912\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.30777007244199\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.601979620714665\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -21.73130023789825\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -24.630686267887324\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.88312372370511\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.164618486734025\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.942196875692726\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.975695712643414\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.54138862470934\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -29.653507528539976\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.727942944255937\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.7670865264466213\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.2986532608869212\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.0498771257518706\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.8565620622169365\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.8906142901713867\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.7781678079685812\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.7518965426583282\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8141079666321291\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8122985178050857\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8001617618016612\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8187995478699923\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8045852405280431\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8302274948878847\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7942292532233182\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8184814401090252\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8026745011512966\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.8072447898376616\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7597647812001316\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7990290989515074\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.8024669748506466\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7652729422188436\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7623197691716075\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7272470803241515\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7398687425312942\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7007358508477521\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7554271592947501\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7604022728375002\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7181297530030843\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.719902181071905\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.6891019602372483\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.699375185901377\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7527966506451267\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7190778411224892\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7089894089511262\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.735231445222849\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7291696785001454\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7333270252080468\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7155320903758144\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7242604154608434\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7253838253999079\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7733662806833472\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7405153930987631\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7080546310553626\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7158067551549403\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7571373703415718\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.769218509780994\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7456095077202178\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.764873544797411\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8329775432646676\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.746556541040457\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8140124356381824\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8318016179200336\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7773867763628544\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7737931422938515\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7305883255632148\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7441951309264891\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7654545950246845\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7170794458240681\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7268857374714002\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.101506197391183\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.9582623153690297\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.866369737333398\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.789915418928267\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7740236883176954\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.760837997881865\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7468002199045134\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7688814927790895\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7711461195305243\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7593743607111956\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7741538491820796\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7387642366850287\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7542635924655556\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7540001623832573\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7510126586112014\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7492613112081792\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7685561200636801\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.750769785614943\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7464438193105472\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.753393286692382\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7519430692452573\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.736384063495671\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7299594397925517\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7071025649688822\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.725394499947427\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7204064405929875\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7185506582482966\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7253014476390485\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7304953686909794\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7337032496441633\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.743509262563807\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7329562509103857\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7415438243688244\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7836286192230437\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8902761751005768\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.9693894860887804\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -2.0299047778190102\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8133352853789426\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7967900486715291\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7611570576404378\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7271595443030856\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7300730193171763\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7449457320939448\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7313166103955338\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7486343747432478\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7238551405227611\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7122287556371827\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.6985123048784152\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6852749599475896\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.710773493599824\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6752618262667645\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.702400605727575\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.705051383803155\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7082074539906988\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.6918442918708332\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.6952237738579812\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.686935298540401\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.6934295343492516\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.6908624462170365\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.697343905771351\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.6986925257456258\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.6951270579249011\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.6859666718930315\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7021060086081945\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.703119964771001\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.689210989257458\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.687872501882304\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7023747582450943\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.6986027185557668\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7076301885587868\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -2.084351213728189\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.716378154295515\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7278967287090103\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7480491524782988\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.741608171203686\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7115789858764163\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7340621198781114\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7067247832389814\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.709455285571221\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.695974247118956\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7282387557326944\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7292143038633088\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7198028155208789\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7192730356380594\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6975513867553496\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7138005238538476\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7368314061553503\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7467327210753438\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7245531964367116\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7276959777058511\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7309257844677899\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7115427246536594\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7145321717837814\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7059101235966267\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7093290707227453\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6975989766249164\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7581231536343096\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7026182671177315\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.788118926223289\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7782990250153095\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6905246530958942\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7056922257973894\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6931351532308947\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7047384272816704\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7098039677440955\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7065701498609203\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7082334659100837\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7031448900842587\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7075795311211914\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7047977796547877\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7047336414008942\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6926233951166545\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6800393740851787\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.699791992468355\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.704612219643373\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6983047646102478\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.707970963615878\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.695617679072293\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7052725192271314\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7086989362053422\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6946331496286136\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.692275527206109\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7037704031423697\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.677951229837767\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.472485393689325\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -20.886121774282532\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.517988317052705\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.77345592133369\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -20.952580275024157\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.074446851408243\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -20.766879509216643\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.769808776593344\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.72784865197212\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.600087441336292\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.138525821677625\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.02317660545855\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.828594136486934\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.291646623233465\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.072917819174553\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -4027.9002703224673\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.78195519884824\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.0512495859200737\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.5332269296862133\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.443889541802844\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -3.2039845792200445\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.770710529418126\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.8501197414420165\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -3.1234485139235324\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -3.423912145199566\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.72129459077817\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.610384827876178\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.7654882300491743\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.780827821128127\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.9460503323258473\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -3.517148267285394\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -3.1928244937630916\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -3.1484241944353797\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.906190681547532\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -3.1633087511687132\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.8821954535692114\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.6310114890865908\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -3.0681066191326134\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.917931179178764\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.8783995031216874\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.682473250562937\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.703443766362874\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.815024348705854\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.5062783660426198\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.391433748366088\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.4711493834160025\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.342660484711045\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.1824679130162505\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.206113470755671\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -2.2675869164481877\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.3184789952382796\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -2.265846852079521\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.2450920198716475\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -2.1394346762537797\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.2143812913353553\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.3045354299979595\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.267121371921969\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.196953284619517\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.1096349264495418\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.091660724782715\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.1024067562916118\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.115123870994797\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -2.1240742358731484\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.0907033424477626\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -2.025578286440857\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.9816296997917562\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.911670311478743\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8610775842927196\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.837114073400458\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8417154371245732\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8334232936484036\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8725629950373905\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8693312288987016\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.854168593518203\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8367799806209357\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.837817374473878\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.8043860425646097\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.822512944555008\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7818130138278503\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.8179235459434608\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.789953272848963\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7894946538554075\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.781301826729574\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.808804735317283\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.8275942055018766\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7907253186992198\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7679085227085896\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7624682399191451\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7723558490829558\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7335159803546214\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7533325653558347\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.756575789266654\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7581338906282775\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7326387199422069\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.74221809275289\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7245893404492931\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7555318043937802\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7406797292994098\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7263036244511953\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.765605496555802\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7172728723473312\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7027012432630777\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.727919215761457\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.6917654234005692\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7258191739279318\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.722246393267349\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7271259194911142\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7402103612203519\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7233093416717937\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7303173545114037\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7467346276061158\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7602381419747621\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7647002324452843\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7370026457130638\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7278163436977076\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7494279024814678\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7394547469863912\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.733981485832798\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.726542591753592\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7360600848773635\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.714192200213268\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.762565239018773\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7389603771266042\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7266326125673082\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7386211204121083\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7245943964123085\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7460368855630022\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.816593571322535\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7905790272091513\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.8087454944144872\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7788794785257271\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.789414596776242\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7607513246639335\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.775103249232847\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7426412765832549\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7197395433288907\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7106377314905024\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7213069499688811\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.722959020634899\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7394193771173705\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7157486111604772\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7403791064305034\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.728743579572951\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7159819704792107\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7463550040980151\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7380262565884648\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7144647142761102\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.721839394304166\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.73836926824258\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7188161680339937\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7285793004260677\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7460989524388921\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7220522247654229\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7138851743367867\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7123613352624658\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6918832130824177\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6884820337183952\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.711125461804932\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6838340235179954\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7055469386901252\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7098190333248664\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7039584709524251\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7018225563058507\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7019094813390974\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7139074627994424\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7069521819698175\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7072080734809592\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6981339153281738\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7129420321958149\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.723215810053345\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.690789456354472\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6897975951256021\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7011830431804196\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6973501995686688\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.695415796542641\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6943159903719305\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6865876869083103\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6941413152006186\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6963324108775137\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7100115892404426\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6971700446805005\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.6870305552609028\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.710418221857351\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7032979243434863\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7011307171276444\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7104953106785055\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7046114031677189\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7242788147378518\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7018948850801425\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7006446913799602\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.718449673009066\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.72194313977361\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7047177923947152\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7086808263326623\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6969421806725102\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6881215034192143\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7054892271045405\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7094017510107176\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6769049193841044\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6374365560319217\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -22.12312702226329\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.178824349559335\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -21.6766440885189\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.697230485040414\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.15619467252575\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.72330110740757\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.66597353915443\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.91784799137502\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -22.70156271870347\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.31095271924836\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.801148114518718\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.213640173381968\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.363559266430908\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -20.890461854928517\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.613728798635737\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -3957.2485389734848\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.414408412801179\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.501104451993937\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -3.6712745153628994\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -10.605523512814111\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1214.2062859376792\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -3.0640711436575234\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -3.0388327986808075\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.5598095733397135\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.180674388213457\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.1463835177297894\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.14085028621809\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.9779602747864151\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.1370899097985223\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.0342650102502047\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.093656772678126\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.0398107418617326\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.0522183808605634\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.1004895704019986\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.084622123646475\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.0866638329703617\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.1585622586850675\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.340655727643559\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.2005948494614107\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.3101465957641394\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.351747006574518\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.257094934644189\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.23945343140996\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.232624792029217\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.30854121553219\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.041735290135463\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.0985023843484005\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.1637566424702386\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.20212020578455\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -2.1501163063332736\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.103539257909544\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.9219047366498494\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.9739205001581746\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.9250567498389382\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.896370517941598\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8595052016542326\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.894183728009304\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.9325070425636377\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.910382937286494\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.785970884199389\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.9147568337554097\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.9329207616199466\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.950804995680176\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.022499521858484\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8239827381442308\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.9053055739516953\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.85373759886421\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.9993913705142756\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8087269365047023\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8787513715606474\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8665905782043717\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.0802025222431144\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.9995185488698488\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.112950441360644\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -2.3727261074236794\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -3.1620934345423217\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -3.221007044417596\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -4.215568555284196\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -4.489071552036221\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -3.064553024938032\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -3.016728269237179\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -3.9149574681333195\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.1721270551858614\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.9247704790374944\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.8979309314443724\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.8658635456576662\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.8969027973711647\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.9192531886709978\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.020655161622173\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -2.0022934785980424\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.9716173012978162\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.042902309187493\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.9274410032047524\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.808013050419351\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8221338815833747\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.8241001394619076\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.819876028319278\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.847139392623647\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7586734343550305\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7714557241264304\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7447836618677093\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7548236961505546\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.808003643351135\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7911999318747045\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7709505107426642\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.769540370711364\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7860686346626813\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7515368657548094\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7371443981762098\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7329397995706062\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.769417545004989\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.737405597278099\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7663130793065478\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7560392617180844\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7893409824502566\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7552854148351986\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7591909625832005\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7753906407882993\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.8464148312208963\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.804518897447536\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.826509285727725\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8284320795192357\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.8142761060333878\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.8086504412333164\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.8011940120861465\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7614894413476976\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7631230755193932\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7526943835406519\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7500199671157042\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.755874537220413\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.761793979209376\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7736044074637383\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7746251619836402\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7778151052996167\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7773955930143348\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8006413876207465\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.8089158497235178\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.8494197976492535\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.9069153779527364\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7557769773358212\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7204908436371074\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7439599596245055\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7413476990150696\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7084587816157426\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.747952151318115\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.726223941054435\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.8016882989322154\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7693065478475143\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.755460952169568\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7472432018559731\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.770937562127242\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7756809312408735\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.789508434546525\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7492676615004796\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7537257658349568\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.743456284271553\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7576527230996337\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7369810821052938\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7184265754169052\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7032964812036162\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7312196568170573\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7188789986859705\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7266105539276806\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7096135695658259\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7187613339614571\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7127765203296252\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7214533475441869\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7000684016398389\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7106920017171097\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.71854035583533\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7329832601322375\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7015940362848527\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7179406040789342\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7308111837151625\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7447373959356478\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7252505512951464\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7148857588702209\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7647801446095008\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7232704258640625\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.704143571904978\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.703676315323093\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7120930608787\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7166063277568133\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7101883060679222\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7097790527256052\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7044148199831273\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6933347699266443\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7080831044612006\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6827540842046522\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.6855944481037433\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.6755743740537474\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6803196991465665\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.6766842365720505\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.673925752023626\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6798573451636372\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.686488172052438\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6837842449577511\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6866976256763957\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6965486493491913\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6755870721226598\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -19.68502425257429\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.66854686279703\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.566249282380205\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.903889624286148\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.695432257715964\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.651422490446265\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.60788469528851\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.446079287175493\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.793839746371688\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -24.442700820614327\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -21.78423624416312\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.16505865108474\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.497265330986227\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.11812854815205\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -20.766101821066062\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -27.424495941921087\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -10.028235668869279\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.379915069654214\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.4004194871216793\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.104137126441312\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.0313427769050834\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9623386063786519\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.977634867044697\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.9666004834608446\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8650701795022389\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8954225806370286\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.9045383446971635\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8142479962587152\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7930239382625865\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.9562217813384273\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7811997828373627\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8379767700596323\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7983771571825355\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7702125020659878\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8477635762591018\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8160897281285258\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7406792376382159\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7990067237542104\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7711037133541014\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.8045365884406004\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7768288758381277\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.8149720912693932\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7883056575722838\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.9200751215185656\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.808355654616597\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.867617481172035\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8725963882241436\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.8553693235899715\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7934161374965214\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.788135171232684\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7636354581232467\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7307342300044974\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.729417687355941\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.741642817467451\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.754685129751466\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7342038624571245\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7617762481537518\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.711371392036192\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.748510149314595\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7333744873013501\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7415721493376586\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7494503342987855\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7311229517419935\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7181200189523056\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.741613842122664\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7539533950142898\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.75548507594418\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.750779601440512\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7663035116628651\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7311852674818022\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7320510040547616\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7555208713830615\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7811037298221948\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7129116119246577\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7475509566169989\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7371937335592218\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.751064105039228\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.72431543717951\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7546026415704425\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7796709514686688\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7350425248767432\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7591151329809054\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.776748270655836\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.8004102827683726\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7854414558982217\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7510310684380392\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.737939082365541\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7710941554370987\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.771283834585455\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.797059279779508\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7977948132450734\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.8049286794761505\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.8082198497240558\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.806213754222056\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8164821883606386\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7671754562876787\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7903686274351904\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7343367993744494\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.729073591838163\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7262044002004493\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7465001460801608\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7568508676735\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.765349149648023\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7703079411128626\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.74141036488374\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8142121522872776\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.8425763984087025\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7900720993852752\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7437975235974188\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.735519203673851\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7319122044480304\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7889716286339863\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.851521797883141\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.8701510995934398\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -3.7538907512245667\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.9727694185482685\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -2.088785440606175\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.87965656541027\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.813004022639214\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7785156462718128\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.79912756702215\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -2.1223508042431822\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -2.900446061739338\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -2.5184282241812337\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -2.6555230507137515\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -2.482464182202891\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -2.363662347234725\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -2.2422649699864654\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -2.4491870362213892\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -2.088466453992835\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -2.0512891481105764\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7983966448060975\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7775222983200354\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.8676151766446878\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.8263947105132905\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7969899336741606\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.78520790132278\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -4.172474999914202\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7932713138227956\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7662687548370297\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7428319455019772\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7694562136627627\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.768051836326677\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7860599240496402\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7668683515052248\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7879062636235092\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7799506748172842\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7918139214879982\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.8024783663727697\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.8052009403619127\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7956722656892512\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7735802054053322\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.810669128142753\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.8050747013928055\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7906839566298465\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.8000432101353232\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7866093859302958\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.8085975270203452\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.8046252444391382\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.8141804134225736\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.8087539836125341\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7966584515472288\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7903055797213994\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7611975360017085\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7612899348761357\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7617386470426362\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7586780595240077\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7685652683654984\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7687657088455615\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.746070030010028\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7336373235171338\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7441882056059916\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7471611104446045\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.8020702852990353\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.739565766054292\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7397815431214338\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7631926783932312\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7402146876245792\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7378190798638204\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7098544759147278\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7237150877580099\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.722591341673404\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7432591584152477\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7355798161985376\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7474286613228067\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.737417952097232\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7233708559587837\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7397120715998131\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7469325020932536\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.8137374155255332\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.803402295263096\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7690683243588137\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7346559100997332\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.757624616130455\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.78647394129203\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.766174519280061\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7790232485033146\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.8102123786962412\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7974765596112037\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7536300569392573\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -20.764965763668496\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.03075025771646\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.349924481470165\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.99233216627241\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.46263460562166\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -24.106690251284945\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.833247458378395\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -25.02023929801844\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -21.899243723175076\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.98054406120303\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.655756230551013\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -21.663005107737735\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.07449101186938\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.125737703250646\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.431361649484963\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -45.46912063175725\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -22426.877735601618\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -17003.396656014804\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1140.3229380175428\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -4.006010211391246\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -5.8608512356681475\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -4.251791468126825\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.301616280252104\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.1835030107482094\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -3.3444240846289652\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -3.182855153446769\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.4919696764520225\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.2670278643779427\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.2915003813779573\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.3124098503833452\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.136357657421948\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.1750260348954464\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.178419696565487\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.1292564009240906\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.3102308549629824\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.418370594408091\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.319245046600196\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.191314042976613\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.232894587695368\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.153124767175568\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.31544230125878\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.27876039031196\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.345934023966291\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.4458543080542876\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.5006578744188035\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.4743369563277846\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.573412068385603\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.742600935143813\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.636468123374858\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -2.645035240883498\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.5940653817915584\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -2.417648118779639\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.3773886753984614\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -2.2614712168937414\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.231555338693349\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.41605960438569\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.3208751602164157\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.4947097919125927\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.2870590270991533\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.2199478781168427\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.2783182324412303\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.0755967510647544\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -2.0147189403980366\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.0666649371725936\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -2.1499151256661952\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -2.1541402147936277\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -2.286334775884842\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -2.809982682771427\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -4.2268977598764295\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -3.515677999408954\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -3.4220155761977638\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.8979219233358604\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.2950642823445757\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.1820723698199003\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -2.1304716415301415\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -2.126939139454962\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.1175149878534016\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.149247952332868\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -2.180811579801661\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.1817131912713745\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.129035675243773\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.0939967226926908\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.1092937925367754\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.098908902973578\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -2.3986171944575676\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.14130483049205\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.136376335990007\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -2.165350998423432\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.1550698781166715\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -2.141533668050323\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.153387932183242\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.1651778579452725\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -2.15593506997433\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -2.171138591506516\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -2.1749726919037244\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -2.164269724128394\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -2.1578412155300906\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.161447898024419\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -2.0655813996988113\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -2.099886318888762\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -2.099389757703166\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -2.0354842668250996\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.9999168747059297\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.999233673963781\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.9414370065217335\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.9175210604229795\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.87180386224958\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7964086383374813\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7921359883815606\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7649281072474319\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8413297427290336\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7938560023354022\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7640888747027865\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7688704101869877\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.8014892832286353\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.8060128480262065\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.8399373810563047\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.864833004139335\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.938331425249813\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.9616942415641505\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.937509413610948\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.9182725658711175\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.900134864674318\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.9200681708646214\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.9167400522991207\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.9259569207636138\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.8978375300306123\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -2.0024524097330016\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.9845854660899684\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.9794344708833342\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.9182463528652374\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.894378634381802\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.849474259971581\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.8289491733785532\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.8100955363237348\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7925029343879588\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7967147459515747\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.791385056598616\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.804992929432139\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7548106063674311\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7841167921357755\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.796924084435384\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7615547023105926\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7759379316779842\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7575542289108599\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.8166331169437524\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.755760739243567\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7533468128332248\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.752838560111432\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7615049701722199\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7558547897861616\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7813691957064872\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.723442475371479\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7524710217455675\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.757663430444141\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7298319198600525\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.726259816388066\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7687479309862686\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.747818151447791\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7483255735763186\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7376458860516635\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.777061527029653\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7462993150792192\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.756083680582247\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7613701994016702\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7648744993705763\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.8051339666969661\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.764937911646383\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7831165907257494\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7520012996875622\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7660007925480097\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.762469763999854\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.8002647761769774\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.8761820222614751\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.8975484582822917\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -2.087891907831415\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -2.086529480582463\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.927834382644095\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.934413870266107\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.8666017612186625\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.8595528886311274\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.9039607811274895\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.872692568160893\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -2.208833388305176\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.8449534934691516\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.8897649335156472\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7851300282054274\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7593464182970324\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7675442109622554\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.8401713098615642\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.775179299299368\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7438241206058784\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.760474786779963\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.763489381939638\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7611087819849072\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7539287547291915\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.767083023887447\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7703662380621086\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7495786585815325\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.746033490256849\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -23.258905808367818\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -26.5693754468727\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.376241041722484\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.877720344738254\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.978538941207315\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.120723727737815\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -21.06528784851136\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.61809620586535\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.25799536432428\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.259565600611058\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.088298247201454\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.723136092246897\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.63179796592476\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -24.734710124301927\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.099340698892494\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -63.26581944117503\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -7425.380445217291\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -57.74660035947948\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2307.960563725674\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -4319.789905093728\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2903.0731570163916\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -543.644553918854\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.738249553823296\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -3.098267436880283\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -3.7929909960230304\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -3.0569575831283875\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -3.171712670580109\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -3.6849559105174445\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -3.8640446721119543\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -4.043173663707701\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -3.131038339287392\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -3.0016635459782663\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.7475425945425673\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.796507428381136\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.7919673784666816\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.623803185321338\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -3.084912413548493\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.8123305273302925\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.408254744255194\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.6104474001996745\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.967156633822248\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -4.122241851552842\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -4.727270087664539\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -5.99165556903695\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -7.5335027036103135\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -13.872201466239476\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -16.180604882422106\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -4.50657045880776\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -3.479823861006939\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -4.867463997507782\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -6.569147217572565\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -5.694853655693084\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -5.724772367711236\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -5.374360430198139\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -5.518070281333319\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -5.0578830573092475\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -4.79046418902316\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -5.143368234867271\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -4.394518475574839\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -3.2836091023214147\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.9624650442153517\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -3.4188764023614704\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -3.425715572132691\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -3.436297472308354\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -3.321074759232112\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -3.249662838008427\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -3.2045754777128144\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -5.868936166279236\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -3.130631503842403\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -3.036078164050814\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -2.895229829474208\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.7751988798874483\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.645247417761774\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.5923098344211555\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -2.6048793333534532\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -2.839831021920278\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -3.1255651295736273\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.8372124216511816\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -2.1857333098817615\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.1636667685102093\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.103750709309402\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.170282745535262\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.079919419357176\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.033273526016365\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.9481705713998325\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.8796502393019403\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.904019089245653\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.858485623029672\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.8699267665532318\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8414406438518298\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.9313919864373283\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.9718170095111063\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.9280628881049375\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.97063660789476\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.9170518270795969\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.9780721879929688\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.995401437806596\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.9491042626285264\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.9534430755992367\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.9807984205460858\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.914228996787709\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.9358622433083064\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.9409084599119\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.9712142904430454\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.9507068099788472\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.990903174887727\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.9490926700978932\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -2.1227282250965276\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -2.2724927786835143\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -2.217632774729779\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -2.161116935947705\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -2.014030105520571\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.965431434566375\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.975675569460901\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.9335729046739494\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.9239366247763399\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.9429598407068138\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.9768366845863603\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.9243477389400903\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.9021078714223358\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.9433210106559862\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -2.004498171723396\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -2.1220360128690667\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.9266596363935695\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.877314297478241\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.8507953361214278\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.85487028820329\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.859476951506905\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.887310397111789\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.8584246407703022\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.8848308437411552\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.8334653575653204\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.8963651609688017\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.9746404300530975\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.8652344980272948\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.9421264452611786\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.885139276669709\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -2.641120828473056\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.934905572464278\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.8755337474888574\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.864074284980375\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.8645901443256552\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7772972037467218\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.8453626127886764\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.8286981229754136\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.8230805335016464\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7887324770255077\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.811734754068708\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.8112307277859523\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.8804433899755169\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -2.265057565971369\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -2.6438040284403352\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.82432185966168\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.8531603865409483\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.8911212546846219\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.845638171442201\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.8768581093118504\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7994124253032981\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7757353278256067\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.785727425776293\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7484143467493365\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.752459221744231\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7806118026991535\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7303557949290473\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.758122560923504\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7534452802067866\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.76554488471571\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7498764581244324\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7204540020469687\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.75369855381381\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.757891208831948\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.746944949479171\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.743457768869614\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.741235144897866\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7403529355645206\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7591170030917318\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.772190613564673\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7644272288966727\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7872050041320613\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.801656312945599\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7893228518033037\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.793931539867616\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7575235010982018\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7498607804485191\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.714725611920472\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7157426626208212\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7068935422585099\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7257280749550916\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7362962423483885\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7360691050759882\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7161701559277907\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7077230664197742\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7682445562286904\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.8558185508882203\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7570901626451667\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7451902791875598\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7509341041225628\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.752091021616982\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7344618443901352\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7029180411427938\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.10264257194159\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.48966617221551\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -22.411330608262887\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -24.249507732162346\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.20746032480447\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.183966224433654\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.100438850678515\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -25.51116325246484\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -26.141020164325138\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -24.14036146032181\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.583492040718774\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -26.99005012253594\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.102824676498685\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.89374211988294\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.25853743790961\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -409.296880341434\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -6634.924548235627\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -4.178493178708504\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -5.067382897434733\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -3.292622119905399\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.8055611289625735\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -3.4782685767045836\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.619860737133428\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.4098580365614897\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.335835710961698\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.2034318779259747\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.3029910504333686\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.310942130644472\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.3866432772195902\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.44038601128193\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.398454412384905\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.646953727417623\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.3578920905665326\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.5239178553964763\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.612247062760249\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.586254338137816\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.5839947900607787\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.445718737186401\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.507635422032974\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.3958396490433542\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.29706061505485\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.2489998394915935\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.2862005250290265\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.241368390854052\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.2506587069865542\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.3397469073739665\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.268103133388949\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.315293191414075\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.204786473107219\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -2.207634345555267\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -2.2468701030888796\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -2.1284902455963497\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.2725271011742865\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -2.1338597214215924\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.1796868190738543\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.2787470543204553\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.0358818101293603\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.115834747266623\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.1295141768909547\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.232244191877824\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.0450994730634795\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.078059592341001\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -2.1815433693801696\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.0724562112921174\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -2.166942147286187\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -2.07066220740077\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.9834602088437845\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.9747714613167477\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.011221972551811\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8832077385368826\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8414789298257233\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8664528752342975\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8440271188374413\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.8411790443623048\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.8941432659207122\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.8575287487225907\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.8347755592251165\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.8884961316822093\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8998988214817996\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.9453989955649142\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.2003130456322335\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.108399576444915\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.327639237863774\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.528618419483649\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -3.2734253897455305\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -4.315497280030106\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.2395837315901512\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.8846095162150414\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.8768417892383422\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.845741912299847\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.8351341684913904\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7915520685680866\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7948654810768556\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.8532790810821858\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.8750283858404295\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.8772041371052524\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.9273294943137755\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.9631490262046978\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -2.015255760019827\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -2.0501613343132066\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.857109155350585\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.858611002830317\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.8566561719249726\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.8232653042575173\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.8391014078992742\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8257737051615672\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.812304764321068\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -2.917939929459142\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7473271018504761\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7338961507434818\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7386746244346287\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7360575511083038\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7163027142982503\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7366070939296918\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7302621791011747\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7226773420373969\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.719924997041963\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7077918518271356\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.696102669054124\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7325903098100712\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7237032014769715\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.711257081327905\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7388690773847204\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7230315293194414\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7100576928218623\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.707360481933301\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6859861242994993\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.683125385071332\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.6663025407297352\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.701064939635395\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.6488966618163534\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7204815040620804\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7026205031165398\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7092787333096615\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.722229082222383\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7134272300660922\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7020794701305844\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7074922572765465\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7110096791546499\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7188061973481064\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7570813145262159\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.728025308069305\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7329203594807583\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7174858613716961\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7132280053856028\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.6804428415292068\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.712082868142864\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.687941588784349\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.6808690648177185\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7180109113044937\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6943397534149378\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7061561480357301\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7226196091261141\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7008141058181923\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7053862022707955\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7260910463148764\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.695396120194385\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6971481532694401\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7156512057261033\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7026049509006258\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6974163304019134\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.6972134064407152\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7028057244016106\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7048317510943432\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.739891443894444\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.711330076160465\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7035722196860366\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7325009832400031\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7516595158816057\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7733978886142399\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7826552542547198\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.8163673072382387\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.8501837903515141\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.8992907717650014\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -2.0818253877248365\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.9729746915986448\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.9292106299102567\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.9602745902767118\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.9239133140855156\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.8366035511498018\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.858561170051687\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.8005159815430785\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.8557385651942695\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.832918192515379\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.8422196942583002\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.8127594632253454\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7810045742255263\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.8415662629046035\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7608835140530734\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.768906251722964\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7666823876541082\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7220602661834115\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7085112113282803\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7056897544323948\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7341842897176094\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6990049766233728\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7148857179488892\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7184917078159432\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.721528043886098\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.695940572097441\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.137199234533313\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -25.051471673958023\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -20.76865230975247\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.350160071304682\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.843577455586797\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.160738556776728\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.540966143383162\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.158957392120893\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.42585560159232\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.275300441083758\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -21.550928946698413\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.546574543017588\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.32829553220996\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -20.470237196853247\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -24.41046641735016\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -50.36122398699717\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -6.776359585304018\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.719001424028586\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.261566264090844\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.0473084716127636\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.9637471005433196\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.040443351928957\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.023994737290001\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.9027225806139354\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.815475860090422\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8354194325815945\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.7883271285771825\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.828313253532713\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7969133621909281\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.800318025260118\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7691863456397323\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7785579460249692\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7384408831857996\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7553314312540584\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.74807445207208\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7572214695042265\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7213036289455934\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7693953433223124\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7538453414081936\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7056944197912207\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.722412291886526\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7506040050146847\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7291610510194815\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.779509844744915\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7502119007442183\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7189233101945691\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7350716611089303\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7624100123341981\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7399834057735724\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7416537743472298\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7580599171352354\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7492844327790684\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.745937150525361\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7626402079838845\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7549355709095453\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.762475277950744\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7430253641912656\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7943579016385263\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8110893916805728\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7493153479703316\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7465787390957956\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7604115328821832\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.804875351407515\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7500672472553236\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7510808460053786\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.742370280980955\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7739021723528174\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7496714563640825\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7561976169271984\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7439680819669223\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7170320245062962\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7513065239947634\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7325218115365728\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7326907204593816\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7251842310725243\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7304292257360618\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7649402814113242\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7108358432240822\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.725361075745795\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7358452819909098\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7561405353086763\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7544679270287278\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7204488594298357\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7695858439294154\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7019788002854088\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7440239204639068\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.783803331067822\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7367521516023492\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.6828317646171276\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.736538923644804\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7603736019732383\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.753957099312077\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7014664839781557\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7336160473950701\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7064770222639418\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7249674176681549\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.6915992120368544\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7214629396101753\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.6673754682774693\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7091107301635111\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.6985310753488099\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.6837984929009928\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7031878518653005\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.712269402549037\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7139130195517203\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.6993852254690602\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.696639306590141\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7057812179291278\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7093672913044384\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7151131701828695\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.714121281377998\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.6913925142648634\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.6970226635032748\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.689084368126829\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.6885649467211896\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.6962687704446084\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6825805759007904\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.6861527911512235\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.6792945418926433\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7013658981876176\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7004012553164114\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.6954362059105115\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.6919490486244984\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.686660804883125\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6907824166877878\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.699566163631836\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6925737159405563\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.685666612002401\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7180356017386333\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7165677438191778\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7065307066970457\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7142936293886624\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7123529781357758\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7018013282960212\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.691195840513112\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7209536762813276\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7016113625582348\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.6929205525899615\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.6802406576023148\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.690652504404443\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7128065041594516\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.6904616329695046\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7127537366850738\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.690845693811713\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.6877568470112387\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.694430794365588\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.703878805665032\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7246194678111453\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7203879488457803\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7547826030322449\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7452762463390061\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.736691652646974\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7119031423650222\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.689395107556114\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.6872889553282504\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6828389234718446\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6909600552832893\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6843933110353477\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.703342920019371\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.6795658446275963\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6902638224803668\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.692938045894417\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6955595726678294\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7018550572687496\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.6921950060088455\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.6830083193524572\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6786323028007495\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6959358828626847\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7025797277625137\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6910168224577578\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7203978001619626\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6998351697176588\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6917094254914176\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6873791126150828\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7126870485585892\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7154252251375826\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7005346468769944\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7043986250944834\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7225601237264982\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.727814388401225\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7152695495876846\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7314049506937692\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7895086322963039\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.6871245774040133\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.690508118860733\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7199625914402832\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6834548593312626\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6823724254132002\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.681525665550326\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.6807411273122461\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.6732496642194203\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.667061902005422\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.672537549304429\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.671413574968493\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6792547485348595\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6715503469034396\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6870560091046474\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6825876226120218\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6677232663204307\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.628736730467582\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9020c7af0bf75c52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
