{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:15.918379Z",
     "start_time": "2026-01-07T03:48:14.911963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.sys_ids import *\n",
    "from utils.helpers import *"
   ],
   "id": "16046c61cdef4105",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "d65ea31bb0ba4726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:15.923932Z",
     "start_time": "2026-01-07T03:48:15.921577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "6f63cf0fe662fad6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:15.929954Z",
     "start_time": "2026-01-07T03:48:15.926991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "\n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "312c969b12ceab62",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:15.934385Z",
     "start_time": "2026-01-07T03:48:15.932662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "20b3282fe3b9878d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:15.966529Z",
     "start_time": "2026-01-07T03:48:15.964462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "3d3817e96f14e9ee",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:16.412193Z",
     "start_time": "2026-01-07T03:48:16.409652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "ecbcb8e16c222297",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "fb386a99f0388bef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:18.604727Z",
     "start_time": "2026-01-07T03:48:18.602646Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "b29950a230ed3c5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:18.612054Z",
     "start_time": "2026-01-07T03:48:18.605876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "261a1a0e23a27a70",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:18.636732Z",
     "start_time": "2026-01-07T03:48:18.634577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "df6de58faa62cf48",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:18.640860Z",
     "start_time": "2026-01-07T03:48:18.637666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "1e16a87713c5df23",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:18.814661Z",
     "start_time": "2026-01-07T03:48:18.812038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "de518a763149ab18",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:19.069077Z",
     "start_time": "2026-01-07T03:48:19.066175Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "e8ddb14425216720",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:19.599380Z",
     "start_time": "2026-01-07T03:48:19.596453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "eec5ea6a972b1fbe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:20.074204Z",
     "start_time": "2026-01-07T03:48:20.071624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "7702cab2e8308754",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:23.250083Z",
     "start_time": "2026-01-07T03:48:23.247005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False]\n",
    "warm_start = 0\n",
    "ACTOR_FREEZE = 0\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "ab0c357ae390714d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:24.854837Z",
     "start_time": "2026-01-07T03:48:24.839385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "33e3d7ed5920f8fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:26.313248Z",
     "start_time": "2026-01-07T03:48:26.310223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "n_inputs = 2\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "u_ss = apply_min_max(steady_states['ss_inputs'], data_min[:n_inputs], data_max[:n_inputs])\n",
    "b_min = apply_min_max(np.array([71.6, 78]), data_min[:n_inputs], data_max[:n_inputs])\n",
    "b_max= apply_min_max(np.array([870, 670]), data_min[:n_inputs], data_max[:n_inputs])\n",
    "b1 = (b_min[0]-u_ss[0], b_max[0]-u_ss[0])\n",
    "b2 = (b_min[1]-u_ss[1], b_max[1]-u_ss[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(n_inputs*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1\n",
    "R2_penalty = 1"
   ],
   "id": "6d420f1bb2abcbe9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:26.856257Z",
     "start_time": "2026-01-07T03:48:26.853984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "cc17562ccfd9dbb1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:27.841848Z",
     "start_time": "2026-01-07T03:48:27.835561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=5.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward with relative tracking bands.\n",
    "\n",
    "    data_min, data_max : arrays for [u_min..., y_min...], [u_max..., y_max...]\n",
    "    n_inputs           : number of inputs (so outputs start at index n_inputs)\n",
    "    k_rel              : per-output relative tolerance factors (same length as outputs)\n",
    "    band_floor_phys    : per-output minimum band in physical units\n",
    "    Q_diag, R_diag     : quadratic weights (same as before)\n",
    "    \"\"\"\n",
    "\n",
    "    data_min = np.asarray(data_min, float)\n",
    "    data_max = np.asarray(data_max, float)\n",
    "    dy = np.maximum(data_max[n_inputs:] - data_min[n_inputs:], 1e-12)  # phys range for each y\n",
    "\n",
    "    k_rel = np.asarray(k_rel, float)\n",
    "    band_floor_phys = np.asarray(band_floor_phys, float)\n",
    "    Q_diag = np.asarray(Q_diag, float)\n",
    "    R_diag = np.asarray(R_diag, float)\n",
    "\n",
    "    # floor in *scaled* coordinates (used if y_sp_phys is not provided)\n",
    "    band_floor_scaled = band_floor_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "    def _sigmoid(x):\n",
    "        x = np.clip(x, -60.0, 60.0)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _phi(z, kind=bonus_kind, k=bonus_k, p=bonus_p, c=bonus_c):\n",
    "        z = np.clip(z, 0.0, 1.0)\n",
    "        if kind == \"linear\":\n",
    "            return 1.0 - z\n",
    "        if kind == \"quadratic\":\n",
    "            return (1.0 - z) ** 2\n",
    "        if kind == \"exp\":\n",
    "            return (np.exp(-k * z) - np.exp(-k)) / (1.0 - np.exp(-k))\n",
    "        if kind == \"power\":\n",
    "            return 1.0 - np.power(z, p)\n",
    "        if kind == \"log\":\n",
    "            return np.log1p(c * (1.0 - z)) / np.log1p(c)\n",
    "        raise ValueError(\"unknown bonus kind\")\n",
    "\n",
    "    def reward_fn(e_scaled, du_scaled, y_sp_phys=None):\n",
    "        \"\"\"\n",
    "        e_scaled : output error in scaled deviation space  (same as before)\n",
    "        du_scaled: input move in scaled deviation space    (same as before)\n",
    "        y_sp_phys: current setpoint in *physical* units (array len = n_outputs)\n",
    "        \"\"\"\n",
    "\n",
    "        e_scaled = np.asarray(e_scaled, float)\n",
    "        du_scaled = np.asarray(du_scaled, float)\n",
    "\n",
    "        # ----- dynamic band based on setpoint -----\n",
    "        if y_sp_phys is None:\n",
    "            # fallback: just use the floor\n",
    "            band_scaled = band_floor_scaled\n",
    "        else:\n",
    "            y_sp_phys_arr = np.asarray(y_sp_phys, float)\n",
    "            # band_phys_i = max(k_rel_i * |y_sp_i|, band_floor_phys_i)\n",
    "            band_phys = np.maximum(k_rel * np.abs(y_sp_phys_arr), band_floor_phys)\n",
    "            band_scaled = band_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "        tau_scaled = tau_frac * band_scaled\n",
    "\n",
    "        # ----- inside/outside gate -----\n",
    "        abs_e = np.abs(e_scaled)\n",
    "        s_i = _sigmoid((band_scaled - abs_e) / np.maximum(tau_scaled, 1e-12))\n",
    "\n",
    "        if gate == \"prod\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64))\n",
    "        elif gate == \"mean\":\n",
    "            w_in = float(np.mean(s_i))\n",
    "        elif gate == \"geom\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64) ** (1.0 / len(s_i)))\n",
    "        else:\n",
    "            raise ValueError(\"gate must be 'prod'|'mean'|'geom'\")\n",
    "\n",
    "        # ----- core quadratic costs -----\n",
    "        err_quad = np.sum(Q_diag * (e_scaled ** 2))\n",
    "        err_eff = (1.0 - w_in) * err_quad + w_in * (lam_in * err_quad)\n",
    "        move = np.sum(R_diag * (du_scaled ** 2))\n",
    "\n",
    "        # ----- linear penalties around band edge -----\n",
    "        slope_at_edge = 2.0 * Q_diag * band_scaled\n",
    "\n",
    "        overflow = np.maximum(abs_e - band_scaled, 0.0)\n",
    "        lin_out = (1.0 - w_in) * np.sum(gamma_out * slope_at_edge * overflow)\n",
    "\n",
    "        inside_mag = np.minimum(abs_e, band_scaled)\n",
    "        lin_in = w_in * np.sum(gamma_in * slope_at_edge * inside_mag)\n",
    "\n",
    "        # ----- bonus near zero error -----\n",
    "        qb2 = Q_diag * (band_scaled ** 2)\n",
    "        z = abs_e / np.maximum(band_scaled, 1e-12)\n",
    "        phi = _phi(z)\n",
    "        bonus = w_in * beta * np.sum(qb2 * phi)\n",
    "\n",
    "        # ----- total reward -----\n",
    "        return -(err_eff + move + lin_out + lin_in) + bonus\n",
    "\n",
    "    params = dict(\n",
    "        k_rel=k_rel,\n",
    "        band_floor_phys=band_floor_phys,\n",
    "        band_floor_scaled=band_floor_scaled,\n",
    "        Q_diag=Q_diag,\n",
    "        R_diag=R_diag,\n",
    "        tau_frac=tau_frac,\n",
    "        gamma_out=gamma_out,\n",
    "        gamma_in=gamma_in,\n",
    "        beta=beta,\n",
    "        gate=gate,\n",
    "        lam_in=lam_in,\n",
    "        bonus_kind=bonus_kind,\n",
    "        bonus_k=bonus_k,\n",
    "        bonus_p=bonus_p,\n",
    "        bonus_c=bonus_c,\n",
    "    )\n",
    "    return params, reward_fn"
   ],
   "id": "405d1b3a683b64e3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reward configuration",
   "id": "35cc17c0469fb22c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:29.231939Z",
     "start_time": "2026-01-07T03:48:29.228333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_inputs = 2\n",
    "\n",
    "dy = data_max[n_inputs:] - data_min[n_inputs:]\n",
    "y_sp_nom = 0.5 * (data_min[n_inputs:] + data_max[n_inputs:])\n",
    "\n",
    "k_rel = np.array([0.003, 0.0003])\n",
    "band_floor_phys = np.array([0.006, 0.07])\n",
    "\n",
    "band_phys = np.maximum(k_rel * np.abs(y_sp_nom), band_floor_phys)\n",
    "\n",
    "scale_factor = 1.0  # use 2.0 for [-1, 1] scaling, 1.0 for [0, 1]\n",
    "band_scaled = scale_factor * band_phys / dy\n",
    "\n",
    "q0 = 1.4\n",
    "Q_diag = q0 / np.maximum(band_scaled ** 2, 1e-12)\n",
    "\n",
    "print(\"dy:\", dy)\n",
    "print(\"y_sp_nom:\", y_sp_nom)\n",
    "print(\"band_phys:\", band_phys)\n",
    "print(\"band_scaled:\", band_scaled)\n",
    "print(\"Q_diag:\", Q_diag)"
   ],
   "id": "3127480f60d88e04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: [0.22165278 0.78153727]\n",
      "y_sp_nom: [  3.83915067 323.21371982]\n",
      "band_phys: [0.01151745 0.09696412]\n",
      "band_scaled: [0.05196169 0.12406845]\n",
      "Q_diag: [518.51529284  90.95055189]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T03:48:31.169230Z",
     "start_time": "2026-01-07T03:48:31.166178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_diag = np.array([518., 90.])          # rounded from the band-based calculation\n",
    "R_diag = np.array([90., 90.])          # move cost for du_scaled ~ 0.02\n",
    "\n",
    "n_inputs = 2\n",
    "\n",
    "print(\"Band scaled are:\")\n",
    "\n",
    "params, reward_fn = make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=7.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    ")\n",
    "print(params)"
   ],
   "id": "6d5c1f7974536a2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band scaled are:\n",
      "{'k_rel': array([0.003 , 0.0003]), 'band_floor_phys': array([0.006, 0.07 ]), 'band_floor_scaled': array([0.02706937, 0.08956707]), 'Q_diag': array([518.,  90.]), 'R_diag': array([90., 90.]), 'tau_frac': 0.7, 'gamma_out': 0.5, 'gamma_in': 0.5, 'beta': 7.0, 'gate': 'geom', 'lam_in': 1.0, 'bonus_kind': 'exp', 'bonus_k': 12.0, 'bonus_p': 0.6, 'bonus_c': 20.0}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T21:50:54.250350Z",
     "start_time": "2026-01-07T21:50:54.247378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "3478789e7ff228e1",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T21:50:54.674682Z",
     "start_time": "2026-01-07T21:50:54.667986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_mpc(system, MPC_obj, y_sp_scenario, n_tests, set_points_len,\n",
    "            steady_states, IC_opt, bnds, cons,\n",
    "            L, data_min, data_max,\n",
    "            test_cycle, reward_fn,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change,\n",
    "            Q1_penalty, Q2_penalty, R1_penalty, R2_penalty, mode=\"disturb\"):\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "    y_mpc = np.zeros((nFE + 1, n_outputs))\n",
    "    y_mpc[0, :] = system.current_output\n",
    "    u_mpc = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    rewards = np.zeros(nFE)\n",
    "    rewards_mpc = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "    avg_rewards_mpc = []\n",
    "\n",
    "    # Recording\n",
    "    delta_y_storage, delta_u_storage = [], []\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # Solving MPC optimization problem\n",
    "        sol = spo.minimize(\n",
    "            lambda x: MPC_obj.mpc_opt_fun(x, y_sp[i, :], scaled_current_input_dev,\n",
    "                                          xhatdhat[:, i]), IC_opt, bounds=bnds, constraints=cons)\n",
    "\n",
    "        # take the first control action (this is in scaled deviation form)\n",
    "        u_mpc[i, :] = sol.x[:MPC_obj.B.shape[1]] + ss_scaled_inputs\n",
    "\n",
    "        # u (reverse scaling of the mpc)\n",
    "        u_plant = reverse_min_max(u_mpc[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_mpc[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "\n",
    "        # Disturbance\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record the system output\n",
    "        y_mpc[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_mpc[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_mpc[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = (np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_mpc[i, :] - ss_scaled_inputs))\n",
    "                            + np.dot(L, (y_prev_scaled - yhat[:, i])).T)\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u, y_sp_phys)\n",
    "\n",
    "        # Reward MPC\n",
    "        reward_mpc = - (Q1_penalty * delta_y[0] ** 2 + Q2_penalty * delta_y[1] ** 2 +\n",
    "                        R1_penalty * delta_u[0] ** 2 + R2_penalty * delta_u[1] ** 2)\n",
    "\n",
    "        # Recording\n",
    "        rewards[i] = reward * 0.01\n",
    "        rewards_mpc[i] = reward_mpc\n",
    "        delta_y_storage.append(delta_y)\n",
    "        delta_u_storage.append(delta_u)\n",
    "\n",
    "        # Calculate average reward and printing\n",
    "        if i in sub_episodes_changes_dict.keys():\n",
    "            # Averaging the rewards from the last setpoint change till curtrent\n",
    "            avg_rewards.append(np.mean(rewards[i - time_in_sub_episodes + 1: i]))\n",
    "            avg_rewards_mpc.append(np.mean(rewards_mpc[i - time_in_sub_episodes + 1: i]))\n",
    "\n",
    "            # printing\n",
    "            print('Sub_Episode : ', sub_episodes_changes_dict[i], ' | avg. reward :', avg_rewards[-1], ' | avg. reward MPC :', avg_rewards_mpc[-1])\n",
    "\n",
    "    u_mpc = reverse_min_max(u_mpc, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_mpc, u_mpc, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, delta_u_storage, rewards_mpc, avg_rewards_mpc"
   ],
   "id": "b5c7c6048d1e1033",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:06:03.925999Z",
     "start_time": "2026-01-07T21:50:58.673151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_mpc, u_mpc, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, delta_u_storage, rewards_mpc, avg_rewards_mpc = run_mpc(cstr, MPC_obj, y_sp_scenario, n_tests, set_points_len,\n",
    "            steady_states, IC_opt, bnds, cons,\n",
    "            L, data_min, data_max,\n",
    "            TEST_CYCLE, reward_fn,\n",
    "            nominal_qi, nominal_qs, nominal_hA,\n",
    "            qi_change, qs_change, ha_change,\n",
    "            Q1_penalty, Q2_penalty, R1_penalty, R2_penalty)"
   ],
   "id": "e1bf8b7a985f608b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode :  1  | avg. reward : -3.3723926917720055  | avg. reward MPC : -3.256132676655319\n",
      "Sub_Episode :  2  | avg. reward : -4.42582380786233  | avg. reward MPC : -4.281643069710799\n",
      "Sub_Episode :  3  | avg. reward : -4.427961701146985  | avg. reward MPC : -4.281334784994731\n",
      "Sub_Episode :  4  | avg. reward : -4.428531108297815  | avg. reward MPC : -4.28067068404145\n",
      "Sub_Episode :  5  | avg. reward : -4.42854457749651  | avg. reward MPC : -4.28002909163142\n",
      "Sub_Episode :  6  | avg. reward : -4.429170468765406  | avg. reward MPC : -4.280150711667606\n",
      "Sub_Episode :  7  | avg. reward : -4.429557516877902  | avg. reward MPC : -4.280060132991141\n",
      "Sub_Episode :  8  | avg. reward : -4.429856834775232  | avg. reward MPC : -4.27995599636914\n",
      "Sub_Episode :  9  | avg. reward : -4.4304604651737165  | avg. reward MPC : -4.280034246829293\n",
      "Sub_Episode :  10  | avg. reward : -4.4310747216926325  | avg. reward MPC : -4.2800126878822224\n",
      "Sub_Episode :  11  | avg. reward : -4.4317352657710245  | avg. reward MPC : -4.279990555439346\n",
      "Sub_Episode :  12  | avg. reward : -4.4323545584248105  | avg. reward MPC : -4.279819600408938\n",
      "Sub_Episode :  13  | avg. reward : -4.4329351605583796  | avg. reward MPC : -4.279651558543717\n",
      "Sub_Episode :  14  | avg. reward : -4.433863075185052  | avg. reward MPC : -4.279886210289419\n",
      "Sub_Episode :  15  | avg. reward : -4.434274873766368  | avg. reward MPC : -4.279707365748201\n",
      "Sub_Episode :  16  | avg. reward : -4.434470334338074  | avg. reward MPC : -4.279363334152598\n",
      "Sub_Episode :  17  | avg. reward : -4.435298210815817  | avg. reward MPC : -4.279594559113361\n",
      "Sub_Episode :  18  | avg. reward : -4.435993279611669  | avg. reward MPC : -4.279727723863179\n",
      "Sub_Episode :  19  | avg. reward : -4.436247981224239  | avg. reward MPC : -4.279466526938328\n",
      "Sub_Episode :  20  | avg. reward : -4.437068414610565  | avg. reward MPC : -4.279702273857163\n",
      "Sub_Episode :  21  | avg. reward : -4.437284755718679  | avg. reward MPC : -4.279372320669284\n",
      "Sub_Episode :  22  | avg. reward : -4.437871631117838  | avg. reward MPC : -4.279384343318029\n",
      "Sub_Episode :  23  | avg. reward : -4.438372267759192  | avg. reward MPC : -4.279307551761916\n",
      "Sub_Episode :  24  | avg. reward : -4.439309959132846  | avg. reward MPC : -4.279638742594663\n",
      "Sub_Episode :  25  | avg. reward : -4.43964288447831  | avg. reward MPC : -4.27941742329365\n",
      "Sub_Episode :  26  | avg. reward : -4.440373143043904  | avg. reward MPC : -4.279581405331437\n",
      "Sub_Episode :  27  | avg. reward : -4.44101425622447  | avg. reward MPC : -4.27966866140619\n",
      "Sub_Episode :  28  | avg. reward : -4.441292777589474  | avg. reward MPC : -4.279422137929133\n",
      "Sub_Episode :  29  | avg. reward : -4.441921751735745  | avg. reward MPC : -4.279505485185733\n",
      "Sub_Episode :  30  | avg. reward : -4.442528568524326  | avg. reward MPC : -4.279571324899361\n",
      "Sub_Episode :  31  | avg. reward : -4.443027254563316  | avg. reward MPC : -4.279539518462654\n",
      "Sub_Episode :  32  | avg. reward : -4.443995091383239  | avg. reward MPC : -4.2799447894473355\n",
      "Sub_Episode :  33  | avg. reward : -4.444314496854382  | avg. reward MPC : -4.279758617906862\n",
      "Sub_Episode :  34  | avg. reward : -4.444723000298527  | avg. reward MPC : -4.279649779884498\n",
      "Sub_Episode :  35  | avg. reward : -4.445283696047117  | avg. reward MPC : -4.279684822404764\n",
      "Sub_Episode :  36  | avg. reward : -4.445769505431049  | avg. reward MPC : -4.279631935300773\n",
      "Sub_Episode :  37  | avg. reward : -4.445955198861791  | avg. reward MPC : -4.279295349321833\n",
      "Sub_Episode :  38  | avg. reward : -4.446792057233987  | avg. reward MPC : -4.2795509721834035\n",
      "Sub_Episode :  39  | avg. reward : -4.447262631440737  | avg. reward MPC : -4.279486900131341\n",
      "Sub_Episode :  40  | avg. reward : -4.448029180139644  | avg. reward MPC : -4.279706360044826\n",
      "Sub_Episode :  41  | avg. reward : -4.448348566957491  | avg. reward MPC : -4.279491640779812\n",
      "Sub_Episode :  42  | avg. reward : -4.44910222108681  | avg. reward MPC : -4.279683429920868\n",
      "Sub_Episode :  43  | avg. reward : -4.4497353525260435  | avg. reward MPC : -4.279753216463572\n",
      "Sub_Episode :  44  | avg. reward : -4.450176924091808  | avg. reward MPC : -4.279645491965854\n",
      "Sub_Episode :  45  | avg. reward : -4.450888667002876  | avg. reward MPC : -4.279811517977231\n",
      "Sub_Episode :  46  | avg. reward : -4.451448670700673  | avg. reward MPC : -4.279831472676424\n",
      "Sub_Episode :  47  | avg. reward : -4.4520429108940585  | avg. reward MPC : -4.279877007510398\n",
      "Sub_Episode :  48  | avg. reward : -4.452754653973805  | avg. reward MPC : -4.280041805633626\n",
      "Sub_Episode :  49  | avg. reward : -4.453336905538778  | avg. reward MPC : -4.2800736157169945\n",
      "Sub_Episode :  50  | avg. reward : -4.453882751096266  | avg. reward MPC : -4.2800717216745525\n",
      "Sub_Episode :  51  | avg. reward : -4.454641624418381  | avg. reward MPC : -4.280257210670554\n",
      "Sub_Episode :  52  | avg. reward : -4.455524591492602  | avg. reward MPC : -4.2805834057981595\n",
      "Sub_Episode :  53  | avg. reward : -4.456150528094983  | avg. reward MPC : -4.28064847603431\n",
      "Sub_Episode :  54  | avg. reward : -4.456774807177449  | avg. reward MPC : -4.280688962947323\n",
      "Sub_Episode :  55  | avg. reward : -4.457084893003426  | avg. reward MPC : -4.280457856615438\n",
      "Sub_Episode :  56  | avg. reward : -4.457969373376964  | avg. reward MPC : -4.280723122176918\n",
      "Sub_Episode :  57  | avg. reward : -4.458489017959543  | avg. reward MPC : -4.280640771518887\n",
      "Sub_Episode :  58  | avg. reward : -4.459212305875961  | avg. reward MPC : -4.28071531462257\n",
      "Sub_Episode :  59  | avg. reward : -4.460011175752102  | avg. reward MPC : -4.280746389317995\n",
      "Sub_Episode :  60  | avg. reward : -4.460847380344455  | avg. reward MPC : -4.280822351628539\n",
      "Sub_Episode :  61  | avg. reward : -4.461152019755766  | avg. reward MPC : -4.280510424137013\n",
      "Sub_Episode :  62  | avg. reward : -4.460901924893354  | avg. reward MPC : -4.279830926846917\n",
      "Sub_Episode :  63  | avg. reward : -4.461753366835631  | avg. reward MPC : -4.28016469133547\n",
      "Sub_Episode :  64  | avg. reward : -4.4620149723522635  | avg. reward MPC : -4.279990857845422\n",
      "Sub_Episode :  65  | avg. reward : -4.462413231890334  | avg. reward MPC : -4.2799138639632694\n",
      "Sub_Episode :  66  | avg. reward : -4.462358674273569  | avg. reward MPC : -4.279386814436611\n",
      "Sub_Episode :  67  | avg. reward : -4.462688561000388  | avg. reward MPC : -4.279316238007714\n",
      "Sub_Episode :  68  | avg. reward : -4.463640783941257  | avg. reward MPC : -4.279897731914892\n",
      "Sub_Episode :  69  | avg. reward : -4.463759384130882  | avg. reward MPC : -4.279616546777281\n",
      "Sub_Episode :  70  | avg. reward : -4.4640352166157165  | avg. reward MPC : -4.279508357135322\n",
      "Sub_Episode :  71  | avg. reward : -4.4645460977900235  | avg. reward MPC : -4.2796081521319715\n",
      "Sub_Episode :  72  | avg. reward : -4.464874390741854  | avg. reward MPC : -4.279546001182232\n",
      "Sub_Episode :  73  | avg. reward : -4.464868979535069  | avg. reward MPC : -4.279164909010873\n",
      "Sub_Episode :  74  | avg. reward : -4.46503127229992  | avg. reward MPC : -4.27896181045702\n",
      "Sub_Episode :  75  | avg. reward : -4.465548895071588  | avg. reward MPC : -4.279085251742403\n",
      "Sub_Episode :  76  | avg. reward : -4.465928911278169  | avg. reward MPC : -4.279095457665002\n",
      "Sub_Episode :  77  | avg. reward : -4.466514800288069  | avg. reward MPC : -4.279315253148627\n",
      "Sub_Episode :  78  | avg. reward : -4.46685297108671  | avg. reward MPC : -4.2792977418050375\n",
      "Sub_Episode :  79  | avg. reward : -4.467280054290352  | avg. reward MPC : -4.2793739618700615\n",
      "Sub_Episode :  80  | avg. reward : -4.46775095041827  | avg. reward MPC : -4.279494705500336\n",
      "Sub_Episode :  81  | avg. reward : -4.468093444208813  | avg. reward MPC : -4.279489737714167\n",
      "Sub_Episode :  82  | avg. reward : -4.468278496528219  | avg. reward MPC : -4.2793554940165786\n",
      "Sub_Episode :  83  | avg. reward : -4.469290090068406  | avg. reward MPC : -4.28000252828355\n",
      "Sub_Episode :  84  | avg. reward : -4.469538282457588  | avg. reward MPC : -4.279932860410328\n",
      "Sub_Episode :  85  | avg. reward : -4.4697660480738435  | avg. reward MPC : -4.279850525247343\n",
      "Sub_Episode :  86  | avg. reward : -4.469877213667131  | avg. reward MPC : -4.279655007467168\n",
      "Sub_Episode :  87  | avg. reward : -4.46967143253682  | avg. reward MPC : -4.279153074474802\n",
      "Sub_Episode :  88  | avg. reward : -4.470748994205423  | avg. reward MPC : -4.279905878631163\n",
      "Sub_Episode :  89  | avg. reward : -4.470929038994723  | avg. reward MPC : -4.279794615864399\n",
      "Sub_Episode :  90  | avg. reward : -4.470999021478803  | avg. reward MPC : -4.2795704531948005\n",
      "Sub_Episode :  91  | avg. reward : -4.471600283992742  | avg. reward MPC : -4.279879719088657\n",
      "Sub_Episode :  92  | avg. reward : -4.471536622588799  | avg. reward MPC : -4.279549257381219\n",
      "Sub_Episode :  93  | avg. reward : -4.472320742367544  | avg. reward MPC : -4.280028616186915\n",
      "Sub_Episode :  94  | avg. reward : -4.472470448741094  | avg. reward MPC : -4.279921824964943\n",
      "Sub_Episode :  95  | avg. reward : -4.472508989332022  | avg. reward MPC : -4.279706336349572\n",
      "Sub_Episode :  96  | avg. reward : -4.47298564904801  | avg. reward MPC : -4.279910261322139\n",
      "Sub_Episode :  97  | avg. reward : -4.473183832313384  | avg. reward MPC : -4.279858571134779\n",
      "Sub_Episode :  98  | avg. reward : -4.47354160898072  | avg. reward MPC : -4.279952910760634\n",
      "Sub_Episode :  99  | avg. reward : -4.473594029540897  | avg. reward MPC : -4.27977463512902\n",
      "Sub_Episode :  100  | avg. reward : -4.473837551337239  | avg. reward MPC : -4.27977827314871\n",
      "Sub_Episode :  101  | avg. reward : -4.474484412596703  | avg. reward MPC : -4.28043938770097\n",
      "Sub_Episode :  102  | avg. reward : -4.471616528020619  | avg. reward MPC : -4.2777654614292935\n",
      "Sub_Episode :  103  | avg. reward : -4.470135173291869  | avg. reward MPC : -4.276394114999644\n",
      "Sub_Episode :  104  | avg. reward : -4.467986162369493  | avg. reward MPC : -4.27438953371361\n",
      "Sub_Episode :  105  | avg. reward : -4.466020659224265  | avg. reward MPC : -4.272558555964724\n",
      "Sub_Episode :  106  | avg. reward : -4.464411050031034  | avg. reward MPC : -4.2710697138653035\n",
      "Sub_Episode :  107  | avg. reward : -4.462598750159026  | avg. reward MPC : -4.269389232304284\n",
      "Sub_Episode :  108  | avg. reward : -4.4608130968409885  | avg. reward MPC : -4.267726769247876\n",
      "Sub_Episode :  109  | avg. reward : -4.458927696240521  | avg. reward MPC : -4.265976070321911\n",
      "Sub_Episode :  110  | avg. reward : -4.457090973259383  | avg. reward MPC : -4.264277433036381\n",
      "Sub_Episode :  111  | avg. reward : -4.455378837011764  | avg. reward MPC : -4.262690324963652\n",
      "Sub_Episode :  112  | avg. reward : -4.453551753472191  | avg. reward MPC : -4.261000035021055\n",
      "Sub_Episode :  113  | avg. reward : -4.451961436600306  | avg. reward MPC : -4.2595234948860154\n",
      "Sub_Episode :  114  | avg. reward : -4.450187213463109  | avg. reward MPC : -4.257879348552804\n",
      "Sub_Episode :  115  | avg. reward : -4.447918038447602  | avg. reward MPC : -4.255759695061429\n",
      "Sub_Episode :  116  | avg. reward : -4.44663758835945  | avg. reward MPC : -4.254590921669894\n",
      "Sub_Episode :  117  | avg. reward : -4.444681916331904  | avg. reward MPC : -4.25277808965625\n",
      "Sub_Episode :  118  | avg. reward : -4.443029355239297  | avg. reward MPC : -4.251246744835179\n",
      "Sub_Episode :  119  | avg. reward : -4.441447424853004  | avg. reward MPC : -4.249786570855086\n",
      "Sub_Episode :  120  | avg. reward : -4.439390472151491  | avg. reward MPC : -4.247872407799392\n",
      "Sub_Episode :  121  | avg. reward : -4.437974344965986  | avg. reward MPC : -4.246570648713084\n",
      "Sub_Episode :  122  | avg. reward : -4.435929791688185  | avg. reward MPC : -4.244667128443316\n",
      "Sub_Episode :  123  | avg. reward : -4.43423123078831  | avg. reward MPC : -4.243100819452034\n",
      "Sub_Episode :  124  | avg. reward : -4.432606406492866  | avg. reward MPC : -4.241601568413716\n",
      "Sub_Episode :  125  | avg. reward : -4.430983824307913  | avg. reward MPC : -4.24009659850205\n",
      "Sub_Episode :  126  | avg. reward : -4.429057856478407  | avg. reward MPC : -4.238313862493987\n",
      "Sub_Episode :  127  | avg. reward : -4.427559345034728  | avg. reward MPC : -4.236930963562775\n",
      "Sub_Episode :  128  | avg. reward : -4.4254522658222  | avg. reward MPC : -4.2349714851188764\n",
      "Sub_Episode :  129  | avg. reward : -4.424177773951136  | avg. reward MPC : -4.233804055467591\n",
      "Sub_Episode :  130  | avg. reward : -4.4220409221444665  | avg. reward MPC : -4.231819494252593\n",
      "Sub_Episode :  131  | avg. reward : -4.420466041322829  | avg. reward MPC : -4.230364843030507\n",
      "Sub_Episode :  132  | avg. reward : -4.418376603304972  | avg. reward MPC : -4.22841452020119\n",
      "Sub_Episode :  133  | avg. reward : -4.417204311324775  | avg. reward MPC : -4.227353785181898\n",
      "Sub_Episode :  134  | avg. reward : -4.415002726015478  | avg. reward MPC : -4.225302445087514\n",
      "Sub_Episode :  135  | avg. reward : -4.4131388666129014  | avg. reward MPC : -4.2235716283381555\n",
      "Sub_Episode :  136  | avg. reward : -4.412226422755902  | avg. reward MPC : -4.222759917858715\n",
      "Sub_Episode :  137  | avg. reward : -4.410469734912395  | avg. reward MPC : -4.221140676349474\n",
      "Sub_Episode :  138  | avg. reward : -4.408753305061325  | avg. reward MPC : -4.219550078301564\n",
      "Sub_Episode :  139  | avg. reward : -4.406749216765508  | avg. reward MPC : -4.217699396349772\n",
      "Sub_Episode :  140  | avg. reward : -4.404768635897244  | avg. reward MPC : -4.215848818066611\n",
      "Sub_Episode :  141  | avg. reward : -4.403542178625571  | avg. reward MPC : -4.214735939020097\n",
      "Sub_Episode :  142  | avg. reward : -4.4018355944742815  | avg. reward MPC : -4.213162516062992\n",
      "Sub_Episode :  143  | avg. reward : -4.399770565109946  | avg. reward MPC : -4.211238030785797\n",
      "Sub_Episode :  144  | avg. reward : -4.398570368411095  | avg. reward MPC : -4.210151631655711\n",
      "Sub_Episode :  145  | avg. reward : -4.397048841912537  | avg. reward MPC : -4.208754537611426\n",
      "Sub_Episode :  146  | avg. reward : -4.3951175775725195  | avg. reward MPC : -4.206971552742714\n",
      "Sub_Episode :  147  | avg. reward : -4.393683264819725  | avg. reward MPC : -4.205655568946379\n",
      "Sub_Episode :  148  | avg. reward : -4.391733627618265  | avg. reward MPC : -4.203854377042702\n",
      "Sub_Episode :  149  | avg. reward : -4.389983588637676  | avg. reward MPC : -4.202240406026922\n",
      "Sub_Episode :  150  | avg. reward : -4.388554857007431  | avg. reward MPC : -4.200930137315015\n",
      "Sub_Episode :  151  | avg. reward : -4.386822119150032  | avg. reward MPC : -4.199333717369115\n",
      "Sub_Episode :  152  | avg. reward : -4.385281893664106  | avg. reward MPC : -4.197918080555887\n",
      "Sub_Episode :  153  | avg. reward : -4.3837449771566375  | avg. reward MPC : -4.196504898987738\n",
      "Sub_Episode :  154  | avg. reward : -4.381942574696499  | avg. reward MPC : -4.1948410945708945\n",
      "Sub_Episode :  155  | avg. reward : -4.3803128628784895  | avg. reward MPC : -4.193347386275104\n",
      "Sub_Episode :  156  | avg. reward : -4.379016202537172  | avg. reward MPC : -4.192156578238481\n",
      "Sub_Episode :  157  | avg. reward : -4.377007822324777  | avg. reward MPC : -4.190292720756783\n",
      "Sub_Episode :  158  | avg. reward : -4.375493444312413  | avg. reward MPC : -4.188907564348607\n",
      "Sub_Episode :  159  | avg. reward : -4.373292029048885  | avg. reward MPC : -4.1868629621140565\n",
      "Sub_Episode :  160  | avg. reward : -4.372279306893671  | avg. reward MPC : -4.185961384219862\n",
      "Sub_Episode :  161  | avg. reward : -4.370481572986593  | avg. reward MPC : -4.184315477405516\n",
      "Sub_Episode :  162  | avg. reward : -4.368986317221656  | avg. reward MPC : -4.182939207987727\n",
      "Sub_Episode :  163  | avg. reward : -4.367460406574623  | avg. reward MPC : -4.181538070075605\n",
      "Sub_Episode :  164  | avg. reward : -4.3657480441607435  | avg. reward MPC : -4.179969710432291\n",
      "Sub_Episode :  165  | avg. reward : -4.364236357510643  | avg. reward MPC : -4.178579226618334\n",
      "Sub_Episode :  166  | avg. reward : -4.362768359281572  | avg. reward MPC : -4.1772354624215895\n",
      "Sub_Episode :  167  | avg. reward : -4.360877418585431  | avg. reward MPC : -4.175487166187499\n",
      "Sub_Episode :  168  | avg. reward : -4.359183631212716  | avg. reward MPC : -4.173929723352328\n",
      "Sub_Episode :  169  | avg. reward : -4.357795769726347  | avg. reward MPC : -4.17267199996532\n",
      "Sub_Episode :  170  | avg. reward : -4.356098359858509  | avg. reward MPC : -4.171100831700717\n",
      "Sub_Episode :  171  | avg. reward : -4.354898681891552  | avg. reward MPC : -4.170018002762792\n",
      "Sub_Episode :  172  | avg. reward : -4.353038300159332  | avg. reward MPC : -4.1683017180555995\n",
      "Sub_Episode :  173  | avg. reward : -4.351552843472169  | avg. reward MPC : -4.1669548071083575\n",
      "Sub_Episode :  174  | avg. reward : -4.350123239725353  | avg. reward MPC : -4.165643342972681\n",
      "Sub_Episode :  175  | avg. reward : -4.348520134541391  | avg. reward MPC : -4.164170240505079\n",
      "Sub_Episode :  176  | avg. reward : -4.347213308992544  | avg. reward MPC : -4.162983443909772\n",
      "Sub_Episode :  177  | avg. reward : -4.345375940308553  | avg. reward MPC : -4.161295388407836\n",
      "Sub_Episode :  178  | avg. reward : -4.343727345189609  | avg. reward MPC : -4.159781056465303\n",
      "Sub_Episode :  179  | avg. reward : -4.342277851614179  | avg. reward MPC : -4.158463364765801\n",
      "Sub_Episode :  180  | avg. reward : -4.340585975024959  | avg. reward MPC : -4.156914743894574\n",
      "Sub_Episode :  181  | avg. reward : -4.338933344805855  | avg. reward MPC : -4.155396790933687\n",
      "Sub_Episode :  182  | avg. reward : -4.337876959718252  | avg. reward MPC : -4.154449238879156\n",
      "Sub_Episode :  183  | avg. reward : -4.3364102401651605  | avg. reward MPC : -4.15310667294778\n",
      "Sub_Episode :  184  | avg. reward : -4.334609968728969  | avg. reward MPC : -4.151459112015739\n",
      "Sub_Episode :  185  | avg. reward : -4.333242996619398  | avg. reward MPC : -4.15020827339061\n",
      "Sub_Episode :  186  | avg. reward : -4.331791668774764  | avg. reward MPC : -4.148881179022895\n",
      "Sub_Episode :  187  | avg. reward : -4.330443377999382  | avg. reward MPC : -4.147661621536145\n",
      "Sub_Episode :  188  | avg. reward : -4.328833052037136  | avg. reward MPC : -4.146184501762873\n",
      "Sub_Episode :  189  | avg. reward : -4.3270724052096305  | avg. reward MPC : -4.144565046625682\n",
      "Sub_Episode :  190  | avg. reward : -4.3252274626304335  | avg. reward MPC : -4.14286546856213\n",
      "Sub_Episode :  191  | avg. reward : -4.323788555753517  | avg. reward MPC : -4.141550056478843\n",
      "Sub_Episode :  192  | avg. reward : -4.3222509245234555  | avg. reward MPC : -4.14015189000977\n",
      "Sub_Episode :  193  | avg. reward : -4.320792763969379  | avg. reward MPC : -4.138822517151718\n",
      "Sub_Episode :  194  | avg. reward : -4.319366281554587  | avg. reward MPC : -4.137517306615994\n",
      "Sub_Episode :  195  | avg. reward : -4.317921098840797  | avg. reward MPC : -4.136200617218674\n",
      "Sub_Episode :  196  | avg. reward : -4.316349472789973  | avg. reward MPC : -4.134763836175481\n",
      "Sub_Episode :  197  | avg. reward : -4.314898556432656  | avg. reward MPC : -4.133445940949166\n",
      "Sub_Episode :  198  | avg. reward : -4.313550763108249  | avg. reward MPC : -4.132224914030191\n",
      "Sub_Episode :  199  | avg. reward : -4.312091101408391  | avg. reward MPC : -4.130899728726861\n",
      "Sub_Episode :  200  | avg. reward : -4.3106212907493955  | avg. reward MPC : -4.129554252201648\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:40:50.925536Z",
     "start_time": "2026-01-08T04:40:50.914055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_mpc_results_cstr(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, xhatdhat=None, yhat=None,\n",
    "    directory=None, prefix_name=\"mpc_result\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Paper-ready MPC figures for CSTR; saves to directory/prefix_name/<timestamp>/.\n",
    "    Same style and layout as distillation-column plot_mpc_results.\n",
    "    Returns: out_dir (str)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name_base):\n",
    "        plt.tight_layout()\n",
    "        png = os.path.join(out_dir, f\"{name_base}.png\")\n",
    "        plt.savefig(png, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # --- scaling logic (unchanged) ---\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # --- style (exactly like distillation) ---\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True,\n",
    "    })\n",
    "\n",
    "    C_MPC = \"tab:blue\"\n",
    "    C_SP = \"tab:red\"\n",
    "    C_U1 = \"tab:green\"\n",
    "    C_U2 = \"tab:orange\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1 (full horizon): outputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot, y_mpc[:, 0], \"-\", lw=2.2, color=C_MPC, label=r\"MPC\", zorder=2)\n",
    "    ax.step(time_plot[:-1], y_sp[0, :], where=\"post\", linestyle=\"--\", lw=2.2,\n",
    "            color=C_SP, alpha=0.95, label=r\"Setpoint\", zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot, y_mpc[:, 1], \"-\", lw=2.2, color=C_MPC, label=r\"MPC\", zorder=2)\n",
    "    ax.step(time_plot[:-1], y_sp[1, :], where=\"post\", linestyle=\"--\", lw=2.2,\n",
    "            color=C_SP, alpha=0.95, label=r\"Setpoint\", zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.82)\n",
    "    _savefig(\"fig_mpc_outputs_full\")\n",
    "\n",
    "    # -------- Plot 1b (last window): outputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2,\n",
    "            color=C_MPC, label=r\"MPC\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=C_SP, alpha=0.95, label=r\"Setpoint\", zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2,\n",
    "            color=C_MPC, label=r\"MPC\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=C_SP, alpha=0.95, label=r\"Setpoint\", zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.82)\n",
    "    _savefig(f\"fig_mpc_outputs_last{time_in_sub_episodes}\")\n",
    "\n",
    "    # -------- Plot 2 (full horizon): inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_U1, label=r\"$Q_c$\", zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_U2, label=r\"$Q_m$\", zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%.1f\"))\n",
    "    # ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.0), borderaxespad=0., facecolor=\"white\")\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.82)\n",
    "    _savefig(\"fig_mpc_inputs_full\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "4baa075bce50ec7f",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:40:52.689494Z",
     "start_time": "2026-01-08T04:40:51.829406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_mpc_results_cstr(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, xhatdhat,\n",
    "    directory=dir_path, prefix_name=\"mpc_result_dist\"\n",
    ")\n",
    "print(\"Saved to:\", out_dir)"
   ],
   "id": "ec1d95369e3a48bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\mpc_result_dist\\20260107_234051\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:06:31.408243Z",
     "start_time": "2026-01-07T22:06:30.456313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Storing the inputs and outputs of the MPC\n",
    "mpc_results = {\n",
    "    \"u_mpc\": u_mpc,\n",
    "    \"y_mpc\": y_mpc,\n",
    "    \"avg_rewards\": avg_rewards,\n",
    "    \"avg_rewards_mpc\": avg_rewards_mpc,\n",
    "    \"xhatdhat\": xhatdhat,\n",
    "    \"delta_y_storage\": delta_y_storage,\n",
    "    \"delat_u_storage\": delta_u_storage\n",
    "}\n",
    "save_path = os.path.join(dir_path, \"mpc_results_dist.pickle\")\n",
    "with open(save_path, 'wb') as file:\n",
    "    pickle.dump(mpc_results, file)"
   ],
   "id": "7aed1f30bd46f2ed",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T01:37:25.967347Z",
     "start_time": "2025-05-15T01:37:25.962347Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d8ddffc740274a4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f579fa0f0ccba97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
