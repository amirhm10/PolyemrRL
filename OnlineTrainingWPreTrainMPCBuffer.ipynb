{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:22.461776Z",
     "start_time": "2026-01-07T04:56:21.396339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from utils.helpers import *"
   ],
   "id": "833e2155a44bd6c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "8c565965fce1dc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:26.001472Z",
     "start_time": "2026-01-07T04:56:25.998696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "d168509011200b21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:26.283250Z",
     "start_time": "2026-01-07T04:56:26.280379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "ef60c6a882f064d1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:26.467554Z",
     "start_time": "2026-01-07T04:56:26.465375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "e0613ee1ad154d3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:26.628941Z",
     "start_time": "2026-01-07T04:56:26.626799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "aea9d86581d16b25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:26.835193Z",
     "start_time": "2026-01-07T04:56:26.831778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "1a7c69161bd5f8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "85287aecd00bcda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:27.262490Z",
     "start_time": "2026-01-07T04:56:27.259958Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "f8fbe1e84f3bb189",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:27.488851Z",
     "start_time": "2026-01-07T04:56:27.483379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "b12cb95ea94b6f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:27.721490Z",
     "start_time": "2026-01-07T04:56:27.718815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "7758bb35c5218c2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:27.987379Z",
     "start_time": "2026-01-07T04:56:27.984765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "c44dfc980e752980",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:28.272911Z",
     "start_time": "2026-01-07T04:56:28.270706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "88095c1c82c5ad36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:28.503545Z",
     "start_time": "2026-01-07T04:56:28.500924Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "59d7edf141de197",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:28.749531Z",
     "start_time": "2026-01-07T04:56:28.743408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "e7463ce513b8ba31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:28.993426Z",
     "start_time": "2026-01-07T04:56:28.987357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "f15067d59c9f2f52",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:29.229368Z",
     "start_time": "2026-01-07T04:56:29.226512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False, False, False, False]\n",
    "warm_start = 10\n",
    "ACTOR_FREEZE = 10 * set_points_len\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "c062e1a79f80912a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:29.776466Z",
     "start_time": "2026-01-07T04:56:29.755756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "85da68bee773cd7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "1daeca8ba3164a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:30.401516Z",
     "start_time": "2026-01-07T04:56:30.388352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TD3Agent.agent import TD3Agent\n",
    "import torch"
   ],
   "id": "49c428a23b3e48b2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:30.860236Z",
     "start_time": "2026-01-07T04:56:30.839175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "n_outputs = C_aug.shape[0]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 40000\n",
    "ACTOR_LR = 5e-5\n",
    "CRITIC_LR = 5e-4\n",
    "SMOOTHING_STD = 0.005\n",
    "NOISE_CLIP = 0.01\n",
    "# EXPLORATION_NOISE_STD = 0.01\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005 # 0.01\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 256\n",
    "STD_START = 0.02\n",
    "STD_END = 0.001\n",
    "STD_DECAY_RATE = 0.99992\n",
    "STD_DECAY_MODE = \"exp\""
   ],
   "id": "e798424baebbc371",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:32.892746Z",
     "start_time": "2026-01-07T04:56:31.857965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "td3_agent = TD3Agent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    actor_hidden=ACTOR_LAYER_SIZES,\n",
    "    critic_hidden=CRITIC_LAYER_SIZES,\n",
    "    gamma=GAMMA,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    max_action=MAX_ACTION,\n",
    "    tau=TAU,\n",
    "    std_start=STD_START,\n",
    "    std_end=STD_END,\n",
    "    std_decay_rate=STD_DECAY_RATE,\n",
    "    std_decay_mode=STD_DECAY_MODE,\n",
    "    buffer_size=BUFFER_CAPACITY,\n",
    "    device=DEVICE,\n",
    "    actor_freeze=ACTOR_FREEZE,\n",
    "    mode=\"mpc\"\n",
    "    )"
   ],
   "id": "1d8ae390b2843fca",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:33.439246Z",
     "start_time": "2026-01-07T04:56:33.197646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "td3_agent.load(agent_path)"
   ],
   "id": "5d2d2b610564c69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MPC Initialization",
   "id": "fac3637c0ee4f291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:35.031640Z",
     "start_time": "2026-01-07T04:56:35.027783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1.\n",
    "R2_penalty = 1.\n",
    "Q_penalty = np.array([[Q1_penalty, 0], [0, Q2_penalty]])\n",
    "R_penalty = np.array([[R1_penalty, 0], [0, R2_penalty]])"
   ],
   "id": "211634f6f62d4c5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:35.634422Z",
     "start_time": "2026-01-07T04:56:35.626438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "7bf97648c044ad79",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying RL Agent on the CSTR",
   "id": "65a4fc461d9d0b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:36.550931Z",
     "start_time": "2026-01-07T04:56:36.548209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reward_fn(delta_y, delta_u, Q1_penalty=1, Q2_penalty=1, R1_penalty=1, R2_penalty=1):\n",
    "\n",
    "    # Reward Calculation\n",
    "    reward = - (Q1_penalty * delta_y[0] ** 2 + Q2_penalty * delta_y[1] ** 2 +\n",
    "                R1_penalty * delta_u[0] ** 2 + R2_penalty * delta_u[1] ** 2)\n",
    "\n",
    "    return reward"
   ],
   "id": "317946f8d424ca6c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:37.301069Z",
     "start_time": "2026-01-07T04:56:37.298296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "829372b3f310f055",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:37.980812Z",
     "start_time": "2026-01-07T04:56:37.974003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rl_train(system, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 test_cycle,\n",
    "                 nominal_qi, nominal_qs, nominal_ha,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"disturb\"):\n",
    "\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "    u_min, u_max = min_max_dict[\"u_min\"], min_max_dict[\"u_max\"]\n",
    "\n",
    "    y_system = np.zeros((nFE + 1, n_outputs))\n",
    "    y_system[0, :] = system.current_output\n",
    "    u_rl = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    # xhatdhat[:, 0] = np.random.uniform(low=min_max_dict[\"x_min\"], high=min_max_dict[\"x_max\"])\n",
    "    rewards = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "\n",
    "    delta_y_storage = []\n",
    "\n",
    "    # ----- helper ------\n",
    "    def map_to_bounds(a, low, high):\n",
    "        return low + ((a + 1.0) / 2.0) * (high - low)\n",
    "\n",
    "    test = False\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # train/test phase\n",
    "        if i in test_train_dict:\n",
    "            test = test_train_dict[i]\n",
    "\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # ---- RL state (scaled) ----\n",
    "        current_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i], y_sp[i, :], scaled_current_input_dev)\n",
    "\n",
    "        # ---- TD3 action ----\n",
    "        if not test:\n",
    "            action = agent.take_action(current_rl_state, explore=(not test))\n",
    "        else:\n",
    "            action = agent.act_eval(current_rl_state)\n",
    "        # Map to bounds\n",
    "        u_scaled = map_to_bounds(action, u_min, u_max)\n",
    "\n",
    "        # scale & step plant\n",
    "        u_rl[i, :] = u_scaled + ss_scaled_inputs\n",
    "        u_plant = reverse_min_max(u_rl[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_rl[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record output\n",
    "        y_system[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_system[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_system[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_rl[i, :] - ss_scaled_inputs)) + np.dot(L, (y_prev_scaled - yhat[:, i])).T\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u)\n",
    "\n",
    "        # Record rewards and delta_y\n",
    "        rewards[i] = reward\n",
    "        delta_y_storage.append(np.abs(delta_y))\n",
    "\n",
    "        # ----- Next state for TD3 -----\n",
    "        next_u_dev = u_rl[i, :] - ss_scaled_inputs\n",
    "        next_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i+1], y_sp[i, :], next_u_dev)\n",
    "\n",
    "        # Episode boundary (treat each setpoint block as an episode end)\n",
    "        # done = 1.0 if (i + 1) % boundary == 0 else 0.0\n",
    "        done = 0.0\n",
    "\n",
    "        # Buffer + train (skip if in test phase)\n",
    "        if not test:\n",
    "            agent.push(current_rl_state,\n",
    "                       action.astype(np.float32),\n",
    "                       float(reward),\n",
    "                       next_rl_state,\n",
    "                       float(done))\n",
    "            if i >= WARM_START:\n",
    "                _ = agent.train_step()  # returns loss or None\n",
    "\n",
    "        # diagnostics at sub-episode boundary\n",
    "        if i in sub_episodes_changes_dict:\n",
    "            avg_rewards.append(np.mean(rewards[max(0, i - time_in_sub_episodes + 1): i + 1]))\n",
    "            print('Sub_Episode:', sub_episodes_changes_dict[i], '| avg. reward:', avg_rewards[-1])\n",
    "            if hasattr(agent, \"_expl_sigma\"):\n",
    "                print('Exploration noise:', agent._expl_sigma)\n",
    "\n",
    "    # unscale to plant units for plotting\n",
    "    u_rl = reverse_min_max(u_rl, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha"
   ],
   "id": "c0b20eb3706f0e6e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:42:17.289130Z",
     "start_time": "2026-01-07T04:23:45.441693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "    = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 TEST_CYCLE,\n",
    "                 nominal_qi, nominal_qs, nominal_hA,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"nominal\")"
   ],
   "id": "eb10ecbf5180672d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 1 | avg. reward: -6.3586161273256785\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.047074273357757\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.7175394832363375\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.095300134316851\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.604626935147463\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.980969085399714\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.026844001998942\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.451234469375522\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.7949109000539805\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.824769292710216\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.959411196299269\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.269337728948454\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.550787702093533\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.802720397928796\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.810614070476413\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -16.359001359518928\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.4999625231654716\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.081803621799279\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8865641915880795\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.9312753965329458\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.9757693570546223\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8667933321778415\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.860423946002463\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.9030090172874579\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7820776815411148\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7911087598938877\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.7645387141209475\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7544430983531947\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.8419433133747529\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.8329517077163178\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.8108643680714099\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.8414601387202297\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.8355606449385854\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.8585303417027192\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.8423847744663427\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.843985132260939\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.9853907368614132\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.0277483833492076\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.8765000463795124\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.7137343748326396\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.7459617384277795\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7309150741476563\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.7285390167185656\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.7181317249659034\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7507044641967948\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.7136434474168718\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.7098282611164024\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.6761317363038463\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.6520820331647446\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6482273584775463\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.6182282887827425\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6274110074240032\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6232531845977172\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5981679771976929\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6032125151350225\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5966875691850438\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5865582122923548\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5729228705923626\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5599953615696065\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5603488850728463\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5520846272872639\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5358224613191359\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5196197051872858\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5321453501561955\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.543331635968855\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5751845541845484\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.6022329733228989\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5868282290913418\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.6078263528573454\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6437848299406619\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6879489935333359\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.7631442447373921\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.7218699864335673\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.7137678597914123\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.6902365564207016\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.6664474970975429\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6583496599476203\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.6734845567674987\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.676589151790351\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6982556441968178\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.7414187843795026\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.7741490585598424\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.8098256337209078\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.7663776901250475\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.7755153093393754\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.8264726820356688\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.9677670824689923\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.1685672340295603\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.5692642134391372\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.8428649925383518\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.005321759740323\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.11922182252435\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.5995906122441488\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.5760300125553834\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.9551926028252928\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.2552040766981487\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.5477259511344406\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.8200613288551751\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.7658351927182891\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.8001844245117025\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.7987112269876658\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.7919240526979557\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.7754777940713402\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.7624701626866461\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.7589332602547727\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.7565960386078597\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.7170040386296364\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6562280451228298\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6543262287469509\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6384486389135218\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6573862900813929\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6469771949812249\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6584201429471891\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6412328796665635\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.6565801034711403\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6817860130883077\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.7047113612023785\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.7417887419499966\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.7532613862117122\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.7892624928601494\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.8272789501489746\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.8689908976736163\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.9343174453021477\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.8660486962146748\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.8526201573563444\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.8703913803718586\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.8424460670092612\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.9527993341572111\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.0369467933968206\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.9403134715574515\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.8493717551318427\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.9287780602058683\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.7158373475763742\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.7136377255612331\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.1580232435902142\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.7014068278996105\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.7237930035810031\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.7502699226988188\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6126948890416268\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5780495893668169\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6534238698307527\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.6221342012760367\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5561893753226269\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5904736956963749\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.588849804625827\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5904045595004055\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6376413118818542\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5935526993833108\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.6025275123961749\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6022300790034788\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.6035249819249962\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.6085155101059891\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.626722675825472\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6326619519295111\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.6210501256534121\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6624558815660463\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.7310590492884552\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.7734983731126213\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.6837929178605535\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.759602645238555\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.7487583537245227\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.8622127244690063\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.8148165096813395\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.8445693752316936\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.9376045583328697\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.9721367857176116\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.7451468680616151\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.7432519009331725\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.8269779955265244\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.8108165357769255\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.8562542249845794\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.7985011852321778\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6244517975314091\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5977448218149071\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5662630689440665\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5818549768680764\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5884967581891815\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5982549305424842\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5869270588905301\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5700618111136344\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5735941008137144\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5806077034082721\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5958136204348732\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.6296986709168171\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.6455833004217179\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5802349414704655\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5749001469024575\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5689790819480226\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5739032016844193\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5688002831152797\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5414880992435154\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6094900108494453\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.6025570832219865\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5603829084299092\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5421310045311561\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5599387862203672\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6643534174181478\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5928427300470775\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.6491643777001055\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6133208808637113\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:56:40.378078Z",
     "start_time": "2026-01-07T04:56:40.359760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=None, prefix_name=\"agent_result\",\n",
    "    agent=None,\n",
    "    delta_y_storage=None,\n",
    "    rewards=None,\n",
    "    dist=None,\n",
    "    start_plot_idx=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation-style plotting (same colors/fonts/no legends).\n",
    "    Saves all figures + input_data.pkl to directory/prefix_name/<timestamp>.\n",
    "    Handles:\n",
    "      dist=None\n",
    "      dist=1D array\n",
    "      dist=dict with keys {\"qi\",\"qs\",\"ha\"}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name):\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, name), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    y_sp_original = np.array(y_sp, copy=True)\n",
    "\n",
    "    actor_losses = getattr(agent, \"actor_losses\", None) if agent is not None else None\n",
    "    critic_losses = getattr(agent, \"critic_losses\", None) if agent is not None else None\n",
    "    dy_arr = np.array(delta_y_storage) if delta_y_storage is not None else None\n",
    "    rewards_arr = np.array(rewards) if rewards is not None else None\n",
    "\n",
    "    input_data = {\n",
    "        \"y_sp\": y_sp_original,\n",
    "        \"steady_states\": steady_states,\n",
    "        \"nFE\": nFE,\n",
    "        \"delta_t\": delta_t,\n",
    "        \"time_in_sub_episodes\": time_in_sub_episodes,\n",
    "        \"y_mpc\": y_mpc,\n",
    "        \"u_mpc\": u_mpc,\n",
    "        \"avg_rewards\": avg_rewards,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"warm_start_plot\": warm_start_plot,\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "        \"delta_y_storage\": dy_arr,\n",
    "        \"rewards\": rewards_arr,\n",
    "        \"dist\": dist,\n",
    "        \"start_plot_idx\": start_plot_idx\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"input_data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(input_data, f)\n",
    "\n",
    "    # Canceling the deviation form (same logic)\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # Distillation-style rcParams (no bold globals; bold comes from \\mathbf in labels)\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    # Colors exactly like distillation code\n",
    "    C_QC = \"tab:green\"\n",
    "    C_QM = \"tab:orange\"\n",
    "    C_RW = \"tab:purple\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    warm_start_plot = np.atleast_1d(warm_start_plot) * delta_t\n",
    "    ws_end = float(warm_start_plot.max()) if warm_start_plot.size > 0 else 0.0\n",
    "\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1: outputs (full) --------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 0], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[0, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{\\eta}$ (L/g)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 1], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[1, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{T}$ (K)\", fontsize=18)\n",
    "    ax.set_xlabel(r\"$\\mathbf{Time}$ (hour)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95, bottom=0.12)\n",
    "    _savefig(\"fig_rl_outputs_full.png\")\n",
    "\n",
    "    # -------- last window --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{time_in_sub_episodes}.png\")\n",
    "\n",
    "    # -------- last 4x window --------\n",
    "    W4 = 4 * time_in_sub_episodes\n",
    "    time_plot_4w = np.linspace(0, W4 * delta_t, W4 + 1)\n",
    "\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[0, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[1, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{W4}.png\")\n",
    "\n",
    "    # -------- Plot 2: inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_QC, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_QM, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(\"fig_rl_inputs_full.png\")\n",
    "\n",
    "    # -------- Plot 3: reward per episode --------\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    xep = np.arange(1, len(avg_rewards) + 1)\n",
    "    plt.plot(xep, avg_rewards, \"o-\", lw=2.2, color=C_RW, zorder=2)\n",
    "    plt.ylabel(\"Avg. Reward\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    _savefig(\"fig_rl_rewards.png\")\n",
    "\n",
    "    # -------- optional losses --------\n",
    "    if actor_losses is not None and len(actor_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(actor_losses, lw=1.8, color=\"tab:blue\")\n",
    "        plt.ylabel(\"Actor Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_actor.png\")\n",
    "\n",
    "    if critic_losses is not None and len(critic_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(critic_losses, lw=1.8, color=\"tab:orange\")\n",
    "        plt.ylabel(\"Critic Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_critic.png\")\n",
    "\n",
    "    # -------- optional delta_y windows (no legend) --------\n",
    "    if dy_arr is not None and dy_arr.ndim == 2 and dy_arr.shape[1] >= 2:\n",
    "        n = dy_arr.shape[0]\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w = dy_arr[i0:n]\n",
    "        if len(w) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w[:, 0], c=\"r\")\n",
    "            plt.plot(w[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_last300.png\")\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w2 = dy_arr[j0:j1]\n",
    "        if len(w2) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w2[:, 0], c=\"r\")\n",
    "            plt.plot(w2[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_700_400.png\")\n",
    "\n",
    "    # -------- optional per-step rewards (no legend) --------\n",
    "    if rewards_arr is not None and rewards_arr.ndim == 1 and rewards_arr.size > 0:\n",
    "        n = rewards_arr.size\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w = rewards_arr[j0:j1]\n",
    "        if w.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w.size), w, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_700_400.png\")\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w2 = rewards_arr[i0:n]\n",
    "        if w2.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w2.size), w2, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_last300.png\")\n",
    "\n",
    "        plt.figure(figsize=(7.6, 4.2))\n",
    "        plt.scatter(range(rewards_arr.size), rewards_arr, s=10)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"rewards_all.png\")\n",
    "\n",
    "    # -------- disturbance (no legend) --------\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, dict) and all(k in dist for k in [\"qi\", \"qs\", \"ha\"]):\n",
    "            qi_arr = np.asarray(dist[\"qi\"]).squeeze()\n",
    "            qs_arr = np.asarray(dist[\"qs\"]).squeeze()\n",
    "            ha_arr = np.asarray(dist[\"ha\"]).squeeze()\n",
    "            n_al = min(nFE, qi_arr.shape[0], qs_arr.shape[0], ha_arr.shape[0])\n",
    "\n",
    "            def _dist_fig(t, q1, q2, hA, suffix):\n",
    "                plt.figure(figsize=(7.6, 6.2))\n",
    "\n",
    "                ax = plt.subplot(3, 1, 1)\n",
    "                ax.plot(t, q1, \"-\", lw=2, color=\"tab:blue\")\n",
    "                ax.set_ylabel(r\"$Q_i$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 2)\n",
    "                ax.plot(t, q2, \"-\", lw=2, color=\"tab:orange\")\n",
    "                ax.set_ylabel(r\"$Q_s$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 3)\n",
    "                ax.plot(t, hA, \"-\", lw=2, color=\"tab:green\")\n",
    "                ax.set_xlabel(\"Time (h)\")\n",
    "                ax.set_ylabel(r\"$h_a$ (J/Kh)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                plt.gcf().subplots_adjust(right=0.95, hspace=0.25)\n",
    "                _savefig(f\"fig_disturbances_{suffix}.png\")\n",
    "\n",
    "            _dist_fig(time_plot[:n_al], qi_arr[:n_al], qs_arr[:n_al], ha_arr[:n_al], suffix=\"full\")\n",
    "\n",
    "            if time_in_sub_episodes > 0:\n",
    "                W = min(time_in_sub_episodes, n_al)\n",
    "                t_lastW = np.linspace(0, W * delta_t, W, endpoint=False)\n",
    "                _dist_fig(\n",
    "                    t_lastW,\n",
    "                    qi_arr[n_al - W:n_al],\n",
    "                    qs_arr[n_al - W:n_al],\n",
    "                    ha_arr[n_al - W:n_al],\n",
    "                    suffix=f\"last{W}\"\n",
    "                )\n",
    "        else:\n",
    "            dist_arr = np.asarray(dist).squeeze()\n",
    "            n_al = min(nFE, dist_arr.shape[0])\n",
    "            plt.figure(figsize=(7.2, 4.2))\n",
    "            plt.plot(time_plot[start_plot_idx:n_al], dist_arr[start_plot_idx:n_al], lw=1.8, color=\"tab:blue\")\n",
    "            plt.ylabel(\"Disturbance\")\n",
    "            plt.xlabel(\"Time (h)\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"disturbance.png\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "f7a5be6d8440239a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T04:46:26.782866Z",
     "start_time": "2026-01-07T04:46:23.723530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=dir_path, prefix_name=\"polymer_nominal_mpc\",\n",
    "    agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "    dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    ")"
   ],
   "id": "9d08421eb40e999f",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:18:59.561179Z",
     "start_time": "2026-01-07T04:57:05.059488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    set_points_number = int(C_aug.shape[0])\n",
    "    inputs_number = int(B_aug.shape[1])\n",
    "    STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "    ACTION_DIM = int(B_aug.shape[1])\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    BUFFER_CAPACITY = 40000\n",
    "    ACTOR_LR = 5e-5\n",
    "    CRITIC_LR = 5e-4\n",
    "    SMOOTHING_STD = 0.005\n",
    "    NOISE_CLIP = 0.01\n",
    "    # EXPLORATION_NOISE_STD = 0.01\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.005  # 0.01\n",
    "    MAX_ACTION = 1\n",
    "    POLICY_DELAY = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 256\n",
    "    STD_START = 0.02\n",
    "    STD_END = 0.001\n",
    "    STD_DECAY_RATE = 0.99992\n",
    "    STD_DECAY_MODE = \"exp\"\n",
    "    td3_agent = TD3Agent(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        actor_hidden=ACTOR_LAYER_SIZES,\n",
    "        critic_hidden=CRITIC_LAYER_SIZES,\n",
    "        gamma=GAMMA,\n",
    "        actor_lr=ACTOR_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        policy_delay=POLICY_DELAY,\n",
    "        target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "        noise_clip=NOISE_CLIP,\n",
    "        max_action=MAX_ACTION,\n",
    "        tau=TAU,\n",
    "        std_start=STD_START,\n",
    "        std_end=STD_END,\n",
    "        std_decay_rate=STD_DECAY_RATE,\n",
    "        std_decay_mode=STD_DECAY_MODE,\n",
    "        buffer_size=BUFFER_CAPACITY,\n",
    "        device=DEVICE,\n",
    "        actor_freeze=ACTOR_FREEZE,\n",
    "    )\n",
    "    agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "    td3_agent.load(agent_path)\n",
    "    cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "    y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "        = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                     steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                     L, data_min, data_max, warm_start,\n",
    "                     TEST_CYCLE,\n",
    "                     nominal_qi, nominal_qs, nominal_hA,\n",
    "                     qi_change, qs_change, ha_change,\n",
    "                     reward_fn, mode=\"nominal\")\n",
    "    out_dir = plot_rl_results_disturbance(\n",
    "        y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "        y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "        directory=dir_path, prefix_name=\"polymer_nominal_mpc_with_buffer\",\n",
    "        agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "        dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    "    )"
   ],
   "id": "fdb4470b25c16864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.5682764670095795\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.794475696060462\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.654143370094512\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.3426268540962045\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.680110544778199\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.927574408948854\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.460259554919896\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.7252753904451446\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.136622635284635\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.689679078147439\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.263029283722269\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.291138733359619\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.157739763307738\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.964960194317987\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.838840345540314\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -6.688884317085654\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.0166118263873343\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.2564206101838866\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8716205362234923\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8020531567798779\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7510684660265059\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6786393412151372\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6861348011984221\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6232213527312941\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6211676411354177\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.62879327359154\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.5717194137390864\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5691684172642144\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6185613064252055\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6332580581899054\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5839118801015829\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.591097133288855\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5647623905779685\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5606903380175638\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5747874202442325\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5624719653050743\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.562099814082534\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5529379051114622\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5745652070263298\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5675598602123787\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5688600584257717\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5757449111664102\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5658266617719181\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5705789180550596\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.6098419636092952\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5785539560260967\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5417879719987888\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5412969805057701\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5601191860160708\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5458669606733373\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5535196039365116\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.560504480209239\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5643749442809098\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5440916982715543\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5613439638739779\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5727347954230989\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5449691902246873\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5419591300334764\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5405212740619919\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5398522297491826\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.6707941614390104\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.6137803846741032\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5464713732701983\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5809413610527171\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5973465182002392\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5972913513606887\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5436175344250914\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5333032730459017\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5316655754297238\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5533929820245655\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.532354849083906\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5291037491947466\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5292603043786471\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5373966518863013\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5456661728972975\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.52988373550854\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.547545018223655\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5212678320479222\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.53375492417691\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5482637283844425\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.583067593453357\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5572709912123324\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5530929606864512\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5744380837069921\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6354619481783008\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6396870701436896\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5810810789500062\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.565595330693776\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5430532099872758\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5679013641501213\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5551208063642867\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5554158788166506\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5494291324173173\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5415577449027063\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.523787787504967\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5516345534937838\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5345517077696011\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.554579835563204\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5454502263397921\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.569126573454341\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5593316702002743\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5794225921025407\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5444365475368544\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5546002792640431\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5654942964012538\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5843158952564725\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5694247251906855\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5458377006903621\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.541025191264216\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5314053564000277\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5478569158700315\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.862044022023596\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.543684321010788\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5669447307053974\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5726417757978404\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5701633781589002\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5738754195940178\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5630893556705078\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5608492932522448\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5807071141462654\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.6082241083366757\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.6138928485729614\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.6066121690262885\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.6191419538742187\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.7543132935831095\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.6967328326461562\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.6261351867402171\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.6129538915757333\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.6252279754870744\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.6567163717290838\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5890774456395831\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5473648487284075\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5614692779050745\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5744133406062174\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5452864765437645\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5741842558535685\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5806889673686539\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5444375146313533\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5344011407476021\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5409369496312884\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6094298496678898\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5642360146429197\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5459706768827793\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5426593760569499\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5364036300805257\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5674701549668344\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5452138595734374\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5527201755411553\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5335314283643394\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5503731132718175\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5750791762704863\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5734763253348533\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6896216788923246\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6293181765175414\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5937823227713701\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5578792685972417\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5445955698505591\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5367858213586143\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5237415334865675\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5444360334623749\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5576835892617034\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5840053182361105\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.6874101349623107\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5508524530386453\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5365016619371158\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.542430537576962\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5489822690567897\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5442165559022629\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5454428238600049\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5698345917204901\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5467043362336476\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.549790837008588\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.556824360723278\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5803701723617359\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.611381908508397\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.676786134018003\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.3367895631716886\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.9732220559598104\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.0679091287920601\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.0662424791685805\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.9695681170388483\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.8300420444124265\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.8550362657733277\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.9959595765189028\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.8953947794877286\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.6396057554944525\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.6726828111126254\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.6493905549025414\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.6683633162452357\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.7298770867083849\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.6755996582683761\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.7553347480220155\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.7498013722627215\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.7606410364213724\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.6995706921067314\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.6902928857964151\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.7047422624771619\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.6907449859445594\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.7351500955466916\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.7352495936371409\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.585473584207944\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.63758249295231\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.358201464241079\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.836706053539335\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.739951307094897\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.539352452890951\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.552156144274628\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.81323981805188\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.4144212746832965\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.692133582603884\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.099533610739015\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.2314175081537515\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.10630892041639\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.1970123542034266\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.852075782086728\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -5.446984819976706\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.2781524683142136\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.8436170078040082\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9313661894726992\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.7485644222623065\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7814670275762299\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6441053829439326\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6431804940143653\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6411059291359248\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6312738494386602\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.643815184667032\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6335092284618448\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6531011768388753\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.64654669325205\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6708265403466475\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.7090125802902485\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6715341455447543\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.7049793548369905\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.711972366057406\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.7077196777589734\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.7444553023688973\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.7471519608678909\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.7530720973940109\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.8253516288346009\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.8076363809376903\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.7811604208662644\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7287469602051047\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.7012068217041048\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.698165341125797\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7337356388576697\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.7503328509721492\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.7897903499836676\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.8754122346235315\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.8867444857564212\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.9058027708636425\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.0012968802215092\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.0672236109573439\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.9407500979077171\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.829563546285359\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6723958059581878\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.6092831075091121\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5878003400040935\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5845291559840798\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5608675697862593\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5598407037777352\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5351115830840935\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5469596170064507\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5803471212210429\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5917746984114155\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.6128766905815217\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.6007880596386228\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.6237412474243353\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5865557100164739\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5875236686791855\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6091413012442274\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6216663491556403\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6303348736359514\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.6183285622019977\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5964243476555151\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5880517781397454\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.6137798117931296\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.810387221860191\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5599098925616761\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5591366759175146\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5545660442739196\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5423491493050296\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5548481301658883\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5444161384374048\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5604888641980655\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5619784306650337\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5525795853406678\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5719722893490058\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5655690875711172\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5538732978148356\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5490354512523098\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5683908331350196\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5590105585481828\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5753746168808854\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.6079857479389327\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5765617592710004\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5588182291357622\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5695802440028594\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5506507631659577\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5398355690144002\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5567896829138482\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5509842137986097\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5604162967037634\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.545801807428147\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5559234462838516\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5665911760854742\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.6515224736942027\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.949460804978896\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5647358297131571\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5734565428593322\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.8483712861415245\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.6064746645532165\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5603070498890488\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5582024300382603\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5669336849249036\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5626341689574252\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5865705878168328\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5903242899630459\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5823354929475744\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5699041036099449\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5546001861495442\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5682922291568526\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5593754439958601\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5506749510634636\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5724880437403383\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5814629418856715\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5861430210805679\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.6136680749682328\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.559052607669746\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.557245482812737\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5741591404092896\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5863086612440932\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5705757852434012\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5802073092496145\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5955252591817617\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5869222768186161\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5962435843149403\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.6145877388071875\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.6122638953710097\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6136783035305079\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.6921905887904372\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.712153636618164\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.7120104524633084\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.7341642230257099\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.6841782868988434\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.7117081907802764\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.6802006736241012\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6081236255392274\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5455801848429604\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.6666266947624194\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6057779111830192\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.6620786707924892\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.7315835652663222\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6153712235367321\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6575405815596844\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.6861072678134444\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6952080073977777\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.6298673032600295\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.6282857138506104\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.6145143185118094\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.6074694960862189\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5838918641042007\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5461386897391795\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.6017242755666712\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.6298320924091595\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.119953636050051\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.7581632581742642\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.6857860350826549\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.6428217798297636\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.592933428091733\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.600180973140006\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5983530215372777\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.6090020984858907\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6122294531997027\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5660260905403126\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.584495489707778\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5710802941277079\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5684207100170299\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.609226368107897\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5849359154519637\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5874771519421284\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.760753709855651\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.6174762186026129\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5895278630223676\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5751878110121104\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5668441043504501\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.6528853113789376\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.6358283639352312\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5819365316416863\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5701305823857655\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5623431530342351\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.565210533356228\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5651019141333776\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5980713730389153\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.589082278184874\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5841663445788114\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5907306637560922\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.6392306504774234\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.6455806881633366\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.6535669587398922\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6307023036328226\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.842443334473495\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.8747021423320565\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.884212698060037\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.378475635029492\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.426220265119081\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.965832426917165\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.3442879040765865\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.398213205229184\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.615870433665006\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.900145964901348\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.5264946820442695\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.063084456839215\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.0485965366539896\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.896289684458411\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.867551991871365\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -6.6628339305486985\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.8252662987888275\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.2637843718356159\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.1228256726264323\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.9329758661592291\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8839275890660889\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.8207515277105892\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6353260745070526\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6560511838739697\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.711490856919431\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.5969408210987265\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6505544493449207\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6219841303123667\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5857933838516982\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6296784359853216\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6636035574987394\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.631583661419306\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.640422963893959\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.6076873582205655\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5583315017974713\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5813464613151722\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5720957810676102\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5775747112959588\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5900206680583309\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5694898791093066\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5997920741060097\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.577986592942763\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5753972055816342\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5941813417365711\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5933408969770488\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.6039992463633307\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5947189278714461\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5620818162230041\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5829759891811321\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5365633594581471\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5519583139339714\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5710684649991127\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5536615807191692\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5822039460639548\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5672256429586885\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5625010363879394\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5888026257533684\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5680635243752161\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5252902426948606\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5418143142313119\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5460545571436565\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.6097284476200329\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5394552094000159\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5457713378708022\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5998275317002033\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5492227558557475\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5602158689102142\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5408847831761967\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5686359049186627\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5707176910057404\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5759909536785698\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5688217689201163\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5764905579316514\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.6425616945961785\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5815099021306365\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.696011892626428\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.7431110274409827\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.6398604917690627\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.6833592197820297\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6068349170826164\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5740693084606812\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5504748052529195\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5621671393846096\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5579129254598605\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5427985058977884\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.569341116474951\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5322540196216856\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5524619821717363\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5343376627221466\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.551099339850454\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5599205944447192\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5451726943332561\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5250954264021004\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5249329323901267\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5176838623698159\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5202386881874358\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5354063838032228\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5507197396616409\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5225077854942318\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.517818671110872\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5559598677981207\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5305689780247347\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5120426538626387\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5480368926453448\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5458074229981397\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5441666397194334\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5242228974441994\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6365291869685845\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5549512012655634\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5359706144931579\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5171288520573764\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5352903052723668\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5216420266633407\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5175282709002319\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5186967753345865\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5432813892376496\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5398146533211898\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5246221313700826\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5262285651086535\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5314210039975578\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5293582245044742\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5386092054066982\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5395257469644577\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5279431612998642\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5275822982974343\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.546940609975984\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5326220427087914\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5560041823285793\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5356035644233159\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5421719978125598\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.563731177943677\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5359648965462187\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5444669885672893\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5422064803658722\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5432155710439772\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5374859167722296\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5419176238034624\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5269083339062897\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5352833786837853\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5342461815951673\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5244178917464982\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.8754475020512836\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5950657808778492\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5922725628454335\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5226113309053617\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5396005245619097\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5616531163168628\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5418323514711719\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5517439943310094\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5532056300971591\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.552574153160678\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5186282792264993\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5456364171716165\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5376623465385579\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5316579572368768\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5248169212243446\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5442330788188414\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5612459638662111\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5645049569334478\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5943874287738127\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5678628660391355\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.580242843452719\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5626932877731331\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5448968410582816\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5312979194366981\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5508401452176979\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5787106497103812\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5528377717290437\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5872724834744365\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.6047942746109307\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.6033206763872135\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.6656523644560746\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.6216445468087858\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5734892251632278\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5505456837629782\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5606470163778571\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5503680736167144\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5352436824240219\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5299403486383029\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5559687846427673\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5438337866120898\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5360588813544932\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5465180104082402\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5488127134025673\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5389073080769691\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5526702012256998\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5468270242817072\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5475828433425453\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5432612325661991\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5542529476970713\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5586186086681947\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5685047514007884\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5504791675965902\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5903493441254039\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5708057978962003\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5800063809301494\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5747464842147345\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.575411648709988\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5720797282876804\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5651359198375873\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.326799301506724\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.490668169735427\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.5756598271460645\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.6045815113739845\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.437075972921703\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.886719829492197\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.141013055558821\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.845672800413549\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.539137072803112\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.743365652745533\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.485350994500489\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.781312663343982\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.574104741234728\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.935260048850819\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.926342814702291\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -7.642813834338792\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.9246924981035516\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.3663596698505573\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9941578276593043\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.7941125218743227\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7244901404232926\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6712245088861502\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.7213295257838048\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6762045069593406\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6449026639866616\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6120693594125328\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.585956582696826\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5702329563725111\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5934015468967305\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5694117383865853\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5721297871270563\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.5706315164754535\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5690093040727305\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5866869652171013\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5543294103261168\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5722142106729509\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5816886349636496\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5453298243089116\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5526201111105428\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5582673144250427\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5601306235115764\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5757935769425582\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5550424728392847\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5810000852177355\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5972419302795564\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.606749408398821\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.6060431551087337\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5821918423473111\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.6150323880962109\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.6392721428379707\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5977595112380779\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.6054496232740036\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6621762051729175\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.6090630508030017\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.6024793728763261\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.580198489648268\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.586985415153783\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5554641188262541\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5321132183735799\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.547198980478146\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5682457155207966\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5482335180306153\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.572878528942458\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5547966212809321\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5639564202611406\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5307578678519917\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5471886695178207\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5426766780214858\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5549734330009285\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.554010074250604\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5683309591042751\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5650448925757925\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5530033957504294\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5675906872387428\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.563778840737848\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5647476722038545\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.571700760289027\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.585623713504299\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5620043330490132\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5586132084737703\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5848543731978775\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5652886024067193\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5914287647027627\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5378733537520994\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5513215540597604\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5837462867405196\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5937180329122917\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5645017236913064\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5644032005041086\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5589756995015778\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5549898171062355\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5863425987237166\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5635041095910448\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5400003493304358\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5613990550960888\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5480194740565879\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5432794390531513\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5542830507615883\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5803583903969004\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5567893140285274\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5411585902389298\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5782801471191721\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5732306857602759\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.605660173740342\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.579594110145317\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.6115709215859133\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5935536284728219\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6872700943813463\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6153604257792685\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6291967268488537\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5892860952585904\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5921555688232374\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6026832940434419\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6126025068958583\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.633277412031592\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6865097802501038\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.740257073861577\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.7245528715114316\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.6850186763131447\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.6616672476205284\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.6163353945221511\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.6425315078641693\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.7101886076987026\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.6397587536635323\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.6234921342567946\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.6221717418157873\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.6285977682121805\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.6522683504317941\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.6358662168953804\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.6533122898194077\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.6247720245353002\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.6096418156292945\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.6173218468392772\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.0724973312558033\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.6260220992647202\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5841907479285929\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5583772862347811\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.560489177973668\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5725716578585117\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.58937876583395\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5617645567193322\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5846072030869688\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.668718594687318\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5968241392651044\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5811251795625662\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5970256134728005\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5750989113515\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5793619773872145\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5657032705480791\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5614810292511474\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5859759010103609\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5750318700484272\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6019912619470754\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6221838671030412\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.6217664671522319\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.6203140859039006\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.6012313981202891\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.6267278884215979\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.6496020011414817\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.6614281954047658\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.633326624736413\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.6325643236395367\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.6051640731206774\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5888248522886448\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5654452449801458\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5529756747848568\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5527676532793324\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5497887749468052\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5525185715615888\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5315167822854425\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5536663557467334\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5287402189695547\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5455349501446135\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5599380118689755\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5614255204550457\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5359458968628041\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5883130770161299\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5627547734387813\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5628413210954343\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5982520859350628\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5418259839752153\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5661084601545887\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5830195407906403\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.6324903290132682\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.6350505416810819\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5837994017707042\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5510538198906914\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5710106226336638\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.6578305167965319\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.7378845174054743\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.6825684847236948\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.7485261459589219\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.6483511626805385\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.7361344897004662\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5901009404378793\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.6379719263179793\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5904264647206187\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.599599851036137\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5938250255192662\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5874058097335773\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.873372526293285\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.687942595491288\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.093042034497793\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.885899406944048\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -6.877897732430078\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.138293589939979\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.6571765411144375\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.4383740569740375\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.823954718522113\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.098148144223187\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.100791022613719\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.523108871926813\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.088314404599163\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.925538574643431\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.899302447331483\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -5.986635692179428\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.1845813581337854\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -0.8326376276446643\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.8579379842262158\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.6459771684256888\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.6135438440168991\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6029387026664417\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.59188248101668\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.5883536262240123\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6007123389316676\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6470713804343373\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.644501177981812\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5734302411502691\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5725637970236986\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5681852034694155\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5951085680617898\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6130012426555024\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5760092528646761\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.564186162306532\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5552327186423054\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.558107992855657\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5702126636751766\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.3007563057161564\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.562602918688422\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5373247718754232\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5551489085429021\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5812634050712393\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5752020805658663\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5907211067980946\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.6054848060871586\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5834601996169826\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5653738005140334\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5977345351412899\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5867653664519323\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5738508126021821\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.6210112164944142\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5821289653456262\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5777623937750206\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.6798814346799088\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.8138974117137562\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5560298586339821\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5799426684520492\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5796503705626999\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5840406739893118\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.61388187843408\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5836763911702395\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.6013106571190663\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5827221560530207\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.6009045606997807\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5843182047181797\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5961496782819076\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5994607166678304\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.6266279649735289\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.6354604694571031\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6266751628227134\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6225536031364486\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6626619906457373\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.7300495753399184\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.7305864668414648\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.6936249233351393\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.7116278468215803\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6773256288243232\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.748448389597582\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.6111720353579594\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6114089532776468\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.597937021903289\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5971954450998852\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6044841656911271\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.632603854784709\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6190835028818432\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6325031542497133\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.6172420177931444\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5835951135500587\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.6072379348938854\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.7476954658146806\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.7173121408670498\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.6303966767304338\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.6623418327778049\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.1414090641255639\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.5354064000630465\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.6153953844490425\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5926523670183136\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5997197872675922\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5768596667513848\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.6158227130540532\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5567903733796639\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5966255398152164\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5976199463596819\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.6086911326906913\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5901279531510732\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5813920133778349\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5907502226149695\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5933866226405962\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6335998316256155\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6670854176403194\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.7081743463911618\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.6939944109988028\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6207078925953792\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.6262354589344648\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.6313061769411998\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5746008430440541\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5811115917988816\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5700375265422543\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5671554277718599\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5503757508690496\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.57153197360074\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5964552583815705\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5613666394423147\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5497855496977645\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5707882867319151\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5785195879388687\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5597144884351907\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5476329913256253\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5553190209367871\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5522032496347279\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5675214619313267\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5620790455856006\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.59311732796618\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5826139469319307\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5698866565321999\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.571809469306293\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5638320080230921\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5594219855891274\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5521091184535895\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5509386073759415\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5340180022655258\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5397464075544982\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5437401605797308\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5456851306770807\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5154314607177087\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5117457636206493\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5321573756707704\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.530543043417791\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5512781909407516\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.526379530715027\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5344749003200792\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5362209508001421\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5352314173366419\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5565343641591141\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.531382863169143\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5424525239607797\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5408686602599456\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5338072707709313\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.657868493883945\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5760004837445482\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5409919363479078\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5372255149182624\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5568841389807421\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5633743873758021\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.6341601363055466\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5713982569760904\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5465342058779137\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5633319752196825\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5808493880555399\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5398936174029424\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5375490606300857\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.547475805157291\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5542613296462637\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5392699463219981\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5411475436606707\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5472880059242295\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5551374259623729\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5257092483856972\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5230974540403405\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.52692837965118\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5178148957190529\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5659366284659355\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5377822332987239\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5323489563609962\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5311443698127546\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5308269655299145\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5563882023554377\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5608108309125049\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5271821322748766\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5538721875189009\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.7878424285184713\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6119782401788257\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5654596752594954\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5427558844412074\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.6705631070056044\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.6310845042925405\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5832706615794832\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.563249219599105\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5654357974738913\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5731478008626376\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -7.037701216120281\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.038185179668745\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.091185602749533\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.623050414546258\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.023309899606584\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.711036426946416\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.788853079943586\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.379101464892044\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.342041565630288\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.8481249042808905\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.193087468044375\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.330339031686269\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.055995997527414\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.861085535744621\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.824365865411388\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -7.396948416832933\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.85359739841372\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.8407800941561763\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.52931596137319\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8348732035376423\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8061072420514219\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.7156151943426629\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.7246207368013228\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.7613010722852215\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.7033619347248787\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7693437061498929\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.69310178708976\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.6504261411730923\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6332312929177542\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.635790877743949\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.6319712745767885\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6084829965046257\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5937280406286201\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5966919028111503\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6015027136675387\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6222917908767728\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6140093262752996\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6398728850953035\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.6438128890063339\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.6042580633254766\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.578885815345476\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5582895544836071\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5381893516944389\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5419205232978794\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5435542606649939\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5492262415624721\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5492163808501305\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5687005554602123\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.708483588618356\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5707330934725721\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.597792103367543\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5704969475649412\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5923660145376967\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5882549940436435\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5682023988029948\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5886918191481171\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5707985332221761\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5938103222181088\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5701509817004404\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5741235446977191\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5769109366371772\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5632172634144718\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5535362247737536\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5487968671629114\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5479740756819037\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5568529123495276\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5702504084924954\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5624388834824714\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5449435322869391\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.5705971955090027\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5502074440519665\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5752977722247603\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5627295498612528\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.6082635240793985\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5620684092726893\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5563341122801188\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5856019164242723\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5711575383812313\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.615619064284903\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6064540769358884\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.6323040839854448\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.7492708360431947\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6072345168188272\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.6141765552168311\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6340337007899456\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6012723832999629\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.6351363718047411\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.6519796244406314\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.6122742018449745\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6055677489073351\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5836984621376442\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5424828572097387\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5568375849357823\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5631590904418573\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5630791463682744\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5603973730873639\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5916372803481116\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5658422059818474\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5461852250327199\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5370185747908963\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5384038149317993\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5353501688607196\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5362875455961704\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5639563387710482\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5443088403064814\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.538644324016129\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5451235672006587\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5384202579375916\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5341507693231188\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5614642670365273\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5502853548934102\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.548830622880896\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5643851239304172\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5724582782294665\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5302074733427533\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5429473676169493\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5380565901217359\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5360224754278196\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.541101862714552\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.557765496598738\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5522831243516881\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5402803747348982\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5377331302741538\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5634396484462977\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5576705696513774\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5730485716778244\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5403363330893053\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5405692688081413\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5453206578366198\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5360362336451285\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5433861495507145\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.536026121951033\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5423796571359142\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5322327486443722\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5242813599593041\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5565269530507259\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5329439592309124\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.54643257733851\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5418883663324894\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5325927160752604\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5371942153947954\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5280325286494141\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5527055242575366\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5490534993683793\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5398439007859533\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5277382764620485\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5154053181658874\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5392391742320657\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5180630600072144\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5322936464014838\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.523544950600134\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5290151858913106\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5494528074922589\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5254276660797117\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5484701153240641\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5546570359743294\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.540675894092129\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5327609331521829\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5204421041973333\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5193623466518268\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5373662266860443\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5370916986565808\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5114930198022264\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5305866346115391\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.5380470383489436\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5582452709096501\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5455190742154922\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5639080227476865\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5796035777302514\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5489171739459283\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5411253362938967\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.536521280921956\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5553339606374386\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5424252540200646\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5379726372027108\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5361870237029894\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5300240051595848\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5450413302169693\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5445793196074953\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5424821280690528\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5460733170408061\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5526637277773215\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5521330954289438\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5514696193012165\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5577553132198146\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5612366657001735\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5498687539650006\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5777781793933201\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5611756010281784\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5786665979010244\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5657841274960267\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.542590983698783\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5515477431308672\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.542983151737963\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5585199253371826\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5363411215191887\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5449084939789983\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5566867301487015\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5517825215840374\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5441324697461816\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -7.028012909743677\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.164433894407909\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.156353987106043\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -7.536230995192616\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.332940079923682\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.48222456228982\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.394123647633985\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.210092989893029\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.953317135706293\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.805389528616504\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.6943822193443285\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.759193019305733\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.145621846617774\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.3900418576732365\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.845264399353564\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -7.572157769716795\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.8697304605742684\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.3287842033456312\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9220074933959578\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.765840558595454\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7361557416782304\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6511856059402976\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.5941447864503947\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.5567475127216219\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.5824888281672291\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7167864528947884\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.55361569074241\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5730666386903897\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5860513957336468\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5508025864554756\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.555001143967076\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.5590307476249656\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5552272057820548\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5673922652726964\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5372218011359282\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5482258945636856\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.5509853089839593\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.5230891895020703\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.5320607274040423\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5279011016810718\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.49210258602350493\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5214068570201479\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5096123811229085\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5482176015513223\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5568899257546776\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5490261077919\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5892222801862563\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.545352801621355\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5352524191953902\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.526057823726888\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5245877368487594\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5283740857907776\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.5362276099027874\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5348698400824439\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5538209334582356\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.6003495726934184\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5545756042231189\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.6174385307098449\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5847410294141743\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.6168152999560269\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5927167274696541\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5921229670045145\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.6186155894022058\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5759224362476378\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.6003227207745913\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5626186213863049\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5878737337685581\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.597737450329978\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.6152465790873581\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6377392011673261\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6369437923243805\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6359805347341271\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.6073075046027168\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5818100487439402\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5466280022346688\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5420346498339365\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.5430391711036004\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5401944239067262\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5525294710766863\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5564835402506857\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5385717598996317\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5379695457476932\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5528417615767098\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5650030488335211\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5551110505341229\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.5616843871729261\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5675758673357798\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5670099254078464\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5795550828844543\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5728345659102967\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5622404865728038\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.5432631010588735\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5799888501826385\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5537822883738438\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5715328897251893\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5408892676867145\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5450720251359472\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5455971188509128\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5847816274365502\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.6882740732723885\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.6014870939386725\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.689932984982624\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -4.063838586579715\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.6482769952210596\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.6179587034319249\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.6057223970128929\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5767665985614575\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5700313402893378\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5692184236670527\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5885613823839311\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5825890457988665\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5678336795862521\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.6068318908365598\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5908419645468027\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5580775239636301\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5663435414550609\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5589642720648942\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5654955909586536\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5731403149383552\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5699650430623819\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5625199059904545\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5750776681875646\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5963696527239621\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5397463704275016\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.578108812539153\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5495974401445624\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5661681259648705\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.540073116022683\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5431557366385797\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5529211732380211\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5575988231482579\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5622563194244268\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5464924429465516\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5543682853733943\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.568501555034316\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5679770192923228\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5453877573218512\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5449405750779543\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5833400588059195\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5699944075958979\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5485307425964404\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5533881123157977\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5853184577238807\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5743982697233505\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5551124917248863\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5344128258323204\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5429394402262475\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5345451847817313\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5181860975903512\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5286209564426404\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -5.410632896167153\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5843861199243255\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6467213972033973\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.6230362429371865\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.573869792272427\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5585309745645377\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5412414556837963\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5513480890983229\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5469238049935611\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5994285097749844\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5584814756116961\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5286627607904367\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.6767171829832731\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.0495344651340666\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.746387389624605\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5923542807026402\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5675423525250197\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5852803232120565\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.6154462818333544\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.642366791799506\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5680936058800501\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5768464191033691\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5710520181922196\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.584106341447567\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5668237582052629\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5670802402333215\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5485997604186177\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5689630756502921\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5570348030906238\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.533684627044953\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5258210267445392\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5181711512756632\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.533628872639664\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5395975397069418\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5467180535003453\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5390017582674643\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5481633090618147\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5399887948560059\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5234342612567635\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5231370442888571\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5265070587772575\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5405236742842512\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5397905229298431\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5440688101302132\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5396344056164883\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5571070648323283\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5448515880237472\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5567407332146275\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.6290231362804195\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5379711089305217\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.556576842873803\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.972339120688108\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.464733623761413\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.763834257373169\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.795463576650909\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -7.0329022194440585\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.4618949779730075\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -6.928506732133209\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -7.0185550021689735\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -7.551291688491262\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.669619252526157\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -6.720796841541458\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -6.765682666150249\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.748838124518834\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.8967448419234225\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -8.173386514152044\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.0166814933475745\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.352310578389364\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.0181429993288198\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8320931935931034\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.7153316829142122\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.6606983926322068\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.5854801282336762\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.5674805643311635\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.5667043445684032\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.5948380098943464\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.5817167262483259\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.5710482137812969\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.5801843945804798\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.5418762959906296\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.5662599569696323\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.581392446312632\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.5754701906622932\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.5640127644723174\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.5780054263041674\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5798747693258148\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.58375610845308\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6162952289163709\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.6075774573110908\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5900500376610032\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5990617665064073\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5917679278555427\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5858019260189252\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.5698998776489468\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5651906776468651\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5764537112783117\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5815538369601295\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5895228347378363\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5917898768468666\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5765361034709674\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5687432560184678\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5786639537431671\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.6276721128140216\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.57407279109854\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5645459978130017\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5854044078083063\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5611650488148453\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.570149349246738\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5688484282519025\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5624368066004678\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5588952047120074\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.59612842570021\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.5703286419668434\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.5635456674187644\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.5597399188923096\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.5589258274169211\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.5642374066629969\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.5753535457667419\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.5877741739005105\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.639137882934447\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.5780113937912236\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.5876293759945048\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5706234266177942\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.5735587298326423\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5724342677153698\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.581746084223993\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.560812094850094\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.5696508190888204\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.5555641927491104\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5588871057893126\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5309938963725255\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5329959964199827\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.5418880506881173\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.5423915891331209\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.5720832995486095\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6805766168090508\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.5840311039201532\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.5956433371894508\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.5947538512905421\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6036870900260121\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5993116238585837\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.6087830169129864\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.6260753941455749\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.6065889070905356\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5941242150424508\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.589571988806542\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5869226923587817\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5837667668543772\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5728926418142589\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.5694064285012568\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.546048216920573\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.5965155115478973\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5341300741907365\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5520790115357531\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.565557693397591\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.634270468974377\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.6317943496165238\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.6545380123996057\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.6026553197250523\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5622027523860202\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5650379493491666\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5559381375933278\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5402685592006272\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5394732319383709\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.538804034393035\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5367888426002628\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5736059821228056\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.775188968385942\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.551672583682491\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.6044573567893008\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5875832071680724\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.6307437359606024\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.6219305223085435\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5764160917772709\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.575251208069258\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5614944811910618\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5749349721363854\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.554563054768266\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.5472307074530339\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.5557916083309564\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5585071188314814\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5880822740951686\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.6108680089616789\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.6017994012092489\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5639757406901831\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5883493095665425\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5531310943947859\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5529696420134056\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.5510503208387894\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5454733702245789\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5599795012660538\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5718807588649468\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5689385296478123\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5573593690024705\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5478856559332282\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5521408711276371\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5287751095695107\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5546397905319952\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5423767975505392\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6428431346483425\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.6181223472283086\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.6400433629880018\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.6309397234561349\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.589732831468993\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.6057374688684548\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5996950661493435\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.576561897184737\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5571136122045423\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5571893338546119\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.4228103655387363\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5879277147767972\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.0031324340876586\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5663648224229659\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.6179809076871812\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.6213920800029755\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5417054217518387\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5997978200888664\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5842978796074203\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.584035200155754\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5688809085020978\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5807147301569201\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5625685732877934\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5536461550661008\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.583454008615615\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5742264596008698\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5891086842423241\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.6772326039885337\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.6171535623362782\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.652011252278075\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.709547813267817\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.6268525620788009\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5877960721293415\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5908083111683142\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5776303874006696\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.7036102727271054\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.7086546742110953\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.8517090905464729\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -2.204091912605141\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.5157418884666032\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.8117483403887246\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.71851606951774\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.6977908926593281\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.7353562218866557\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.6322942160958368\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.6500874901322986\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.581368104449529\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.57290979124306\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.59215310636389\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.585052339646128\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5599161213235871\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.602346552266307\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -7.853394817309041\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -6.721063658873984\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.981432368378302\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.262234039366982\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.694124300976934\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -6.965796352501006\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.020213785825583\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.831955324506259\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.495652770664187\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -6.9591646402839205\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.599056039206408\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.0106684481713675\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -7.003983742143819\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.985414508479316\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -6.21829832911664\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1.2410786664297053\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -0.9732249995406161\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -0.9756960209139467\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -0.8765456598838284\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -0.8839525284892992\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -0.7398942877627301\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.6767256185358046\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6715913226694247\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6515494671917184\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.6523806604767819\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.679716826739573\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7058264637557363\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.6390941439257416\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.6997052532764534\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.829874755493366\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.6927153475623562\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.9343544607216347\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.712975417119982\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.6628362370774065\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.5964560661647672\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.6763533038447559\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.6625408146379834\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.6265821607441646\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -0.5985353954093184\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.5833466191061922\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.5652377549405092\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.5682167628623374\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.574676281350538\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.5830102290448211\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.5739576310213388\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.5665428018886083\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.5599554324929407\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.5723063436676915\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.5920826871402128\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.5684122991829187\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.5677536343975107\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.550383966020132\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.5583030645085592\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.5436530466111871\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.5475242160535472\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.5937048393462518\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.5387260145415089\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.5579021383273017\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.5793448141839306\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.5755091046671009\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.5614611419132582\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.6098075604851506\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.7442460578789215\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.639914086606446\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.6382412307846086\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.6378306691805069\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -0.6039826496063739\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.7771601128312596\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.6775713251650265\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.6873391262926202\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.7613592995281162\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.7962444810231328\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.0584419291005358\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.142053504690813\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.1382420940134947\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.0581510305975501\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.8732351627005716\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -0.6699882366744156\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.6646698253799286\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.716152834012532\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.60714341595353\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6258648059813038\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.6239974142239585\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6444395809593307\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6396703668405713\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.6465934349524584\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.6282131843523753\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.6443744721368302\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.6085446743394374\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.6316288702771384\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.6352092787531476\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.6157277440766716\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.6292283009057713\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.6316974163901921\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.6211947775235678\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.6388719144788438\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.6465264054312317\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.6725358049579981\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.6746224538517867\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.6540996581146752\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.6278474534299192\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.6136654858209397\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.6087839509580333\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5700209320939548\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.578904187042523\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.6021491786183837\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5889340942440602\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5818292414654813\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.6000386470092911\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5830479793762712\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5806626912657462\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5739129077728193\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5765704553735557\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.5937537060052924\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.6137605569485636\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.6505318130257048\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.6274062400038848\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.6151217492223434\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.5931106921674494\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.5600216665159373\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.5734305503728888\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5654846992754142\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5673778778624103\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5720202726887673\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5670597102603245\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5572676551393942\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.631166718862304\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.7423095812474846\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.6046164332509153\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5772915210165582\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5751556834454694\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5742519602990191\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.557655722722436\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5574517798835932\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5714806246009756\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5706521781939117\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.5721128283017695\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.578467605183933\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.5819734486628786\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.5746121288414073\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5782896301637541\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5919614289735877\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5706561760645045\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5708148731248337\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.6044068393937669\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.6717008993320228\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.7146238332516778\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.7452207692839085\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.6196949617948264\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5554539246623429\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5643464215550702\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5875208700448689\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5577949963179687\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5361940643648506\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.531888877923223\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5456254063958341\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.5429853730034135\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5459869313101586\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.54711931590016\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5589640014409026\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5525670804701814\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5495716932737661\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5562437751171874\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.542449626334887\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.558098717644081\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5461621844930751\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.5594675406613738\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.5557532519246482\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.5596338355778687\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5599794070556883\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5551993975319187\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5482364116093396\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5590375449553376\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.552311696600581\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.570734630650367\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5503332154367471\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5405708662745132\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5523940955099661\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5490276057804294\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5626978968374864\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.5625061881274496\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5727413042337743\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.558470008910875\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5461187684873638\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5469928800485887\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5665720600203841\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5598742336857283\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5732308825859848\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5431132229248674\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5487453590131826\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5434814038482985\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5786746553342119\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.5628416066773322\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5621936480797113\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.569159225411604\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5944732660660534\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.6444508652652752\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.6467354715916572\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.6308856818310461\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -6.691278203829484\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -6.862407653123421\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -7.045328360184125\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -6.216299731434552\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -7.827359834151978\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -6.649100417568438\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -7.293062102246813\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -7.035172916973897\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -6.901137068806367\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -6.91750563375247\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -7.582669373236572\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -7.041383502184419\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -7.300602655603814\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -6.757940759408809\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -6.9063606864828015\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -6.616189139496889\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.1642074292724196\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -1.1222340962182937\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -1.1555007005777824\n",
      "Exploration noise: 0.006631581657719379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\system_functions.py:61: RuntimeWarning: invalid value encountered in scalar power\n",
      "  CP = (2 * self.fi * kd * CI / kt) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 20 | avg. reward: -5989.269477514046\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -32.13304772096627\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.406943387867028\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -0.7707966561884803\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -0.6969408105435542\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -0.6972554281021732\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -0.7902126592621959\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -0.6683813050090427\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -0.7137337884217512\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -0.8575467500667132\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -0.9820940598429685\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -0.802217670883938\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -0.8380847467808573\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -0.894330329084162\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -0.8640534153626888\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -0.7272271166906421\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -0.6377728913488864\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -0.707702866254117\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -0.7905089056959195\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -0.8869130743335265\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.0230507593591467\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -0.8487366779305495\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -0.7247456470213389\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -0.734123653291026\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -0.7055565062144751\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -0.7607843327588903\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -0.6804693296774008\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -0.6996678742966717\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -0.6837810821870812\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -0.7413713591302414\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -0.7245303193538143\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -0.7261571067367965\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -0.7564584382100351\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -0.8429963002138215\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -0.9494762189762992\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -0.7299188597459958\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -0.760310400064368\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -0.8860915889563026\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -0.8825628739106154\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -0.7776227960440408\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -0.7356909642302807\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -0.7215647642116904\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -0.7384693924427576\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -0.9188036116022392\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -0.7808043781527115\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -0.8191533959182493\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -0.9129531249439508\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -0.8272196850888398\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.096652622995181\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -0.8450376004337143\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -0.8864559577578571\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -0.8968509408885126\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -0.6752363913979132\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -0.5893361344477779\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -0.6009884750431869\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -0.5700133368305129\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -0.5858207606054384\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -0.6061731685785818\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -0.605084066318162\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.0488274611957704\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -0.5846869604667311\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -0.5919634408119525\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -0.5808827784899246\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -0.6158770718217244\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -0.6051644054789059\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -0.6010959103790188\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -0.6027832449452796\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -0.6393052721408171\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -0.6155722425973117\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -0.57011055701514\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -0.5929023353936644\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -0.5595851706480314\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -0.564875407898224\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -0.5456532960274819\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -0.5402902019803776\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -0.5514519497832585\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -0.5610864137966887\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -0.5372970056004377\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -0.5451853573080978\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -0.5553190579196446\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -0.551282665610389\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -0.5354349459669308\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -0.551975515331089\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -0.5624557143193212\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -0.5511965350565186\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -0.5524848535483121\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -0.5521747249153294\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -0.5206574929456167\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -0.5383484130362691\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -0.5274113010268756\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -0.5452983402392559\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -0.5431170080772955\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -0.5435196224422847\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -0.5572541504369876\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -0.5406153291616851\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -0.551065972450474\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -0.5583473625029859\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -0.5678413507840163\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -0.5840610622799595\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -0.5920707669790155\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -0.6626572006898885\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -0.7204761585706957\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -0.8081063215454982\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -0.5741225341582327\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -0.5607935468969386\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -0.5736049488409639\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -0.5789326871695238\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -0.5454279773259715\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -0.5745005250807813\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -0.553167771144515\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -0.584823380323893\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -0.5692192579423608\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -0.5859240896985223\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -0.5637301909329042\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -0.5757137529741461\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -0.5572189631741606\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -0.5727139961375698\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -0.5591161683785462\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -0.6154972075486659\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -0.6443754269813609\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -0.6023448339208483\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -0.6010238207297054\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -0.5434759361180594\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -0.5308725607332114\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -0.5495778538228651\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -0.5351505675313541\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -0.5275672345533434\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -0.5359387752341553\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -0.5362350531392431\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -0.5553020460159246\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -0.5391531398345998\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -0.5396658370425017\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -0.5352123109286279\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -0.5415950548265783\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -0.5277619789449534\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -0.5379686679318232\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -0.5423988929831287\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -0.5458113263942443\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -0.549586392975052\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -0.5362848827342644\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -0.5356002740780036\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -0.5582161502299522\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -0.5419526121251408\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -0.5462871765281474\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -0.5916032900549923\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -0.6285209135995835\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -0.5520349384401606\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -0.5900347108232483\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -0.6635465008213509\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -0.6298694168505599\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -0.6417554547657474\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -0.5322843326143953\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -0.5550709962877644\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -0.5419184839378666\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -0.5664791182355606\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -0.5980617935372388\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -0.5308219182097571\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -0.5244866514800187\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -0.5280554394765852\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -0.5215928202668308\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -0.5227251124355529\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -0.5306720566164057\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -0.520227252838608\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -0.5197445563697001\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -0.5265841781692875\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -0.5347385135128149\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -0.5311767301296241\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -0.5288633214809872\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -0.5285027962552479\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -0.5256943089984175\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -0.5318599565334132\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -0.5470866264753161\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -0.5374438002526672\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -0.5061379391869377\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -0.536046871396401\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -0.5608901735864127\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -0.5416854664108852\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -0.5412552383300079\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -0.5350104782679718\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -0.5201457334463938\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -0.5156886649296001\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b7f76b83fbe66cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
