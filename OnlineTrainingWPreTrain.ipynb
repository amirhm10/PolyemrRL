{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:25.684073Z",
     "start_time": "2026-01-07T01:21:24.858630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from utils.helpers import *"
   ],
   "id": "833e2155a44bd6c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "8c565965fce1dc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:28.669032Z",
     "start_time": "2026-01-07T01:21:28.665405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "d168509011200b21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:28.978708Z",
     "start_time": "2026-01-07T01:21:28.976110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "ef60c6a882f064d1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:29.234312Z",
     "start_time": "2026-01-07T01:21:29.232264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "e0613ee1ad154d3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:29.523369Z",
     "start_time": "2026-01-07T01:21:29.521495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "aea9d86581d16b25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:30.234885Z",
     "start_time": "2026-01-07T01:21:30.231589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "1a7c69161bd5f8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "85287aecd00bcda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:34.344570Z",
     "start_time": "2026-01-07T01:21:34.342339Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "f8fbe1e84f3bb189",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:34.786298Z",
     "start_time": "2026-01-07T01:21:34.781932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "b12cb95ea94b6f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:35.238152Z",
     "start_time": "2026-01-07T01:21:35.236144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "7758bb35c5218c2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:21:35.672617Z",
     "start_time": "2026-01-07T01:21:35.670234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "c44dfc980e752980",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:22:06.294893Z",
     "start_time": "2026-01-07T01:22:06.292162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_states = {'max_s': np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ]),\n",
    "                  'min_s': np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])}"
   ],
   "id": "88095c1c82c5ad36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:22:06.720873Z",
     "start_time": "2026-01-07T01:22:06.718617Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "59d7edf141de197",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:22:07.124765Z",
     "start_time": "2026-01-07T01:22:07.122553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "e7463ce513b8ba31",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:22:07.559222Z",
     "start_time": "2026-01-07T01:22:07.556990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([256.79686253, 256.01560603,  48.99447186, 144.79949103,\n",
    "          2.82199733,   3.14014989,   2.78866348,   3.71691422,\n",
    "          6.2029936 ])\n",
    "min_max_dict[\"x_min\"] = np.array([ -272.28060121, -1112.33972595,   -76.63993491,  -608.60327886,\n",
    "           -3.94399122,    -3.93115257,    -2.9532091 ,    -4.06547624,\n",
    "          -28.25906582])"
   ],
   "id": "f15067d59c9f2f52",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:25:17.088491Z",
     "start_time": "2026-01-07T01:25:17.085242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setpoints in deviation form\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "y_sp_scenario = np.array([[4.5, 324],\n",
    "                          [3.4, 321]])\n",
    "\n",
    "y_sp_scenario = (apply_min_max(y_sp_scenario, data_min[inputs_number:], data_max[inputs_number:])\n",
    "                 - apply_min_max(steady_states[\"y_ss\"], data_min[inputs_number:], data_max[inputs_number:]))\n",
    "n_tests = 200\n",
    "set_points_len = 400\n",
    "TEST_CYCLE = [False, False, False, False, False]\n",
    "warm_start = 10\n",
    "ACTOR_FREEZE = 10 * set_points_len\n",
    "warm_start_plot = warm_start * 2 * set_points_len + ACTOR_FREEZE"
   ],
   "id": "c062e1a79f80912a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:25:18.124179Z",
     "start_time": "2026-01-07T01:25:18.106804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observer Gain\n",
    "poles = np.array(np.array([0.44619852, 0.33547649, 0.36380595, 0.70467118, 0.3562966,\n",
    "                           0.42900673, 0.4228262 , 0.96916776, 0.91230187]))\n",
    "L = compute_observer_gain(A_aug, C_aug, poles)"
   ],
   "id": "85da68bee773cd7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system is observable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Simulation\\mpc.py:124: UserWarning: Convergence was not reached after maxiter iterations.\n",
      "You asked for a tolerance of 0.001, we got 0.9999999422182038.\n",
      "  obs_gain_calc = signal.place_poles(A.T, C.T, desired_poles, method='KNV0')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "1daeca8ba3164a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T01:25:21.113331Z",
     "start_time": "2026-01-07T01:25:21.110469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TD3Agent.agent import TD3Agent\n",
    "import torch"
   ],
   "id": "49c428a23b3e48b2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:42.359460Z",
     "start_time": "2026-01-07T02:27:42.356426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "n_outputs = C_aug.shape[0]\n",
    "ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "BUFFER_CAPACITY = 40000\n",
    "ACTOR_LR = 5e-5\n",
    "CRITIC_LR = 5e-4\n",
    "SMOOTHING_STD = 0.005\n",
    "NOISE_CLIP = 0.01\n",
    "# EXPLORATION_NOISE_STD = 0.01\n",
    "GAMMA = 0.995\n",
    "TAU = 0.005 # 0.01\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 256\n",
    "STD_START = 0.02\n",
    "STD_END = 0.001\n",
    "STD_DECAY_RATE = 0.99992\n",
    "STD_DECAY_MODE = \"exp\""
   ],
   "id": "e798424baebbc371",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:42.763655Z",
     "start_time": "2026-01-07T02:27:42.720795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "td3_agent = TD3Agent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    actor_hidden=ACTOR_LAYER_SIZES,\n",
    "    critic_hidden=CRITIC_LAYER_SIZES,\n",
    "    gamma=GAMMA,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "    noise_clip=NOISE_CLIP,\n",
    "    max_action=MAX_ACTION,\n",
    "    tau=TAU,\n",
    "    std_start=STD_START,\n",
    "    std_end=STD_END,\n",
    "    std_decay_rate=STD_DECAY_RATE,\n",
    "    std_decay_mode=STD_DECAY_MODE,\n",
    "    buffer_size=BUFFER_CAPACITY,\n",
    "    device=DEVICE,\n",
    "    actor_freeze=ACTOR_FREEZE,\n",
    "    )"
   ],
   "id": "1d8ae390b2843fca",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:43.198034Z",
     "start_time": "2026-01-07T02:27:43.101317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "td3_agent.load(agent_path)"
   ],
   "id": "5d2d2b610564c69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MPC Initialization",
   "id": "fac3637c0ee4f291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:43.472435Z",
     "start_time": "2026-01-07T02:27:43.469753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1_penalty = 5.\n",
    "Q2_penalty = 1.\n",
    "R1_penalty = 1.\n",
    "R2_penalty = 1.\n",
    "Q_penalty = np.array([[Q1_penalty, 0], [0, Q2_penalty]])\n",
    "R_penalty = np.array([[R1_penalty, 0], [0, R2_penalty]])"
   ],
   "id": "211634f6f62d4c5",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:43.503344Z",
     "start_time": "2026-01-07T02:27:43.501122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1_penalty, Q2_penalty, R1_penalty, R2_penalty,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "7bf97648c044ad79",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying RL Agent on the CSTR",
   "id": "65a4fc461d9d0b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:44.227770Z",
     "start_time": "2026-01-07T02:27:44.218496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=5.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward with relative tracking bands.\n",
    "\n",
    "    data_min, data_max : arrays for [u_min..., y_min...], [u_max..., y_max...]\n",
    "    n_inputs           : number of inputs (so outputs start at index n_inputs)\n",
    "    k_rel              : per-output relative tolerance factors (same length as outputs)\n",
    "    band_floor_phys    : per-output minimum band in physical units\n",
    "    Q_diag, R_diag     : quadratic weights (same as before)\n",
    "    \"\"\"\n",
    "\n",
    "    data_min = np.asarray(data_min, float)\n",
    "    data_max = np.asarray(data_max, float)\n",
    "    dy = np.maximum(data_max[n_inputs:] - data_min[n_inputs:], 1e-12)  # phys range for each y\n",
    "\n",
    "    k_rel = np.asarray(k_rel, float)\n",
    "    band_floor_phys = np.asarray(band_floor_phys, float)\n",
    "    Q_diag = np.asarray(Q_diag, float)\n",
    "    R_diag = np.asarray(R_diag, float)\n",
    "\n",
    "    # floor in *scaled* coordinates (used if y_sp_phys is not provided)\n",
    "    band_floor_scaled = band_floor_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "    def _sigmoid(x):\n",
    "        x = np.clip(x, -60.0, 60.0)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _phi(z, kind=bonus_kind, k=bonus_k, p=bonus_p, c=bonus_c):\n",
    "        z = np.clip(z, 0.0, 1.0)\n",
    "        if kind == \"linear\":\n",
    "            return 1.0 - z\n",
    "        if kind == \"quadratic\":\n",
    "            return (1.0 - z) ** 2\n",
    "        if kind == \"exp\":\n",
    "            return (np.exp(-k * z) - np.exp(-k)) / (1.0 - np.exp(-k))\n",
    "        if kind == \"power\":\n",
    "            return 1.0 - np.power(z, p)\n",
    "        if kind == \"log\":\n",
    "            return np.log1p(c * (1.0 - z)) / np.log1p(c)\n",
    "        raise ValueError(\"unknown bonus kind\")\n",
    "\n",
    "    def reward_fn(e_scaled, du_scaled, y_sp_phys=None):\n",
    "        \"\"\"\n",
    "        e_scaled : output error in scaled deviation space  (same as before)\n",
    "        du_scaled: input move in scaled deviation space    (same as before)\n",
    "        y_sp_phys: current setpoint in *physical* units (array len = n_outputs)\n",
    "        \"\"\"\n",
    "\n",
    "        e_scaled = np.asarray(e_scaled, float)\n",
    "        du_scaled = np.asarray(du_scaled, float)\n",
    "\n",
    "        # ----- dynamic band based on setpoint -----\n",
    "        if y_sp_phys is None:\n",
    "            # fallback: just use the floor\n",
    "            band_scaled = band_floor_scaled\n",
    "        else:\n",
    "            y_sp_phys_arr = np.asarray(y_sp_phys, float)\n",
    "            # band_phys_i = max(k_rel_i * |y_sp_i|, band_floor_phys_i)\n",
    "            band_phys = np.maximum(k_rel * np.abs(y_sp_phys_arr), band_floor_phys)\n",
    "            band_scaled = band_phys / np.maximum(dy, 1e-12)\n",
    "\n",
    "        tau_scaled = tau_frac * band_scaled\n",
    "\n",
    "        # ----- inside/outside gate -----\n",
    "        abs_e = np.abs(e_scaled)\n",
    "        s_i = _sigmoid((band_scaled - abs_e) / np.maximum(tau_scaled, 1e-12))\n",
    "\n",
    "        if gate == \"prod\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64))\n",
    "        elif gate == \"mean\":\n",
    "            w_in = float(np.mean(s_i))\n",
    "        elif gate == \"geom\":\n",
    "            w_in = float(np.prod(s_i, dtype=np.float64) ** (1.0 / len(s_i)))\n",
    "        else:\n",
    "            raise ValueError(\"gate must be 'prod'|'mean'|'geom'\")\n",
    "\n",
    "        # ----- core quadratic costs -----\n",
    "        err_quad = np.sum(Q_diag * (e_scaled ** 2))\n",
    "        err_eff = (1.0 - w_in) * err_quad + w_in * (lam_in * err_quad)\n",
    "        move = np.sum(R_diag * (du_scaled ** 2))\n",
    "\n",
    "        # ----- linear penalties around band edge -----\n",
    "        slope_at_edge = 2.0 * Q_diag * band_scaled\n",
    "\n",
    "        overflow = np.maximum(abs_e - band_scaled, 0.0)\n",
    "        lin_out = (1.0 - w_in) * np.sum(gamma_out * slope_at_edge * overflow)\n",
    "\n",
    "        inside_mag = np.minimum(abs_e, band_scaled)\n",
    "        lin_in = w_in * np.sum(gamma_in * slope_at_edge * inside_mag)\n",
    "\n",
    "        # ----- bonus near zero error -----\n",
    "        qb2 = Q_diag * (band_scaled ** 2)\n",
    "        z = abs_e / np.maximum(band_scaled, 1e-12)\n",
    "        phi = _phi(z)\n",
    "        bonus = w_in * beta * np.sum(qb2 * phi)\n",
    "\n",
    "        # ----- total reward -----\n",
    "        return -(err_eff + move + lin_out + lin_in) + bonus\n",
    "\n",
    "    params = dict(\n",
    "        k_rel=k_rel,\n",
    "        band_floor_phys=band_floor_phys,\n",
    "        band_floor_scaled=band_floor_scaled,\n",
    "        Q_diag=Q_diag,\n",
    "        R_diag=R_diag,\n",
    "        tau_frac=tau_frac,\n",
    "        gamma_out=gamma_out,\n",
    "        gamma_in=gamma_in,\n",
    "        beta=beta,\n",
    "        gate=gate,\n",
    "        lam_in=lam_in,\n",
    "        bonus_kind=bonus_kind,\n",
    "        bonus_k=bonus_k,\n",
    "        bonus_p=bonus_p,\n",
    "        bonus_c=bonus_c,\n",
    "    )\n",
    "    return params, reward_fn"
   ],
   "id": "317946f8d424ca6c",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reward configuration",
   "id": "29803d0d4ebeff3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:44.819263Z",
     "start_time": "2026-01-07T02:27:44.815673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_inputs = 2\n",
    "\n",
    "dy = data_max[n_inputs:] - data_min[n_inputs:]\n",
    "y_sp_nom = 0.5 * (data_min[n_inputs:] + data_max[n_inputs:])\n",
    "\n",
    "k_rel = np.array([0.003, 0.0003])\n",
    "band_floor_phys = np.array([0.006, 0.07])\n",
    "\n",
    "band_phys = np.maximum(k_rel * np.abs(y_sp_nom), band_floor_phys)\n",
    "\n",
    "scale_factor = 1.0  # use 2.0 for [-1, 1] scaling, 1.0 for [0, 1]\n",
    "band_scaled = scale_factor * band_phys / dy\n",
    "\n",
    "q0 = 1.4\n",
    "Q_diag = q0 / np.maximum(band_scaled ** 2, 1e-12)\n",
    "\n",
    "print(\"dy:\", dy)\n",
    "print(\"y_sp_nom:\", y_sp_nom)\n",
    "print(\"band_phys:\", band_phys)\n",
    "print(\"band_scaled:\", band_scaled)\n",
    "print(\"Q_diag:\", Q_diag)"
   ],
   "id": "85575e2c60b10163",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: [0.22165278 0.78153727]\n",
      "y_sp_nom: [  3.83915067 323.21371982]\n",
      "band_phys: [0.01151745 0.09696412]\n",
      "band_scaled: [0.05196169 0.12406845]\n",
      "Q_diag: [518.51529284  90.95055189]\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:45.507607Z",
     "start_time": "2026-01-07T02:27:45.504412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_diag = np.array([518., 90.])          # rounded from the band-based calculation\n",
    "R_diag = np.array([90., 90.])          # move cost for du_scaled ~ 0.02\n",
    "\n",
    "n_inputs = 2\n",
    "\n",
    "print(\"Band scaled are:\")\n",
    "\n",
    "params, reward_fn = make_reward_fn_relative_QR(\n",
    "    data_min, data_max, n_inputs,\n",
    "    k_rel, band_floor_phys,\n",
    "    Q_diag, R_diag,\n",
    "    tau_frac=0.7,\n",
    "    gamma_out=0.5, gamma_in=0.5,\n",
    "    beta=7.0, gate=\"geom\", lam_in=1.0,\n",
    "    bonus_kind=\"exp\", bonus_k=12.0, bonus_p=0.6, bonus_c=20.0,\n",
    ")\n",
    "print(params)"
   ],
   "id": "599d7fb29af995d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band scaled are:\n",
      "{'k_rel': array([0.003 , 0.0003]), 'band_floor_phys': array([0.006, 0.07 ]), 'band_floor_scaled': array([0.02706937, 0.08956707]), 'Q_diag': array([518.,  90.]), 'R_diag': array([90., 90.]), 'tau_frac': 0.7, 'gamma_out': 0.5, 'gamma_in': 0.5, 'beta': 7.0, 'gate': 'geom', 'lam_in': 1.0, 'bonus_kind': 'exp', 'bonus_k': 12.0, 'bonus_p': 0.6, 'bonus_c': 20.0}\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:46.416656Z",
     "start_time": "2026-01-07T02:27:46.414212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nominal_qs = 459\n",
    "nominal_qi = 108\n",
    "nominal_hA = 1.05e6\n",
    "qi_change = 0.95\n",
    "qs_change = 1.05\n",
    "ha_change = 0.92"
   ],
   "id": "829372b3f310f055",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:27:46.828158Z",
     "start_time": "2026-01-07T02:27:46.821220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rl_train(system, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 test_cycle,\n",
    "                 nominal_qi, nominal_qs, nominal_ha,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"disturb\"):\n",
    "\n",
    "    # --- setpoints generation ---\n",
    "    y_sp, nFE, sub_episodes_changes_dict, time_in_sub_episodes, test_train_dict, WARM_START, qi, qs, ha = \\\n",
    "        generate_setpoints_training_rl_gradually(\n",
    "            y_sp_scenario, n_tests, set_points_len, warm_start, test_cycle,\n",
    "            nominal_qi, nominal_qs, nominal_ha,\n",
    "            qi_change, qs_change, ha_change\n",
    "        )\n",
    "\n",
    "    # inputs and outputs of the system dimensions\n",
    "    n_inputs = B_aug.shape[1]\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    n_states = A_aug.shape[0]\n",
    "\n",
    "    # Scaled steady states inputs and outputs\n",
    "    ss_scaled_inputs = apply_min_max(steady_states[\"ss_inputs\"], data_min[:n_inputs], data_max[:n_inputs])\n",
    "    y_ss_scaled = apply_min_max(steady_states[\"y_ss\"], data_min[n_inputs:], data_max[n_inputs:])\n",
    "    u_min, u_max = min_max_dict[\"u_min\"], min_max_dict[\"u_max\"]\n",
    "\n",
    "    y_system = np.zeros((nFE + 1, n_outputs))\n",
    "    y_system[0, :] = system.current_output\n",
    "    u_rl = np.zeros((nFE, n_inputs))\n",
    "    yhat = np.zeros((n_outputs, nFE))\n",
    "    xhatdhat = np.zeros((n_states, nFE + 1))\n",
    "    # xhatdhat[:, 0] = np.random.uniform(low=min_max_dict[\"x_min\"], high=min_max_dict[\"x_max\"])\n",
    "    rewards = np.zeros(nFE)\n",
    "    avg_rewards = []\n",
    "\n",
    "    delta_y_storage = []\n",
    "\n",
    "    # ----- helper ------\n",
    "    def map_to_bounds(a, low, high):\n",
    "        return low + ((a + 1.0) / 2.0) * (high - low)\n",
    "\n",
    "    test = False\n",
    "\n",
    "    for i in range(nFE):\n",
    "        # train/test phase\n",
    "        if i in test_train_dict:\n",
    "            test = test_train_dict[i]\n",
    "\n",
    "        # Current scaled input & deviation\n",
    "        scaled_current_input = apply_min_max(system.current_input, data_min[:n_inputs], data_max[:n_inputs])\n",
    "        scaled_current_input_dev = scaled_current_input - ss_scaled_inputs\n",
    "\n",
    "        # ---- RL state (scaled) ----\n",
    "        current_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i], y_sp[i, :], scaled_current_input_dev)\n",
    "\n",
    "        # ---- TD3 action ----\n",
    "        if not test:\n",
    "            action = agent.take_action(current_rl_state, explore=(not test))\n",
    "        else:\n",
    "            action = agent.act_eval(current_rl_state)\n",
    "        # Map to bounds\n",
    "        u_scaled = map_to_bounds(action, u_min, u_max)\n",
    "\n",
    "        # scale & step plant\n",
    "        u_rl[i, :] = u_scaled + ss_scaled_inputs\n",
    "        u_plant = reverse_min_max(u_rl[i, :], data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "        # delta u cost variables\n",
    "        delta_u = u_rl[i, :] - scaled_current_input\n",
    "\n",
    "        # Apply to plant and step\n",
    "        system.current_input = u_plant\n",
    "        system.step()\n",
    "        if mode == \"disturb\":\n",
    "            # disturbances\n",
    "            system.hA = ha[i]\n",
    "            system.Qs = qs[i]\n",
    "            system.Qi = qi[i]\n",
    "\n",
    "        # Record output\n",
    "        y_system[i+1, :] = system.current_output\n",
    "\n",
    "        # ----- Observer & model roll -----\n",
    "        y_current_scaled = apply_min_max(y_system[i+1, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "        y_prev_scaled = apply_min_max(y_system[i, :], data_min[n_inputs:], data_max[n_inputs:]) - y_ss_scaled\n",
    "\n",
    "        # Calculate Delta y in deviation form\n",
    "        delta_y = y_current_scaled - y_sp[i, :]\n",
    "\n",
    "        # Calculate the next state in deviation form\n",
    "        yhat[:, i] = np.dot(MPC_obj.C, xhatdhat[:, i])\n",
    "        xhatdhat[:, i+1] = np.dot(MPC_obj.A, xhatdhat[:, i]) + np.dot(MPC_obj.B, (u_rl[i, :] - ss_scaled_inputs)) + np.dot(L, (y_prev_scaled - yhat[:, i])).T\n",
    "\n",
    "        # y_sp in physical band\n",
    "        y_sp_phys = reverse_min_max(y_sp[i, :] + y_ss_scaled, data_min[n_inputs:], data_max[n_inputs:])\n",
    "\n",
    "        # Reward Calculation\n",
    "        reward = reward_fn(delta_y, delta_u, y_sp_phys)\n",
    "\n",
    "        # Record rewards and delta_y\n",
    "        rewards[i] = reward * 0.01\n",
    "        delta_y_storage.append(np.abs(delta_y))\n",
    "\n",
    "        # ----- Next state for TD3 -----\n",
    "        next_u_dev = u_rl[i, :] - ss_scaled_inputs\n",
    "        next_rl_state = apply_rl_scaled(min_max_dict, xhatdhat[:, i+1], y_sp[i, :], next_u_dev)\n",
    "\n",
    "        # Episode boundary (treat each setpoint block as an episode end)\n",
    "        # done = 1.0 if (i + 1) % boundary == 0 else 0.0\n",
    "        done = 0.0\n",
    "\n",
    "        # Buffer + train (skip if in test phase)\n",
    "        if not test:\n",
    "            agent.push(current_rl_state,\n",
    "                       action.astype(np.float32),\n",
    "                       float(reward),\n",
    "                       next_rl_state,\n",
    "                       float(done))\n",
    "            if i >= WARM_START:\n",
    "                _ = agent.train_step()  # returns loss or None\n",
    "\n",
    "        # diagnostics at sub-episode boundary\n",
    "        if i in sub_episodes_changes_dict:\n",
    "            avg_rewards.append(np.mean(rewards[max(0, i - time_in_sub_episodes + 1): i + 1]))\n",
    "            print('Sub_Episode:', sub_episodes_changes_dict[i], '| avg. reward:', avg_rewards[-1])\n",
    "            if hasattr(agent, \"_expl_sigma\"):\n",
    "                print('Exploration noise:', agent._expl_sigma)\n",
    "\n",
    "    # unscale to plant units for plotting\n",
    "    u_rl = reverse_min_max(u_rl, data_min[:n_inputs], data_max[:n_inputs])\n",
    "\n",
    "    return y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha"
   ],
   "id": "c0b20eb3706f0e6e",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:48:44.499Z",
     "start_time": "2026-01-07T02:27:47.299316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "    = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                 steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                 L, data_min, data_max, warm_start,\n",
    "                 TEST_CYCLE,\n",
    "                 nominal_qi, nominal_qs, nominal_hA,\n",
    "                 qi_change, qs_change, ha_change,\n",
    "                 reward_fn, mode=\"nominal\")"
   ],
   "id": "eb10ecbf5180672d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_Episode: 1 | avg. reward: -19.19575890913601\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.062771369703952\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.30397748062933\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.39185043849061\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.028003561880112\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.915260782907495\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.50976340458148\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.814389234774485\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.60950024509586\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.04995959142853\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.334067261220326\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.28894534848662\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.466078373812437\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.387789648685356\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.366440109033583\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -26.1925490170738\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -2.9873168616474444\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.5530690636039144\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.3811303985869343\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.3589011120479575\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.278939159837328\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9320990866575414\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9114179571698893\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8210353970136355\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8117492620918114\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8026353839525369\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8124254857499427\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.7965358755511573\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8004978836863381\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.7805261284898635\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.77469200231368\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.748236187198425\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7873109964239418\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7853137504343015\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.77079850016777\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7527542116923736\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.796062819733436\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7313123984173047\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7773940478475865\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7761192725930715\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7576869370558221\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.794870885685508\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7401595975145148\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.777374373815406\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.76955506918277\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7340706396865415\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.8004494056548959\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.771403366076963\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.753714232317436\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7884561828421313\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7575561800346549\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8034927929333515\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7726407129342687\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7629815204993735\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7935329668731201\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8050954533208694\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8104844502481774\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7877288498765536\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7809998115843046\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.780146107948571\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7722953004352493\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7596128823663082\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.762528837702751\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7749966947578566\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7289494590232437\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7923113242531488\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8033280533049043\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8073027103069093\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.8036360000858986\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.75776151270823\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8063338794756825\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8174218255238728\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7389618031631557\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.726641116504046\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7636116743163612\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7301643806106222\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7229736555543684\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.777740549133946\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8077904274026793\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.737071896464737\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7219470557975667\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.736516092185974\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7289117299587695\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7290301296889448\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7314562681010914\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7746775092305376\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7556368632132333\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7303160088102791\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7413001710024927\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7503555377206368\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7447754078153093\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7301502246538218\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7362452659170213\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7263406557808736\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7446814518068805\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.717751443821195\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7246815254924477\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.773558415334945\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.759531667575477\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7389291949880872\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7070154247269154\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.718645896589765\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.6863592176891198\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7253193792978743\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7062130817696248\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.6797071133314057\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.6738990866744223\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7109163586950011\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.6929672697630962\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.713553045312942\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.6749488870026987\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7026958392785059\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7053897179227056\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.6956372389504526\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7079560568740022\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.6717845271579386\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6952560882571412\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.70956690530225\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.6815458371810286\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.696706802333906\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.6938993959958384\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.6854625455965833\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.675886171056063\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.6742005366114023\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6831255245595602\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7028522363518197\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6906667083239204\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.710719504865725\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.6913556214607937\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.6924692209257888\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.704499957088068\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7062261171711037\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7193589905183198\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.6970691794237844\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7072575995207193\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7229546786482433\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7217879429903338\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7489466724120166\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.71592672578593\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7191133374724046\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7453993831745778\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7615841813005733\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7204261938615855\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7089865105692805\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.6923444161748022\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.716964255337145\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7195616547283117\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7119366887613034\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7588883849046686\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7053830691083263\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7123734424294212\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7353578982551654\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7010121050460105\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.6974880230609302\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7181786217288972\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7671572068838504\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.719886014202159\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.742723282162096\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7034811903023461\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7400636201339652\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7000580610370701\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7086784332290648\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7071778507162207\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7123135253097144\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7175224646443474\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7129774601106604\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7369107955632563\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7293700606723055\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7074563167240866\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7017076861176943\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.720803919983835\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6790991866835498\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6944393158133984\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7017399605738133\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.708398785688106\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.696421591091232\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7105455862191896\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7018960363807876\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.714306172765624\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7067009461751397\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.714303847638089\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.689415193806886\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7230514093530835\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.705582414298664\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7137775398842359\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6985167435176212\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7038937447365072\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7260205954658097\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7358612923272556\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.713561567430367\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.732357410322528\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7378705050500551\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7080724310531787\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6950897724590102\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7055582182221372\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7311033865153547\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7025902938615474\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.6875069554001192\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.6971141581593612\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6629883767675921\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:50:03.036616Z",
     "start_time": "2026-01-07T02:50:03.017124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_mpc, u_mpc, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=None, prefix_name=\"agent_result\",\n",
    "    agent=None,\n",
    "    delta_y_storage=None,\n",
    "    rewards=None,\n",
    "    dist=None,\n",
    "    start_plot_idx=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation-style plotting (same colors/fonts/no legends).\n",
    "    Saves all figures + input_data.pkl to directory/prefix_name/<timestamp>.\n",
    "    Handles:\n",
    "      dist=None\n",
    "      dist=1D array\n",
    "      dist=dict with keys {\"qi\",\"qs\",\"ha\"}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    from utils.helpers import apply_min_max, reverse_min_max\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(directory, prefix_name, timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(name):\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, name), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    y_sp_original = np.array(y_sp, copy=True)\n",
    "\n",
    "    actor_losses = getattr(agent, \"actor_losses\", None) if agent is not None else None\n",
    "    critic_losses = getattr(agent, \"critic_losses\", None) if agent is not None else None\n",
    "    dy_arr = np.array(delta_y_storage) if delta_y_storage is not None else None\n",
    "    rewards_arr = np.array(rewards) if rewards is not None else None\n",
    "\n",
    "    input_data = {\n",
    "        \"y_sp\": y_sp_original,\n",
    "        \"steady_states\": steady_states,\n",
    "        \"nFE\": nFE,\n",
    "        \"delta_t\": delta_t,\n",
    "        \"time_in_sub_episodes\": time_in_sub_episodes,\n",
    "        \"y_mpc\": y_mpc,\n",
    "        \"u_mpc\": u_mpc,\n",
    "        \"avg_rewards\": avg_rewards,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"warm_start_plot\": warm_start_plot,\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "        \"delta_y_storage\": dy_arr,\n",
    "        \"rewards\": rewards_arr,\n",
    "        \"dist\": dist,\n",
    "        \"start_plot_idx\": start_plot_idx\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"input_data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(input_data, f)\n",
    "\n",
    "    # Canceling the deviation form (same logic)\n",
    "    y_ss = apply_min_max(steady_states[\"y_ss\"], data_min[2:], data_max[2:])\n",
    "    y_sp = (y_sp + y_ss)\n",
    "    y_sp = (reverse_min_max(y_sp, data_min[2:], data_max[2:])).T  # (n_out, nFE)\n",
    "\n",
    "    # Distillation-style rcParams (no bold globals; bold comes from \\mathbf in labels)\n",
    "    mpl.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"grid.alpha\": 0.35,\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    # Colors exactly like distillation code\n",
    "    C_QC = \"tab:green\"\n",
    "    C_QM = \"tab:orange\"\n",
    "    C_RW = \"tab:purple\"\n",
    "\n",
    "    time_plot = np.linspace(0, nFE * delta_t, nFE + 1)\n",
    "    warm_start_plot = np.atleast_1d(warm_start_plot) * delta_t\n",
    "    ws_end = float(warm_start_plot.max()) if warm_start_plot.size > 0 else 0.0\n",
    "\n",
    "    time_plot_hour = np.linspace(0, time_in_sub_episodes * delta_t, time_in_sub_episodes + 1)\n",
    "\n",
    "    # -------- Plot 1: outputs (full) --------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 0], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[0, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{\\eta}$ (L/g)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot[start_plot_idx:], y_mpc[start_plot_idx:, 1], \"b-\", lw=2, zorder=2)\n",
    "    ax.step(time_plot[start_plot_idx:-1], y_sp[1, start_plot_idx:], \"r--\", lw=2, where=\"post\", zorder=3)\n",
    "    for t_ws in warm_start_plot:\n",
    "        ax.axvline(float(t_ws), color=\"k\", linestyle=\"--\", linewidth=1.2, zorder=1)\n",
    "    if ws_end > 0.0:\n",
    "        ax.axvspan(0.0, ws_end, facecolor=\"0.9\", alpha=0.6, zorder=0)\n",
    "    ax.set_ylabel(r\"$\\mathbf{T}$ (K)\", fontsize=18)\n",
    "    ax.set_xlabel(r\"$\\mathbf{Time}$ (hour)\", fontsize=18)\n",
    "    ax.set_xlim(0, time_plot[-1])\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(\"%d\"))\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.tick_params(axis=\"both\", labelsize=16)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95, bottom=0.12)\n",
    "    _savefig(\"fig_rl_outputs_full.png\")\n",
    "\n",
    "    # -------- last window --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[0, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_hour, y_mpc[nFE - time_in_sub_episodes:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_hour[:-1], y_sp[1, nFE - time_in_sub_episodes:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{time_in_sub_episodes}.png\")\n",
    "\n",
    "    # -------- last 4x window --------\n",
    "    W4 = 4 * time_in_sub_episodes\n",
    "    time_plot_4w = np.linspace(0, W4 * delta_t, W4 + 1)\n",
    "\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 0], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[0, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$\\eta$ (L/g)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.plot(time_plot_4w, y_mpc[nFE - W4:, 1], \"-\", lw=2.2, color=\"b\", zorder=2)\n",
    "    ax.step(time_plot_4w[:-1], y_sp[1, nFE - W4:], where=\"post\",\n",
    "            linestyle=\"--\", lw=2.2, color=\"r\", alpha=0.95, zorder=3)\n",
    "    ax.set_ylabel(r\"$T$ (K)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(f\"fig_rl_outputs_last{W4}.png\")\n",
    "\n",
    "    # -------- Plot 2: inputs --------\n",
    "    plt.figure(figsize=(7.6, 5.2))\n",
    "\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 0], where=\"post\", lw=2.2, color=C_QC, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_c$ (L/h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    ax.step(time_plot[:-1], u_mpc[:, 1], where=\"post\", lw=2.2, color=C_QM, zorder=2)\n",
    "    ax.set_ylabel(r\"$Q_m$ (L/h)\")\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.gcf().subplots_adjust(right=0.95)\n",
    "    _savefig(\"fig_rl_inputs_full.png\")\n",
    "\n",
    "    # -------- Plot 3: reward per episode --------\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    xep = np.arange(1, len(avg_rewards) + 1)\n",
    "    plt.plot(xep, avg_rewards, \"o-\", lw=2.2, color=C_RW, zorder=2)\n",
    "    plt.ylabel(\"Avg. Reward\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    _savefig(\"fig_rl_rewards.png\")\n",
    "\n",
    "    # -------- optional losses --------\n",
    "    if actor_losses is not None and len(actor_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(actor_losses, lw=1.8, color=\"tab:blue\")\n",
    "        plt.ylabel(\"Actor Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_actor.png\")\n",
    "\n",
    "    if critic_losses is not None and len(critic_losses) > 0:\n",
    "        plt.figure(figsize=(7.2, 4.2))\n",
    "        plt.plot(critic_losses, lw=1.8, color=\"tab:orange\")\n",
    "        plt.ylabel(\"Critic Loss\")\n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"loss_critic.png\")\n",
    "\n",
    "    # -------- optional delta_y windows (no legend) --------\n",
    "    if dy_arr is not None and dy_arr.ndim == 2 and dy_arr.shape[1] >= 2:\n",
    "        n = dy_arr.shape[0]\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w = dy_arr[i0:n]\n",
    "        if len(w) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w[:, 0], c=\"r\")\n",
    "            plt.plot(w[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_last300.png\")\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w2 = dy_arr[j0:j1]\n",
    "        if len(w2) > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.plot(w2[:, 0], c=\"r\")\n",
    "            plt.plot(w2[:, 1], c=\"b\")\n",
    "            plt.ylabel(r\"$\\Delta y$\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"delta_y_700_400.png\")\n",
    "\n",
    "    # -------- optional per-step rewards (no legend) --------\n",
    "    if rewards_arr is not None and rewards_arr.ndim == 1 and rewards_arr.size > 0:\n",
    "        n = rewards_arr.size\n",
    "\n",
    "        j0 = max(0, n - 700)\n",
    "        j1 = max(0, n - 400)\n",
    "        w = rewards_arr[j0:j1]\n",
    "        if w.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w.size), w, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_700_400.png\")\n",
    "\n",
    "        i0 = max(0, n - 300)\n",
    "        w2 = rewards_arr[i0:n]\n",
    "        if w2.size > 0:\n",
    "            plt.figure(figsize=(7.6, 4.2))\n",
    "            plt.scatter(range(w2.size), w2, s=10)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"rewards_last300.png\")\n",
    "\n",
    "        plt.figure(figsize=(7.6, 4.2))\n",
    "        plt.scatter(range(rewards_arr.size), rewards_arr, s=10)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        _savefig(\"rewards_all.png\")\n",
    "\n",
    "    # -------- disturbance (no legend) --------\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, dict) and all(k in dist for k in [\"qi\", \"qs\", \"ha\"]):\n",
    "            qi_arr = np.asarray(dist[\"qi\"]).squeeze()\n",
    "            qs_arr = np.asarray(dist[\"qs\"]).squeeze()\n",
    "            ha_arr = np.asarray(dist[\"ha\"]).squeeze()\n",
    "            n_al = min(nFE, qi_arr.shape[0], qs_arr.shape[0], ha_arr.shape[0])\n",
    "\n",
    "            def _dist_fig(t, q1, q2, hA, suffix):\n",
    "                plt.figure(figsize=(7.6, 6.2))\n",
    "\n",
    "                ax = plt.subplot(3, 1, 1)\n",
    "                ax.plot(t, q1, \"-\", lw=2, color=\"tab:blue\")\n",
    "                ax.set_ylabel(r\"$Q_i$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 2)\n",
    "                ax.plot(t, q2, \"-\", lw=2, color=\"tab:orange\")\n",
    "                ax.set_ylabel(r\"$Q_s$ (L/h)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                ax = plt.subplot(3, 1, 3)\n",
    "                ax.plot(t, hA, \"-\", lw=2, color=\"tab:green\")\n",
    "                ax.set_xlabel(\"Time (h)\")\n",
    "                ax.set_ylabel(r\"$h_a$ (J/Kh)\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "                ax.xaxis.set_minor_locator(mtick.AutoMinorLocator(2))\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "                plt.gcf().subplots_adjust(right=0.95, hspace=0.25)\n",
    "                _savefig(f\"fig_disturbances_{suffix}.png\")\n",
    "\n",
    "            _dist_fig(time_plot[:n_al], qi_arr[:n_al], qs_arr[:n_al], ha_arr[:n_al], suffix=\"full\")\n",
    "\n",
    "            if time_in_sub_episodes > 0:\n",
    "                W = min(time_in_sub_episodes, n_al)\n",
    "                t_lastW = np.linspace(0, W * delta_t, W, endpoint=False)\n",
    "                _dist_fig(\n",
    "                    t_lastW,\n",
    "                    qi_arr[n_al - W:n_al],\n",
    "                    qs_arr[n_al - W:n_al],\n",
    "                    ha_arr[n_al - W:n_al],\n",
    "                    suffix=f\"last{W}\"\n",
    "                )\n",
    "        else:\n",
    "            dist_arr = np.asarray(dist).squeeze()\n",
    "            n_al = min(nFE, dist_arr.shape[0])\n",
    "            plt.figure(figsize=(7.2, 4.2))\n",
    "            plt.plot(time_plot[start_plot_idx:n_al], dist_arr[start_plot_idx:n_al], lw=1.8, color=\"tab:blue\")\n",
    "            plt.ylabel(\"Disturbance\")\n",
    "            plt.xlabel(\"Time (h)\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "            _savefig(\"disturbance.png\")\n",
    "\n",
    "    return out_dir"
   ],
   "id": "f7a5be6d8440239a",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T02:50:07.083182Z",
     "start_time": "2026-01-07T02:50:03.563883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = plot_rl_results_disturbance(\n",
    "    y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "    y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "    directory=dir_path, prefix_name=\"polymer_nominal\",\n",
    "    agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "    dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    ")"
   ],
   "id": "9d08421eb40e999f",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:17:22.536806Z",
     "start_time": "2026-01-07T04:53:00.379491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    set_points_number = int(C_aug.shape[0])\n",
    "    inputs_number = int(B_aug.shape[1])\n",
    "    STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "    ACTION_DIM = int(B_aug.shape[1])\n",
    "    n_outputs = C_aug.shape[0]\n",
    "    ACTOR_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    CRITIC_LAYER_SIZES = [512, 512, 512, 512, 512]\n",
    "    BUFFER_CAPACITY = 40000\n",
    "    ACTOR_LR = 5e-5\n",
    "    CRITIC_LR = 5e-4\n",
    "    SMOOTHING_STD = 0.005\n",
    "    NOISE_CLIP = 0.01\n",
    "    # EXPLORATION_NOISE_STD = 0.01\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.005  # 0.01\n",
    "    MAX_ACTION = 1\n",
    "    POLICY_DELAY = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 256\n",
    "    STD_START = 0.02\n",
    "    STD_END = 0.001\n",
    "    STD_DECAY_RATE = 0.99992\n",
    "    STD_DECAY_MODE = \"exp\"\n",
    "    td3_agent = TD3Agent(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        actor_hidden=ACTOR_LAYER_SIZES,\n",
    "        critic_hidden=CRITIC_LAYER_SIZES,\n",
    "        gamma=GAMMA,\n",
    "        actor_lr=ACTOR_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        policy_delay=POLICY_DELAY,\n",
    "        target_policy_smoothing_noise_std=SMOOTHING_STD,\n",
    "        noise_clip=NOISE_CLIP,\n",
    "        max_action=MAX_ACTION,\n",
    "        tau=TAU,\n",
    "        std_start=STD_START,\n",
    "        std_end=STD_END,\n",
    "        std_decay_rate=STD_DECAY_RATE,\n",
    "        std_decay_mode=STD_DECAY_MODE,\n",
    "        buffer_size=BUFFER_CAPACITY,\n",
    "        device=DEVICE,\n",
    "        actor_freeze=ACTOR_FREEZE,\n",
    "    )\n",
    "    agent_path = r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\"\n",
    "    td3_agent.load(agent_path)\n",
    "    cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "    y_system, u_rl, avg_rewards, rewards, xhatdhat, nFE, time_in_sub_episodes, y_sp, yhat, delta_y_storage, qi, qs, ha\\\n",
    "        = run_rl_train(cstr, y_sp_scenario, n_tests, set_points_len,\n",
    "                     steady_states, min_max_dict, td3_agent, MPC_obj,\n",
    "                     L, data_min, data_max, warm_start,\n",
    "                     TEST_CYCLE,\n",
    "                     nominal_qi, nominal_qs, nominal_hA,\n",
    "                     qi_change, qs_change, ha_change,\n",
    "                     reward_fn, mode=\"nominal\")\n",
    "    out_dir = plot_rl_results_disturbance(\n",
    "        y_sp, steady_states, nFE, delta_t, time_in_sub_episodes,\n",
    "        y_system, u_rl, avg_rewards, data_min, data_max, warm_start_plot,\n",
    "        directory=dir_path, prefix_name=\"polymer_nominal\",\n",
    "        agent=td3_agent, delta_y_storage=delta_y_storage, rewards=rewards,\n",
    "        dist={\"qi\": qi, \"qs\": qs, \"ha\": ha}\n",
    "    )"
   ],
   "id": "fdb4470b25c16864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -19.7342108540215\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.69333744966246\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.900155772676936\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -25.177833162362713\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -26.422498594726918\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -21.81236507875138\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.852802257494112\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -23.286301737469284\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.80688874952652\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.3658461875807\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.089257500477792\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.285400489689003\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.895412958631432\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.447101347376318\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.28855109793324\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -3893.028205673223\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -1039.811390746782\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -6.044114551951826\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -5.827476389160053\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -6.047445084241004\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -4.246980835633294\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.9611263745793837\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.826093603181897\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.778053954266798\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.283771840878616\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.190633810729574\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.5395497265176705\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.580360242363871\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.667184563398365\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -3.0982966936966236\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.293547597625232\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.5712630780168797\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.211097291114626\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.2426528714984757\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.9291298977655789\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.9757918309958296\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.0730907713816022\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.073881390144296\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.925835939263585\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.9864858442118896\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.9848915097961504\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.197244424224928\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.196303536686938\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.1712030193909677\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.0220986911708976\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.112148043031473\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.971471167765868\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.9032855232123271\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.9059511243028953\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.81110318126648\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.798040891380208\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.92544850866551\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.8457122869476728\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.9011340027716812\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.8312220348935346\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.797609034728615\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8571718692191883\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8242246287925774\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8472978107340714\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.8789459605003012\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -3.45050055143306\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.8362791961372815\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8862768218010613\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.8688438653070494\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8755920182223624\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.9812544495014737\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.9850654558228553\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8969446171764566\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.011181203359757\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8550344245012622\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.867873182666281\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.9191908613812303\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8704669739176063\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.901346646606632\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.9430005855094794\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.9268086036673697\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.011133628070987\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.024467297329294\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.9061120110216205\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.007772153057174\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.8529729048011832\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.8102516637925354\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7850190060296054\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7852194628099785\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.759480161530628\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7841704574761377\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7637058867400948\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7872248443449033\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7393299456218378\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7534809114431786\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.748531677247775\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.772313886693882\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.8049427829992895\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7772129076507819\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.87403515976003\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7770860308910859\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7839109115966805\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7946143871915876\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8059036895551208\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.8015808267705018\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.8018119678758222\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7701563632108976\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7461673944250924\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7761169771775611\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7227641515413987\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7394199811268094\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7163121914044268\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.6996521894131815\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7147886591212889\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7344560173202237\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7083185692661766\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7446339582895438\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7286833173124878\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7388365358169693\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7417510916471952\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7190609843554585\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7123977580265126\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7264059769183222\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.713639291342277\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7170719042402771\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.712992474002665\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7252149120721685\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7627025480518936\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7572632027777542\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7307655163928644\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7059879309566912\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7175482734261407\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7777578208783864\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.6861794618147798\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7185294414712533\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7050948443047105\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7167871443021463\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7069051183308162\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7291678794591008\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7393681802986884\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7058629082772705\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7065254304458255\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.6990620402673051\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7069392532107015\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7134069639826992\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7161444999402216\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.72533648454412\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7662898219643552\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.8015008377152002\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7779805453654143\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.8296616510436763\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.8132837611072485\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7648732205009026\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7496820220274218\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7222510440512684\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7215965562577136\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7130441323190668\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7028680450819058\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.692441145375446\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.724174187484985\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6999473232053885\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6974652015166993\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.700839408996554\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.6995604789514172\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.714397417712047\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7015483195603724\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.699900815669903\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.6955605466048234\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.728150550327317\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.6970233725796606\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7071660803317428\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7205108577295294\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7332727754551969\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.713093173742753\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7284712748847268\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7177292441903926\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6893719154324156\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7050418539486096\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.6834232253216193\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.694550794920899\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7247192618970741\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.6992380706548522\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.709350312672942\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6909935761274977\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.669322877091641\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6917398227746805\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.6887728706507528\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7020981512100815\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.6742061732486704\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7068685157272292\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6906988142333796\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.68397522768785\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7249786405660235\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7119325219708275\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.8461350573573585\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7250650254633393\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7040038914898015\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7271251421062448\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7202303697616554\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.715730636079425\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.6964401302775856\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7066731289911914\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.691702024879798\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.693688508138182\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6862844309620413\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -25.581251531357903\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.23022900091968\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -22.623464736989654\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -25.597979112237407\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -25.08011547915854\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -22.504393676046313\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.829564876365247\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -26.87970533179306\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.154091519233003\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.05413179082536\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.372618387014835\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.13589835210947\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -21.349315244700264\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.60129444142259\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.38592394913316\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -26.168510738115454\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.580366560131016\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.48552252084608\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.7072316943638643\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.1329833138649024\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.960477239949612\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9616543006952627\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9728612756552253\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8773540098621613\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.9271136357689154\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8892693431427876\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.826650490106351\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8680982490153513\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8349704467415568\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8324985758106578\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7405630247085693\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7796004822324045\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7610178700177663\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7882813700547553\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7840011919994674\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.9079265789598019\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7750618105868095\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7880200882517778\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.804252375905496\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.8523952563497232\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7883316657039106\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7960091438327652\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.8354731102504078\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.8729657574819765\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7390984550841408\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7725397080842118\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7456436648348557\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7316946602421803\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7508906445742003\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7410732791945958\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7717576370978794\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7423310959837903\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.744056761452161\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7164074646553633\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7073955097495166\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7619914139926567\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7068124311497201\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7793893092502078\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.727346098383742\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7668874884751862\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7531872165535025\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7363083540698825\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7571404549283003\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.743244629925678\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.716055554949946\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7293349143009076\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7018577151777206\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.73808093721876\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7009032308410774\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.721275514006611\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7437342212533167\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.706603639151033\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.716690628850005\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7344642825593217\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7346037060108086\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7138969517850882\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7444238895200364\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7396745021837554\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.707876798131952\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7574366416781952\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7553671995556868\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.750447027731038\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.724685528979128\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7274789686988026\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7173267121697353\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7402153340667257\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.713340673706428\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7240581272247246\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7501387228349052\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7501154373397856\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7481712791583788\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7407736077987517\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.746929952241301\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.705394071427067\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7343179383148415\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.721770161158544\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7742547875812602\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7431781015422887\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7323299518189788\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7440447203922433\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.729278418628826\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7100915124245009\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.73183518995224\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7180127272759937\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.746707286801493\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7073019238713245\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7060371300079475\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7153226177252032\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.748774638907518\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.733731708062403\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7174909420361224\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7158935452204473\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.722156160258999\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7195695722232838\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7025209617941532\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7038335491714351\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7087800904482264\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7192411601786841\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7140235575155658\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7442352227800613\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7995508257352695\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7123000510048982\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.716328882305306\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7153205573003873\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.6990522306843672\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7021205746647943\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.6829136521020354\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7059579997608691\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7200847933377494\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7018543040404188\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7340567417183346\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.6954390605636847\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7021932826560917\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7134699874160373\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.6947370646673292\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7270796096366374\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.6844257355869439\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.6972590930623386\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.6838336906012723\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7031424111097833\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.700120285878653\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.717438714578592\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7404256067640227\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7065732652167114\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7166899047408593\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7184313634876325\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7137201263812443\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.72065250750372\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.702163172569995\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7097263515216907\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7068944650999363\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7245700951747132\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7393280123067212\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7675086707205563\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7711860998801179\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.848849599731758\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.9114072131139113\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.736648326121538\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.795403258138874\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7554130386664195\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7383604092312595\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7483626493586217\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7276395340828032\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7201285584248138\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7367222312823298\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7392922152687078\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7350346905254566\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -2.571981932068037\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7254366051459165\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7148423952350962\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7297668254179024\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.727744302908046\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7188639197166118\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.745820379208232\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7423549244748895\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.770243480550031\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.740215797586596\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.8139767116683725\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7590802317150933\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7271206186092414\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.715701268133416\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7206497875716469\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7134295353421443\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7135341127760513\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.698738840637809\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.702830534312309\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7097278648991159\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7219605189301452\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7262855166853273\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7141160006642042\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7040579647440735\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.710677693606164\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7039439793856161\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.706442953513739\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7068433226765205\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.723235097275194\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7131745192344183\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7304273733827409\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7327513704665336\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7064300144598095\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -25.1026342739559\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -24.631090677078927\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.22452095483646\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.02915121422552\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -21.895716526559298\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.273292375746227\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.655068292160095\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.985334563958414\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -25.47051269909712\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -25.751524650484935\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.610383349215336\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.034841247033455\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.373595433727235\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.557884314146172\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.152835133533554\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -29.93168160315312\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -8.425352330445548\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.347072636087225\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.014732833317889\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -1.9644580098331408\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.9499713328937558\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9783258165275288\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9447002543501497\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8735022675510729\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8707651020746277\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8045854841386129\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8641610598680807\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.8681824728702106\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8772806430421347\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8944214463689695\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8443841243811832\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7651947355413922\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.777176662974797\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7718770548271618\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.775469329414265\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7856397133531317\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7744625937761582\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7596330893061167\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7639855703809377\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7868991669458587\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7517820013506635\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7164680390272549\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.766223103206406\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7840309681646909\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7601383272108486\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.73925664933138\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7476380899785875\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7379984272882922\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.764522752823671\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7512955215485533\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7497499281156796\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7481321555129588\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7698366955873939\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7576261106267923\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7679396273704422\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7028994396020487\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7554589367244846\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7476945854668196\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7568096489889544\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7774203678599207\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7352530179396604\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.75183429312263\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7634470328918221\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.739696845119381\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7297780131994818\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7279914098360754\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7587316845835554\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8176965788847081\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7880259089764394\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7360837855642075\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7809060991858916\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7510298637119834\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.732109833254532\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7318247514060396\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7494338342107263\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7311209763455775\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.718548313073314\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7345103862553812\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.734003546433521\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7319255849144395\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7996939413080337\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7408776055250252\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.715963384624045\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7358700343485542\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7235558110754032\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7406549470377086\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7600251720371518\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7737412733229463\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7492713178936827\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.715453542440169\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.6975338166740703\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.738685862505609\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7265609469761933\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7183195004386749\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.698508183560615\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7368030646677783\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.707474065586906\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7275959039533297\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.743059697693016\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7279410198091751\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.717106997666209\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7065993979877434\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7370510226766243\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7048963536770783\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7132041190086735\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7308293212435217\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.717520867829357\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7305524991097867\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7221431828235665\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7137046441233657\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7307983109959282\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.6856670050694236\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.732043399033242\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.712636138929825\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7071972779473712\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7342803087725318\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6973123648655866\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.8739236624668376\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7503092600958743\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7126510993684325\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7070811930891403\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7238038789246384\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.6971753634865692\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7130009793442829\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7578195881135008\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7236817030699336\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.752393080260913\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7138673874032924\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7026938308798345\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7150684587283604\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7113050938469037\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7150421575055912\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.727231729236754\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7286825124681093\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.711736576892616\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7169592075557512\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7096599804564299\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7066946781432921\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7093316031540722\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7087707558849854\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.743736210207502\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7493354013416962\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.721991574826737\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7372262099470488\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7248946846190274\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7063352594892478\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7348597939967902\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7073470864073095\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7336169214257142\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7241629498435906\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7169871828442842\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7417035119999758\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7058537085089074\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7288664966487954\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7091657741727817\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7085152950025548\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7146663088038503\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7013125784039718\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7013779478444697\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7344099162666202\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6882945296704108\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.72673765620778\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7328243884071584\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7074752628933692\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.736957276887008\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7264409766073276\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7465278658899548\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7090636831710544\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7033866194820302\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6994263618388674\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7152171634421591\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.684578087649553\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6883638790633382\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7033049360595636\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.6958266588798137\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -2.3078094199987915\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7442271293687042\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7329517687613094\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7450973941565433\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7188610656713044\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7494392010586512\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.757874754108965\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7460795102981144\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.704241751867616\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7189148545728472\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6915786972644378\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.7160046505593904\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7440109790392107\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.722209149009322\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.869448862541394\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7246097598672174\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7032182626198271\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7134915423467822\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.695636273819007\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7148688995480388\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.714659777607642\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.711796193781255\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7168441867452344\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7612170237561335\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7112397232153225\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.97021922535284\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.42681135838648\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.515655078468086\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.53711588844586\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.540972853285858\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -19.02019067190968\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.912568267763046\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -20.708027131754456\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.670111214440812\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.3659364398241\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.561131794042304\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -23.79018264443208\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.831003113427823\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -21.61795838763645\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.81493598289667\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -4113.322138284482\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -6421.993361158923\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2877.624073399719\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -836.1395517762853\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -841.5926617237363\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -819.5396420464223\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -907.1181158474808\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1009.0221356276385\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1031.2392571712342\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1030.1341651689104\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -982.419271271996\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -987.6581065191302\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -929.8975736956401\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -918.9143853405901\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -940.2332370604083\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -957.8047883247325\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -926.7430558315315\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -920.1796104213391\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -923.3141628807815\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -607.5858168387431\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -34.922427524906325\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -10.1951762916175\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -6.781323152727111\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -5.5161533542600925\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -6.286006862558833\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -4.118721801596991\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -3.3859792667634396\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -3.053074569905744\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -3.171269945279694\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -3.4741274358324965\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -4.241269411218846\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -3.567864238373659\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -3.556958248638988\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -4.079240752222478\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -3.9762065748762585\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -3.6662615713254185\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -3.3014232669128454\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.8229040043267037\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -3.036590415320306\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -2.9003622551018213\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -2.8161957574731327\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -2.6447220975865533\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -2.7908167535563706\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -2.836884226643534\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -2.658670832076997\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -2.577216983224702\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -2.471251021312573\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -2.368466969074346\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -2.4482685451847805\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -2.3599917332337275\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -2.205047863020612\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -2.368215685389762\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -2.2548130481066924\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -2.321679877425792\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -2.334551403678115\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -2.133731833926467\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -2.198329703720895\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -2.1909489443829484\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -2.066076295554883\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -2.067495685897311\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -2.2137676928420382\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -2.2501948229863964\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -2.162727749438233\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -3.002546157952537\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -2.29142693559566\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -2.375381184480609\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -2.2996208767709536\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -2.568475809603566\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -2.332181275279604\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -2.3977026455180406\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.426290404607101\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -2.4298480016894812\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -2.7652975614063293\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -2.5094478230470703\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -2.244007806339632\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -2.1486899786300593\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -2.1984695322542693\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -2.0688671119353135\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -2.1042580348096362\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -2.019411994622467\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.921957591538382\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.9131132070699073\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -2.0558342194120343\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8904923386030992\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.9523820962022336\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -2.012196222731665\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -2.0498327723418517\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.9290061631051258\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.9443591942579526\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.8931042997821044\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.8939281441348839\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.9579402668418033\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.9055690686624251\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.9454865532318297\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.957131487136765\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.8531378339120252\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.919226411537765\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -2.040726691597445\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.916288877806027\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.9262691234002858\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.9920298632729685\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -2.194437085655604\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -2.6775091905192654\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -2.205356341606032\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -2.2273271859810166\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.979737856962979\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.9647789983523491\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.8453260896863553\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.8716300803975365\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.8024845518342434\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.8731387722420427\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.9477525975025376\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.9020398584274398\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.873200268679502\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.9628516051024716\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.8959731792344838\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.9324440420190867\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.8886715269103402\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.8848616087783636\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.9163419761875906\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.8838646505046064\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.8689275245549697\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.9024172669723165\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.9447881428291856\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.9307368227194734\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.8987344732851557\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.8339619778686176\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.8877091404794504\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -2.1603536368738694\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -2.0613867600389244\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.984212278933731\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.9867164764986875\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.9978139217664597\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -2.0387081124524062\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -2.0517002770398407\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -2.1392712162205156\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -2.1031837664097957\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -2.0358890567034393\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -2.0073797419039985\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -2.0922623797699074\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.9076753883434991\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.8873506949327166\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.8803796159396888\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -2.118053177426661\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.9129215582947785\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -2.038960413349201\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -2.1543028725473814\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -2.185009150884795\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -2.244590582961308\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -2.3109002254891085\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -2.3457076039037306\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -2.4957878854681796\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -2.5882059326431572\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -2.6459846234306212\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -2.741966766990791\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -2.7840974418715154\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -2.9024854547133203\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -3.2521851047172583\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -3.3616854362307254\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -3.640099699654512\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -3.8615778152823825\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -3.924100722540722\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -3.601370931866086\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -2.999787045894477\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -10.632469172682526\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -2.9383422339782794\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -2.988838188547943\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -2.9286886386025026\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -2.9879500264881615\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -3.0571066479414832\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -3.1472238188908777\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -3.211995421512326\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -3.202189602240652\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -3.0389381441246877\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -3.052553110120508\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -2.8894261303309445\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -2.8903142578546963\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -2.938689574985636\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -2.9622351337976354\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -3.1443553356569645\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -3.2842281703047433\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -3.4577560799429166\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -3.457911009921608\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -3.3313970070438175\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -2.974667418299755\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -20.613464890190656\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -27.449493728396348\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -25.71437187530999\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.031009207669367\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -25.340212434952377\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.164148762952355\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -22.617879555055357\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -21.388924238076125\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.979607630431634\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.254512532627132\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -22.990641010572386\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.05447692522637\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.12414603649945\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.199090143159925\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.694746638010823\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -30.0513194875689\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -6055.000092356802\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -5.643930704815118\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -3.179406272631158\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.800616109831725\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -4.458490019023132\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.6331490200614\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -3.637744996909796\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.6096624972739266\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.2324092371424373\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.2911222162960354\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.220353632829324\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.149768629988638\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.0939145661696807\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.019104718122831\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.012439227118037\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.9621264276887773\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.45231894667842\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.0985016871369613\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.9853519905314445\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.0983335375941565\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.111142214929137\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.1200961813752253\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.132892832594952\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.0777283644094298\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.9859730802324436\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.0914821033633264\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.0442300076725033\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.098867830276671\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -2.0128878458986446\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -2.1062841942095094\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.0291345581714144\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.123474155995373\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.9077504667394376\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.8908662855116904\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.8847448779575848\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.9044535292198628\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -2.1323595592537523\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.9446236089939588\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.8876164559754198\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7876337298528509\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.825801190132051\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8767519998169908\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8409712366086532\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.8346655760682078\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.882148238845278\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.8456909326172612\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8456183051471435\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.86155449240253\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7805489395925616\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.8208007811274642\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8322364745904742\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8229199753785792\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7780508522963154\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.80464542867592\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.811346935771976\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7744228028764337\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.742948721209684\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7525106191181272\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.780444663040481\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.777627170807637\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.740799172992385\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.746981194393034\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7372959597950757\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7567430069972425\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7251885662504378\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.8017595143813798\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7657806627869923\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7723120782056754\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7662490760564822\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7645843950220723\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7676933969945\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7670942394558624\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7527864427713944\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.759828688679903\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7471527944711012\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7496663490564341\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7595590249807578\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7495481301249485\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.740329006698465\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7216520122683658\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7271675415247125\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7365939841757017\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7583119193342407\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7854830006656317\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7994712834365014\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7401464090766166\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7695558758134404\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7421371801897874\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7372322210324518\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7336786175612429\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7411740048148008\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7209317642433786\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7287261813226211\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7064711608414518\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7175706974008265\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7021338728222597\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.8107434315654103\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7392457520560396\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.8064099824239213\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.9371857314968088\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7573454976146383\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.768842253726927\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7601074237581338\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.760067052190214\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7956359249228904\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.8036598619437245\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.78545632522862\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7375508209751294\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7770866837949126\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7378635255016126\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7508122592102735\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7539743552364422\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7748109112630168\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7295809044452488\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.706012427060618\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7403725004996276\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7335481419044172\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7642802371808415\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7598721072671382\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.6914653247959766\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7243078114723078\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.738582255565517\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.703086436531745\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.70949297176078\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7271025832874853\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7447873019392057\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7508382381157288\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7113743798873218\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7367061873656366\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7202276522910873\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7192774190484987\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7139438512137648\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7229015543529544\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7149746624000823\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7126529833654514\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7135300984171533\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7149895406212174\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.705449540945155\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7091498728796788\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.6990183288864396\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.699508733131883\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.70199339187317\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7115502989740037\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7177089461856867\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7152417043249966\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7261219495216722\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7662604450972719\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7961927203686587\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7726206302435188\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7330969352060976\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7654293685345135\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7163855477362175\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7062452093959528\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7305518151016874\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7212765836376624\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7384370721693745\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7767823900281194\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7369946729466403\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7311059490635199\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7203859780439814\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.799695781176781\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.8584940770351142\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7757600795818491\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7278824238981332\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -2.021478002755756\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -4.644809213453694\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7746819045684337\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.742977292054372\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7289768972088815\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7073407156245022\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.73649353864168\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7480660995561124\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7287163586108862\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7405360038461737\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7063631874046399\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.73163912388238\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.724222270765366\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7144814416544574\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7517818273235037\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7528296973741908\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.732497379869165\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7467616171042326\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7467767611607856\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6921842988226254\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -21.744551800917428\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -23.1961696455553\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -22.03589567634742\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -23.115945064136987\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.329057054079268\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -25.41142127015191\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.380261657473984\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.660981681981205\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -25.711359424171068\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.303816629375014\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.252582634167187\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.043822807080232\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -23.367748617000792\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -24.522291279585136\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.64752256428127\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -18.811265268219476\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -4.631117899345255\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.9851975232082726\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -3.8438138901448338\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.116714184071928\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -1.877676609003375\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9160793370231437\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.8706810810188896\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8279670448347831\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.896395795661548\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.9276324114827088\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8631175866737495\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.9025676528133122\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.9199081750870277\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8992763577828442\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8375503693142254\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8206049491932992\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.8343962253361097\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.8046794405488964\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.754921629275455\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7690707254710958\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7925226878370668\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7145157820031391\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7544934796768723\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7800092553486746\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.8167712982750552\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7503404758053158\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7435062362546543\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.76529733306307\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7728747606880086\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7595071058062044\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7633394255860748\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.787404858251536\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7385586046414516\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7353528697642633\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.783561490718077\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7757409789347565\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.7206042019360501\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7204408628869385\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7986211912551306\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.738738666809546\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7354159649601206\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.813033313028336\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7105451444963418\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7292762205568157\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.812733178346411\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7314103016746412\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.7671102000687897\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.754461733624178\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7809657568168342\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7313221380737547\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.760353353169055\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7496331893069563\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7270610910218116\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7391571631709826\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.751887348142929\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7497491087436605\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7594091893336141\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7747461005539242\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7543840134314355\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7716667605244083\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7482728669862333\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7419672236882742\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7339057155618465\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7548317281634274\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.79125874346002\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7379778052750654\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8169054981823103\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.6998026305793013\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7362614010960715\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.700876389972678\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7799872896812297\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7139892978105637\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7233690543500257\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7305780162156266\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.719012600322024\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7355627181653546\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.720351840701586\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7283088431722817\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7335909188200713\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7049749995187005\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7031131659680887\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7223434913021054\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8159476259670135\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7221664562675483\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7355076048620328\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7377599803969286\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7206317779526925\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7412305199832476\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7405805232801344\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7184434137501006\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.717672688142659\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7434105758846687\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.6931312989013927\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7203735078776887\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7284016235342579\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.746449737153057\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7264811186768305\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.723448589636547\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.7059415043464445\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.6957341811821884\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7317597619870986\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7224644067776673\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7149571407214426\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.6990089129166825\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7009888073387134\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7193972752648106\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.6902788162613234\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.729157400158552\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7608229905942567\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -2.058792816754042\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7118302504505052\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.6976071073149774\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7007177648903598\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.708108638150033\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7261354688445065\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.71009677493888\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.71900907405399\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.6990745049263345\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7189684213253753\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7217760889860865\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7550590190617927\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7239228869187122\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7546814356347227\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.711939493249402\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7262729402849561\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7334203685442975\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7222881561511532\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7427757667825083\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7625428796948954\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7406978903510906\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.743635058395871\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7880928936527871\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.8372772825394197\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7202115149952981\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6955616062626762\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.686747616303961\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.6992940149989402\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7211755678461724\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7090063815581937\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.687782402369898\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.680779041524877\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6943616061618452\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7063302707928942\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.6980071368122218\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.709312954766582\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.6947306734981549\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7000009243570355\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.8604747985916055\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7149661122695925\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.701423724407988\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.6909266189007486\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6969924674014643\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7051559904113167\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.6900674584437452\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.697562041729065\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.701624967449034\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6804802453293326\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7066327728479274\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7023157221516885\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6976967454364456\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.69626407773403\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6909570561164862\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.6890907587042534\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.6993533110567158\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.6948769501021785\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.68179337787736\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6895106838185123\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.688025653744207\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.688067613459432\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.690936819594797\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.70117592282869\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.6931593591475047\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6967134229907628\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7003325824236928\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.6745483762367344\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6967732302915195\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7029840935857226\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.6899530946395545\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.6902986550104722\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.688182290161334\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.6836224339460824\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7149806945936723\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7022900369057248\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6286141900857984\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -22.513383336889763\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -21.929822707880216\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -24.973452541263754\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.232809867640416\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -23.543583547358203\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.60508276020924\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -24.296013785728118\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.712090908587342\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -19.703283545279955\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.764619952643116\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -24.720658374962714\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.183537271876286\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -24.452825768903963\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -22.57839071261376\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.044623409892292\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -19.425762118371253\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -8.399058922607741\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.6200824829537988\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.2067805156115012\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.1850350254284416\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.004080161793149\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9412204755959215\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9911032697538298\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.936706993831397\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.9261801085228785\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.9355514041182451\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.9333145676400318\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.9812572637453894\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8892641675715205\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.87818221237659\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.862283172556544\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8509999287840575\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7607379127649607\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.8075591179941097\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7567724676147618\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.742480280382087\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.7826451475615261\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7410104948411915\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7281787546064975\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7620023234546927\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7825789704779513\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.757812604540265\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7299501111956277\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7724142160365057\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7279175941070986\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.73738091708533\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7923320868856059\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7393726518522157\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7231506293188885\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.6808313269896253\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7190594928690042\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7116731430546923\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.6990732702710716\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7109004598990603\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7218454394129339\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7232781289598944\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7288217217889013\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.744047881709728\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.6991048260309976\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7413550610015887\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.7597310885105621\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.750352181823883\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.726200421743876\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7658879801580047\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.723784611688552\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7354397069155052\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.7406173302679895\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7011702923552325\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7396752742173953\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.7127778616934954\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.771430660078593\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7364912755094521\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.758209716507746\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7675845997791992\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7578733013100782\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7973677544104734\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7942907016612077\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.8144421063860534\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.8016373109541555\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7511469823345367\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7518789768716585\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.8094345930449032\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.8118459240839229\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7817907372465418\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.741748510751636\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7621986608336453\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7882921817356703\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7541369248872545\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7411096208582626\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7704597096466717\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7632534449473576\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.8160600643797182\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7475864815516524\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7259942661582581\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7482059229575087\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7827417334139186\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7657943680205654\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7481252308669695\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7011904897882544\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7533380190479704\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7487356076914198\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7674264822593744\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.778523068611601\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7581667828531307\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.748341629252539\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.708032882280928\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.701016970601385\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7090688750726293\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7335464786668329\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7068387644590617\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7223439994303762\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7204950025145038\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7185709753956042\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7574876092400213\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.756858947857385\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.8583714227324573\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7066757116992388\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.746609372259169\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7142753828298731\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7133793956068581\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7604584525643514\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.6879126689452557\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.7195930487631483\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.6976780191893304\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.707306466682104\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7002797678597295\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7164110218995339\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7277423765135882\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.686763219321996\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7137644091588635\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7244266975567348\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.708308577407301\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.6772540746620297\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.6927775580579145\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.691311403641435\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7005248154360886\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.725407729698948\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7172293793787552\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7241750057033212\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.725860910959282\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.71025067695752\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.703678439249221\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7186448085890838\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7324075434006818\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7065945884823674\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7008489747328839\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7223153471606145\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6977942445567158\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7181654400026958\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.710281060241513\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7014437821354573\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.689715451981037\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.7077492761338688\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7290536070305285\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7187762267581954\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7115371124470296\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7100929141929675\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7303068870962037\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7231932031323494\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7022547155321666\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7338632321781347\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.714526000534002\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7098417214613084\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.715545777108153\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7058934799272598\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7124803305321346\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7001386549053477\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.714110184399121\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7016893330106955\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7012619520363124\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7155825778538314\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.6980877834125618\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7218653015994057\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7216818983416602\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7111323491944312\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.6939367744221783\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7002773548653598\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.685555171376879\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7049540727758532\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7015373760841075\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.690500998115014\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.6977202334388293\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7039302687172482\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.704615707795669\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7278936793531647\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7110344659756804\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6906997936660324\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7054017087421978\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7140928343475381\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7363068513098114\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -2.4790652736866243\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7398315385094625\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7561319973750074\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7407346316928534\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7157977892285425\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7365222459549903\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7319193620003557\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7425473237550622\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7690695416572253\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7393154335576382\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -23.697076544431123\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.286085648240277\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -23.240646392883047\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.023444439363512\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.812708316938757\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.24553915957081\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -23.702247187594462\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -24.071456439968458\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -23.67173571081027\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -22.593318584355494\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -21.524623578024094\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -24.64691011048053\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.004946216902212\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -23.630213063611436\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -21.59080321267438\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -1682.383043807521\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -244.40642509278564\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -3.8755720319281393\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -59.6184671131071\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.6667541738320724\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.5810345821791447\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.445284196053283\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -2.4973340356549665\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -2.486018511076632\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -2.7658454018545835\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -2.9212605041335906\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -2.618069193222476\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -2.791872695573226\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -2.4904532970971536\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -2.2987114033621285\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -2.140764766163343\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -2.3715382656045625\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -2.1755088069392765\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -2.3289787983049663\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -2.384283677144406\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -2.274013925590849\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -2.205119174713752\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -2.374105174222257\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -2.5066798079011776\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -2.1285513270270995\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -2.093550015133155\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -2.1149438687214643\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -2.0034592693457562\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -2.0107812736178596\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.9877966914145244\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.9985824483358743\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -2.010243171838929\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -2.005675060790487\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -2.0765222163283936\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.9875065584407003\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.924878705380977\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.8816655774553976\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.9324629126573962\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.8728672238026944\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.82924867097135\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.8818828067633313\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.8124955547346873\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8184637061895272\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.8015395299667347\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.8407779000194489\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.8709149719307698\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.9944934286803926\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.8979198625182394\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.8666525556640747\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.8767354402591905\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.857440874691043\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8094605619243456\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.8368679009568947\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.805314608792715\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.8751727944022485\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.8291217784908398\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.8430295512905517\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.8297131317454898\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7860781874583216\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.758819360013489\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7955424669721234\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7615086981188526\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7567873443253494\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7618892755189786\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7548884013564297\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7401840296678899\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.709818484002176\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7697789175303276\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7547094817722577\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7361760315355121\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7810179429855353\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7106325222684182\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7638502670037228\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7606088608820756\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7592238570656034\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7438122531615383\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.7483366739703197\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7218963719760814\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.712263328022682\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7674462130275976\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7343966612239228\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7564359068155142\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7787376232451344\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7767365109400983\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7683354682122587\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7244671481074265\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7274133381643537\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -2.6391098300559825\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.759353849595853\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7320912477259902\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7046165237788058\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7531279914263747\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7606527861076262\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.730683332563855\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7149032909985555\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7378058792679292\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7209475054154852\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7113985962216947\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7678948199571294\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.6972510625396182\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.731175920797413\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.7018524268037314\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7721842875864484\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7511694160878255\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.7146612639443009\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7086446450202106\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7319181970485307\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.728650740924179\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7059057695662363\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7191391651676862\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7361699987722352\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7241257863913448\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7642194692734834\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.721643074271914\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7002637317624134\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7154333335570018\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7282641621929502\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7287827028499698\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.723676940662683\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7274309700972819\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7122347228012753\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7307507177905983\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.7345841977030665\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7239318661642482\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7773655453465358\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7275250273203364\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.707485222364226\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.6920019909017012\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.6785636782579405\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.7092455557821804\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7109271956597722\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7014612004583918\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7189771292041245\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7235262446343027\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7070597816092508\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.706081783159194\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7136365934932984\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.721684841365805\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.700052639490529\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7026846440885242\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.700062784368575\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.6976043627836603\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.723209181470318\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7164686967971603\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7042068881411816\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.6921359845425792\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7155727968835004\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7194412702920416\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.7090510411647892\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.72192447445363\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7110556473052305\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7126497896995836\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7193871859999694\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7136842655817748\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7184221862446094\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7569902403782112\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7226819674859792\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.6946110690372649\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7179426442283476\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7362563203833912\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.728562916225744\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7386150629125479\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7192197012591435\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7114130647325558\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7230538648044489\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.7236487085305743\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7075386266760857\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.7039834741865645\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7271875439069078\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7078714966652258\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.6982924508610022\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.729388571872658\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7738868914179584\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7121878747236872\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.728684978760966\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7275085207143714\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7193677256674436\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7634578595054853\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7575517320504894\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7345274846049383\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7179097027175783\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.742088018515501\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7482618777809535\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -2.3756341749757217\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.7699501146089052\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -23.6293878666389\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -22.95116040586511\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -21.135530355927823\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -26.029637141914908\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -22.823054510488582\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -23.824289553488143\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -21.831832624051323\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -22.933713882794844\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -24.245588115112465\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.082079138463385\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.794474208811287\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -22.644165040555674\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.156038869945434\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -25.34913576569495\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -23.676139725284376\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -26.1497557551374\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.8211015391572674\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.900738302627027\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.9231022021704987\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.9352318167816556\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.6986607818960033\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -2.4620840498249033\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.9846580152118025\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8060115267535548\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.7821619464345089\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.7416069200452626\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.7500647575111943\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.72547035183567\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.7795625009879676\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.767767507542077\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.8003519607717562\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.7587168164486138\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.6850107598153672\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.7144262830418984\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.7548174246016168\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.7343595791144142\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.765923124148161\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.7573424025762734\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.7767087752019872\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.7770262375556505\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.75894890257005\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7728378159240201\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7365306817705328\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7801350144398027\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.76955876012616\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.761080533735168\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.7867697772185205\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.735457891649113\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.766496841791236\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.7439163954779684\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.70398630245136\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7428922942318337\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.69892040592439\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7470232568842499\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.72336334405471\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.7421041201503933\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.702945680191266\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.7255607870182537\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.7352102221638221\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7248628215547643\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.6933831623863107\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7182565836848505\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.766441456244271\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.744137992596115\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7469753196810363\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.759376294917932\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.771032099020843\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.7392934285229875\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.7618303329952176\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.738859789489792\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.758768029906659\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7565266218101356\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7560590732246493\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7964992898438612\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.7821195886171342\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7856735940769741\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7716124157141109\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.7936184132090518\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7651171435268527\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.7233463376525657\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7526151083631418\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7514036758919826\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7672280603176613\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7646837868660727\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7439710165164772\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -1.7689411705728753\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.7194592406461553\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7383785645472636\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7428978005844424\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7031445047146156\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.6951184199076557\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.717358259809313\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7093084995217913\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.737176283476831\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7484147729293995\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7495514924561923\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.7552168797974235\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.7708543651936768\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.7714465508011943\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7646217347800717\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7473254736882162\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.750529418624257\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7296731021456029\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.7630947912942645\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.731191636164403\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7394463067318833\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7174232692485025\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.7159183937834825\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7020285106442652\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7039749517039564\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7232575208704803\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.699725759144162\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.692047737083781\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7136949750375836\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.721682039639336\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.713999054777402\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.6962105872712405\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.6994241183520973\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7232682836135018\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.746684722099979\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7235274547617179\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7255731094111968\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.729545865915006\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7298872818332613\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7106130174029865\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7544207298915668\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.7194126877670994\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.6976692897849506\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7009863005579433\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.693949900498186\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.7236088976606876\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7026808249949\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.7364025786539699\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.7464837786331981\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7696352670268283\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7343052393545038\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7354706706503242\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.708017909430604\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7243524657013625\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7220962310205974\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.717982204089559\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.714832337582092\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7868038928253624\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.9025584460144949\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -7.669888946982788\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.6970670409619644\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7164346363349607\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.6971998805891348\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7325961376996508\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.7259950835978066\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.7317240993416392\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.7433004448577611\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.6948746573219458\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.721570863276611\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.702753607234311\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7292221239938521\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7363336905386046\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.7215278129367206\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7105518312480705\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7122122660088053\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -3.2269214638344885\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.8434677714359935\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7320420710446098\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.744868587830973\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.7612935261863647\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.7371425920926395\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.727006625928525\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.7276344828562369\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.7154561097767538\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7008626264576499\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7274140270629834\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7109394567936147\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7437619003791753\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.716175182307307\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7156615274818077\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7236428258265826\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7052391605086938\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.7230790630123876\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7184037051193553\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.7339803911539582\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -2.3743820057305585\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7495783321161906\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.712413171381308\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7146636713799195\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7386333185758491\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.714131870091249\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.6985571039849376\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.772593698057042\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.6922786048011653\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.7022277074946117\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.69203520120996\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.7094786291250943\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7170800662723626\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.7107394041968433\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7339218361129463\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7590594768449381\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7264586045657113\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7265510657228862\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.9390977430779162\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -2.3010658539583932\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2507171027.pkl\n",
      "Sub_Episode: 1 | avg. reward: -22.551705479215144\n",
      "Exploration noise: 0.018822049364146075\n",
      "Sub_Episode: 2 | avg. reward: -20.283035250208\n",
      "Exploration noise: 0.017717128607266288\n",
      "Sub_Episode: 3 | avg. reward: -27.56058171875759\n",
      "Exploration noise: 0.016680710066604123\n",
      "Sub_Episode: 4 | avg. reward: -22.603153757393358\n",
      "Exploration noise: 0.01570854678273058\n",
      "Sub_Episode: 5 | avg. reward: -24.74225969118408\n",
      "Exploration noise: 0.014796655096667173\n",
      "Sub_Episode: 6 | avg. reward: -25.38174482184348\n",
      "Exploration noise: 0.013941298325942101\n",
      "Sub_Episode: 7 | avg. reward: -20.387467122454158\n",
      "Exploration noise: 0.013138971452688476\n",
      "Sub_Episode: 8 | avg. reward: -25.348677834655383\n",
      "Exploration noise: 0.01238638676104074\n",
      "Sub_Episode: 9 | avg. reward: -22.74109105428533\n",
      "Exploration noise: 0.011680460364975123\n",
      "Sub_Episode: 10 | avg. reward: -23.671201044167685\n",
      "Exploration noise: 0.011018299571389067\n",
      "Sub_Episode: 11 | avg. reward: -23.70923245531406\n",
      "Exploration noise: 0.010397191026636811\n",
      "Sub_Episode: 12 | avg. reward: -20.99008361691018\n",
      "Exploration noise: 0.00981458959794904\n",
      "Sub_Episode: 13 | avg. reward: -22.01035056630073\n",
      "Exploration noise: 0.009268107944175596\n",
      "Sub_Episode: 14 | avg. reward: -24.073909780731928\n",
      "Exploration noise: 0.008755506733115042\n",
      "Sub_Episode: 15 | avg. reward: -22.178711309005415\n",
      "Exploration noise: 0.008274685465344395\n",
      "Sub_Episode: 16 | avg. reward: -18.522772115198485\n",
      "Exploration noise: 0.007823673866947569\n",
      "Sub_Episode: 17 | avg. reward: -3.2647041602774816\n",
      "Exploration noise: 0.007400623815872268\n",
      "Sub_Episode: 18 | avg. reward: -2.201539255394061\n",
      "Exploration noise: 0.00700380176883182\n",
      "Sub_Episode: 19 | avg. reward: -2.387627392945746\n",
      "Exploration noise: 0.006631581657719379\n",
      "Sub_Episode: 20 | avg. reward: -2.08349045037955\n",
      "Exploration noise: 0.00628243822642602\n",
      "Sub_Episode: 21 | avg. reward: -2.0110153861756297\n",
      "Exploration noise: 0.005954940780758777\n",
      "Sub_Episode: 22 | avg. reward: -1.9323165960474395\n",
      "Exploration noise: 0.005647747325847548\n",
      "Sub_Episode: 23 | avg. reward: -1.8564374737627434\n",
      "Exploration noise: 0.005359599067017522\n",
      "Sub_Episode: 24 | avg. reward: -1.8389004361463974\n",
      "Exploration noise: 0.005089315251593234\n",
      "Sub_Episode: 25 | avg. reward: -1.8794154007810602\n",
      "Exploration noise: 0.0048357883304973705\n",
      "Sub_Episode: 26 | avg. reward: -1.8650935425710815\n",
      "Exploration noise: 0.0045979794198178735\n",
      "Sub_Episode: 27 | avg. reward: -1.8384017285935215\n",
      "Exploration noise: 0.004374914043746094\n",
      "Sub_Episode: 28 | avg. reward: -1.839547567158221\n",
      "Exploration noise: 0.004165678141441724\n",
      "Sub_Episode: 29 | avg. reward: -1.8295820350964112\n",
      "Exploration noise: 0.003969414321461716\n",
      "Sub_Episode: 30 | avg. reward: -1.8222850755558344\n",
      "Exploration noise: 0.0037853183484048954\n",
      "Sub_Episode: 31 | avg. reward: -1.7760272470876197\n",
      "Exploration noise: 0.0036126358473754666\n",
      "Sub_Episode: 32 | avg. reward: -1.8016267120722123\n",
      "Exploration noise: 0.0034506592127612197\n",
      "Sub_Episode: 33 | avg. reward: -1.7786191585261104\n",
      "Exploration noise: 0.003298724708659464\n",
      "Sub_Episode: 34 | avg. reward: -1.805147560486628\n",
      "Exploration noise: 0.0031562097490690144\n",
      "Sub_Episode: 35 | avg. reward: -1.8198328737613556\n",
      "Exploration noise: 0.0030225303467032102\n",
      "Sub_Episode: 36 | avg. reward: -1.8136770945714835\n",
      "Exploration noise: 0.0028971387199698993\n",
      "Sub_Episode: 37 | avg. reward: -1.8436564261864787\n",
      "Exploration noise: 0.0027795210483124444\n",
      "Sub_Episode: 38 | avg. reward: -1.8288522394825322\n",
      "Exploration noise: 0.002669195366713756\n",
      "Sub_Episode: 39 | avg. reward: -1.8147179876488002\n",
      "Exploration noise: 0.002565709590735603\n",
      "Sub_Episode: 40 | avg. reward: -1.790689797526598\n",
      "Exploration noise: 0.0024686396640003615\n",
      "Sub_Episode: 41 | avg. reward: -1.7600760668822721\n",
      "Exploration noise: 0.002377587820524071\n",
      "Sub_Episode: 42 | avg. reward: -1.7673892543663563\n",
      "Exploration noise: 0.002292180954780337\n",
      "Sub_Episode: 43 | avg. reward: -1.7640495062813863\n",
      "Exploration noise: 0.0022120690928160297\n",
      "Sub_Episode: 44 | avg. reward: -1.7770663096306323\n",
      "Exploration noise: 0.002136923958153844\n",
      "Sub_Episode: 45 | avg. reward: -1.7854361865326616\n",
      "Exploration noise: 0.002066437626605166\n",
      "Sub_Episode: 46 | avg. reward: -1.7266245670359899\n",
      "Exploration noise: 0.0020003212644810553\n",
      "Sub_Episode: 47 | avg. reward: -1.74873477586535\n",
      "Exploration noise: 0.0019383039450308623\n",
      "Sub_Episode: 48 | avg. reward: -1.7292074960168333\n",
      "Exploration noise: 0.0018801315382585809\n",
      "Sub_Episode: 49 | avg. reward: -1.7637818644588708\n",
      "Exploration noise: 0.0018255656695676972\n",
      "Sub_Episode: 50 | avg. reward: -1.776723487821816\n",
      "Exploration noise: 0.0017743827429673583\n",
      "Sub_Episode: 51 | avg. reward: -1.7241981296018616\n",
      "Exploration noise: 0.0017263730248372158\n",
      "Sub_Episode: 52 | avg. reward: -1.7411216420007423\n",
      "Exploration noise: 0.0016813397844964717\n",
      "Sub_Episode: 53 | avg. reward: -1.800488987505851\n",
      "Exploration noise: 0.0016390984880554089\n",
      "Sub_Episode: 54 | avg. reward: -1.7544008338110684\n",
      "Exploration noise: 0.0015994760422460325\n",
      "Sub_Episode: 55 | avg. reward: -1.7949383613577237\n",
      "Exploration noise: 0.0015623100851332478\n",
      "Sub_Episode: 56 | avg. reward: -1.746819655835991\n",
      "Exploration noise: 0.0015274483208001012\n",
      "Sub_Episode: 57 | avg. reward: -1.7996107326647717\n",
      "Exploration noise: 0.0014947478952808085\n",
      "Sub_Episode: 58 | avg. reward: -1.8098164591263415\n",
      "Exploration noise: 0.0014640748111843125\n",
      "Sub_Episode: 59 | avg. reward: -1.776567102800724\n",
      "Exploration noise: 0.0014353033786096625\n",
      "Sub_Episode: 60 | avg. reward: -1.7872078216971818\n",
      "Exploration noise: 0.0014083157001032092\n",
      "Sub_Episode: 61 | avg. reward: -1.8114277436966852\n",
      "Exploration noise: 0.001383001187547119\n",
      "Sub_Episode: 62 | avg. reward: -1.7665768271293438\n",
      "Exploration noise: 0.0013592561089995434\n",
      "Sub_Episode: 63 | avg. reward: -1.797011185303338\n",
      "Exploration noise: 0.0013369831636295213\n",
      "Sub_Episode: 64 | avg. reward: -1.7408555340134058\n",
      "Exploration noise: 0.0013160910830048128\n",
      "Sub_Episode: 65 | avg. reward: -1.7708389359612378\n",
      "Exploration noise: 0.001296494257098851\n",
      "Sub_Episode: 66 | avg. reward: -1.7417920491755097\n",
      "Exploration noise: 0.0012781123834842915\n",
      "Sub_Episode: 67 | avg. reward: -1.8048617934463294\n",
      "Exploration noise: 0.0012608701382756508\n",
      "Sub_Episode: 68 | avg. reward: -1.727330356685564\n",
      "Exploration noise: 0.0012446968674726454\n",
      "Sub_Episode: 69 | avg. reward: -1.798383226727082\n",
      "Exploration noise: 0.0012295262974394419\n",
      "Sub_Episode: 70 | avg. reward: -1.825047848922989\n",
      "Exploration noise: 0.0012152962633334425\n",
      "Sub_Episode: 71 | avg. reward: -1.7916995690023783\n",
      "Exploration noise: 0.0012019484543707793\n",
      "Sub_Episode: 72 | avg. reward: -1.7671759403333132\n",
      "Exploration noise: 0.0011894281748846857\n",
      "Sub_Episode: 73 | avg. reward: -1.7633323232220277\n",
      "Exploration noise: 0.0011776841201976296\n",
      "Sub_Episode: 74 | avg. reward: -1.7925777360079558\n",
      "Exploration noise: 0.0011666681663887906\n",
      "Sub_Episode: 75 | avg. reward: -1.817250108416577\n",
      "Exploration noise: 0.0011563351730954072\n",
      "Sub_Episode: 76 | avg. reward: -1.7979111327129464\n",
      "Exploration noise: 0.00114664279853993\n",
      "Sub_Episode: 77 | avg. reward: -1.7896837078550414\n",
      "Exploration noise: 0.0011375513260250083\n",
      "Sub_Episode: 78 | avg. reward: -1.8147947299346656\n",
      "Exploration noise: 0.0011290235011853396\n",
      "Sub_Episode: 79 | avg. reward: -1.7989058968102185\n",
      "Exploration noise: 0.001121024379329478\n",
      "Sub_Episode: 80 | avg. reward: -1.797032205976463\n",
      "Exploration noise: 0.0011135211822460576\n",
      "Sub_Episode: 81 | avg. reward: -1.7399232859331157\n",
      "Exploration noise: 0.001106483163887656\n",
      "Sub_Episode: 82 | avg. reward: -1.7467161640072397\n",
      "Exploration noise: 0.0010998814843819085\n",
      "Sub_Episode: 83 | avg. reward: -1.7355741595131118\n",
      "Exploration noise: 0.0010936890918536082\n",
      "Sub_Episode: 84 | avg. reward: -1.7507174227004132\n",
      "Exploration noise: 0.0010878806115735276\n",
      "Sub_Episode: 85 | avg. reward: -1.7702976722220454\n",
      "Exploration noise: 0.001082432241979724\n",
      "Sub_Episode: 86 | avg. reward: -2.049297137596133\n",
      "Exploration noise: 0.0010773216571452566\n",
      "Sub_Episode: 87 | avg. reward: -1.786430762847899\n",
      "Exploration noise: 0.0010725279152926494\n",
      "Sub_Episode: 88 | avg. reward: -1.7733235921204082\n",
      "Exploration noise: 0.0010680313729802212\n",
      "Sub_Episode: 89 | avg. reward: -1.7374493464496368\n",
      "Exploration noise: 0.0010638136046086388\n",
      "Sub_Episode: 90 | avg. reward: -1.7372349994871652\n",
      "Exploration noise: 0.0010598573269178558\n",
      "Sub_Episode: 91 | avg. reward: -1.7453934611179227\n",
      "Exploration noise: 0.001056146328165045\n",
      "Sub_Episode: 92 | avg. reward: -1.753950841526783\n",
      "Exploration noise: 0.0010526654016933146\n",
      "Sub_Episode: 93 | avg. reward: -1.7610736986115472\n",
      "Exploration noise: 0.0010494002836189914\n",
      "Sub_Episode: 94 | avg. reward: -1.7806555249417915\n",
      "Exploration noise: 0.0010463375943821306\n",
      "Sub_Episode: 95 | avg. reward: -1.7777309458250183\n",
      "Exploration noise: 0.0010434647839207426\n",
      "Sub_Episode: 96 | avg. reward: -1.7308065171814166\n",
      "Exploration noise: 0.0010407700802440745\n",
      "Sub_Episode: 97 | avg. reward: -1.8281540672060077\n",
      "Exploration noise: 0.0010382424411942155\n",
      "Sub_Episode: 98 | avg. reward: -1.8110754215212994\n",
      "Exploration noise: 0.0010358715091983558\n",
      "Sub_Episode: 99 | avg. reward: -1.8523081439097686\n",
      "Exploration noise: 0.0010336475688262905\n",
      "Sub_Episode: 100 | avg. reward: -1.7633134784452686\n",
      "Exploration noise: 0.0010315615069792447\n",
      "Sub_Episode: 101 | avg. reward: -1.7285689576349323\n",
      "Exploration noise: 0.0010296047755468916\n",
      "Sub_Episode: 102 | avg. reward: -1.7273526373585781\n",
      "Exploration noise: 0.001027769356379535\n",
      "Sub_Episode: 103 | avg. reward: -1.7229951894932218\n",
      "Exploration noise: 0.0010260477284319285\n",
      "Sub_Episode: 104 | avg. reward: -1.728738798096591\n",
      "Exploration noise: 0.0010244328369440894\n",
      "Sub_Episode: 105 | avg. reward: -1.7192798113250134\n",
      "Exploration noise: 0.001022918064532826\n",
      "Sub_Episode: 106 | avg. reward: -1.7590487234209247\n",
      "Exploration noise: 0.001021497204075511\n",
      "Sub_Episode: 107 | avg. reward: -1.7516725749616437\n",
      "Exploration noise: 0.0010201644332749936\n",
      "Sub_Episode: 108 | avg. reward: -1.778633225650017\n",
      "Exploration noise: 0.0010189142908014193\n",
      "Sub_Episode: 109 | avg. reward: -1.7585619736862044\n",
      "Exploration noise: 0.0010177416539131953\n",
      "Sub_Episode: 110 | avg. reward: -1.7527485017271862\n",
      "Exploration noise: 0.0010166417174653978\n",
      "Sub_Episode: 111 | avg. reward: -1.7473173265345223\n",
      "Exploration noise: 0.001015609974219605\n",
      "Sub_Episode: 112 | avg. reward: -1.7579001941391312\n",
      "Exploration noise: 0.0010146421963744656\n",
      "Sub_Episode: 113 | avg. reward: -1.7256955240284255\n",
      "Exploration noise: 0.0010137344182413288\n",
      "Sub_Episode: 114 | avg. reward: -1.7004925063316418\n",
      "Exploration noise: 0.0010128829199939364\n",
      "Sub_Episode: 115 | avg. reward: -1.764282119584808\n",
      "Exploration noise: 0.0010120842124255935\n",
      "Sub_Episode: 116 | avg. reward: -1.7381381399602025\n",
      "Exploration noise: 0.0010113350226513555\n",
      "Sub_Episode: 117 | avg. reward: -1.757596295425139\n",
      "Exploration noise: 0.0010106322806966407\n",
      "Sub_Episode: 118 | avg. reward: -1.7352326174961759\n",
      "Exploration noise: 0.0010099731069173152\n",
      "Sub_Episode: 119 | avg. reward: -1.7586756059482656\n",
      "Exploration noise: 0.0010093548001996998\n",
      "Sub_Episode: 120 | avg. reward: -1.769230115705443\n",
      "Exploration noise: 0.001008774826892146\n",
      "Sub_Episode: 121 | avg. reward: -1.7605440115030877\n",
      "Exploration noise: 0.0010082308104228243\n",
      "Sub_Episode: 122 | avg. reward: -1.7564278461469471\n",
      "Exploration noise: 0.0010077205215611844\n",
      "Sub_Episode: 123 | avg. reward: -1.721258498765383\n",
      "Exploration noise: 0.001007241869283178\n",
      "Sub_Episode: 124 | avg. reward: -1.7610215099902158\n",
      "Exploration noise: 0.0010067928922028154\n",
      "Sub_Episode: 125 | avg. reward: -1.7328934203305146\n",
      "Exploration noise: 0.001006371750534942\n",
      "Sub_Episode: 126 | avg. reward: -1.7296468630956323\n",
      "Exploration noise: 0.0010059767185563032\n",
      "Sub_Episode: 127 | avg. reward: -1.717459015469573\n",
      "Exploration noise: 0.0010056061775340023\n",
      "Sub_Episode: 128 | avg. reward: -1.7204184679862402\n",
      "Exploration noise: 0.0010052586090923766\n",
      "Sub_Episode: 129 | avg. reward: -1.7487110327075488\n",
      "Exploration noise: 0.0010049325889911096\n",
      "Sub_Episode: 130 | avg. reward: -1.7514603481984705\n",
      "Exploration noise: 0.0010046267812890842\n",
      "Sub_Episode: 131 | avg. reward: -1.749910664797515\n",
      "Exploration noise: 0.0010043399328700613\n",
      "Sub_Episode: 132 | avg. reward: -1.7312854489030334\n",
      "Exploration noise: 0.0010040708683077533\n",
      "Sub_Episode: 133 | avg. reward: -1.71971672773859\n",
      "Exploration noise: 0.0010038184850492482\n",
      "Sub_Episode: 134 | avg. reward: -1.71503516930527\n",
      "Exploration noise: 0.0010035817488970504\n",
      "Sub_Episode: 135 | avg. reward: -1.7330467048542357\n",
      "Exploration noise: 0.0010033596897712214\n",
      "Sub_Episode: 136 | avg. reward: -1.7185255244276185\n",
      "Exploration noise: 0.0010031513977342592\n",
      "Sub_Episode: 137 | avg. reward: -1.7218639644442004\n",
      "Exploration noise: 0.0010029560192624225\n",
      "Sub_Episode: 138 | avg. reward: -1.709661218040065\n",
      "Exploration noise: 0.0010027727537482243\n",
      "Sub_Episode: 139 | avg. reward: -1.7042624955506107\n",
      "Exploration noise: 0.0010026008502197618\n",
      "Sub_Episode: 140 | avg. reward: -1.7262848021628547\n",
      "Exploration noise: 0.0010024396042634391\n",
      "Sub_Episode: 141 | avg. reward: -1.7113024291946473\n",
      "Exploration noise: 0.0010022883551374733\n",
      "Sub_Episode: 142 | avg. reward: -1.7129959858134243\n",
      "Exploration noise: 0.0010021464830643552\n",
      "Sub_Episode: 143 | avg. reward: -1.7223779903315608\n",
      "Exploration noise: 0.0010020134066911705\n",
      "Sub_Episode: 144 | avg. reward: -1.7147288510966843\n",
      "Exploration noise: 0.0010018885807073759\n",
      "Sub_Episode: 145 | avg. reward: -1.714067934263075\n",
      "Exploration noise: 0.0010017714936102647\n",
      "Sub_Episode: 146 | avg. reward: -1.7346073950588272\n",
      "Exploration noise: 0.0010016616656089688\n",
      "Sub_Episode: 147 | avg. reward: -1.7376718167990202\n",
      "Exploration noise: 0.0010015586466584076\n",
      "Sub_Episode: 148 | avg. reward: -1.7316992881134299\n",
      "Exploration noise: 0.0010014620146151264\n",
      "Sub_Episode: 149 | avg. reward: -1.7411414881942442\n",
      "Exploration noise: 0.0010013713735074676\n",
      "Sub_Episode: 150 | avg. reward: -1.729393527855998\n",
      "Exploration noise: 0.001001286351912988\n",
      "Sub_Episode: 151 | avg. reward: -1.6953056162544573\n",
      "Exploration noise: 0.0010012066014364702\n",
      "Sub_Episode: 152 | avg. reward: -1.728834757417455\n",
      "Exploration noise: 0.001001131795282296\n",
      "Sub_Episode: 153 | avg. reward: -1.854721463730285\n",
      "Exploration noise: 0.0010010616269153256\n",
      "Sub_Episode: 154 | avg. reward: -1.7217572565813055\n",
      "Exploration noise: 0.001000995808804802\n",
      "Sub_Episode: 155 | avg. reward: -1.7217104632167604\n",
      "Exploration noise: 0.0010009340712461281\n",
      "Sub_Episode: 156 | avg. reward: -1.7512561177985817\n",
      "Exploration noise: 0.0010008761612556908\n",
      "Sub_Episode: 157 | avg. reward: -1.7025185036473527\n",
      "Exploration noise: 0.0010008218415342037\n",
      "Sub_Episode: 158 | avg. reward: -1.6915268853202508\n",
      "Exploration noise: 0.0010007708894943202\n",
      "Sub_Episode: 159 | avg. reward: -1.7203426537883757\n",
      "Exploration noise: 0.0010007230963485305\n",
      "Sub_Episode: 160 | avg. reward: -1.7202795207784753\n",
      "Exploration noise: 0.0010006782662536076\n",
      "Sub_Episode: 161 | avg. reward: -1.7029837211001542\n",
      "Exploration noise: 0.0010006362155080962\n",
      "Sub_Episode: 162 | avg. reward: -1.7253781291472574\n",
      "Exploration noise: 0.001000596771799554\n",
      "Sub_Episode: 163 | avg. reward: -1.7297362025431045\n",
      "Exploration noise: 0.0010005597734984622\n",
      "Sub_Episode: 164 | avg. reward: -1.724967235293097\n",
      "Exploration noise: 0.0010005250689959122\n",
      "Sub_Episode: 165 | avg. reward: -1.716489049196087\n",
      "Exploration noise: 0.0010004925160823543\n",
      "Sub_Episode: 166 | avg. reward: -1.6956573722369177\n",
      "Exploration noise: 0.0010004619813648607\n",
      "Sub_Episode: 167 | avg. reward: -1.7083394818202617\n",
      "Exploration noise: 0.001000433339720519\n",
      "Sub_Episode: 168 | avg. reward: -1.6994065167411794\n",
      "Exploration noise: 0.0010004064737837125\n",
      "Sub_Episode: 169 | avg. reward: -1.6998286839935866\n",
      "Exploration noise: 0.0010003812734651872\n",
      "Sub_Episode: 170 | avg. reward: -1.7089374902606311\n",
      "Exploration noise: 0.0010003576355009371\n",
      "Sub_Episode: 171 | avg. reward: -1.7105940970843618\n",
      "Exploration noise: 0.0010003354630290565\n",
      "Sub_Episode: 172 | avg. reward: -1.7297240615213474\n",
      "Exploration noise: 0.0010003146651928257\n",
      "Sub_Episode: 173 | avg. reward: -1.7224373671930708\n",
      "Exploration noise: 0.0010002951567684062\n",
      "Sub_Episode: 174 | avg. reward: -1.7332571938663568\n",
      "Exploration noise: 0.0010002768578156157\n",
      "Sub_Episode: 175 | avg. reward: -1.7675522654255997\n",
      "Exploration noise: 0.0010002596933503553\n",
      "Sub_Episode: 176 | avg. reward: -1.7227382451660191\n",
      "Exploration noise: 0.001000243593037346\n",
      "Sub_Episode: 177 | avg. reward: -1.7068125005508352\n",
      "Exploration noise: 0.0010002284909019128\n",
      "Sub_Episode: 178 | avg. reward: -1.6824417153393991\n",
      "Exploration noise: 0.0010002143250596393\n",
      "Sub_Episode: 179 | avg. reward: -1.7057526329241655\n",
      "Exploration noise: 0.0010002010374627825\n",
      "Sub_Episode: 180 | avg. reward: -1.6809068572459172\n",
      "Exploration noise: 0.0010001885736624079\n",
      "Sub_Episode: 181 | avg. reward: -1.704417629908094\n",
      "Exploration noise: 0.0010001768825852744\n",
      "Sub_Episode: 182 | avg. reward: -1.7027969573219763\n",
      "Exploration noise: 0.0010001659163245483\n",
      "Sub_Episode: 183 | avg. reward: -1.6994829725652556\n",
      "Exploration noise: 0.0010001556299434956\n",
      "Sub_Episode: 184 | avg. reward: -1.7286160709674454\n",
      "Exploration noise: 0.0010001459812913431\n",
      "Sub_Episode: 185 | avg. reward: -1.7249448809586971\n",
      "Exploration noise: 0.0010001369308305556\n",
      "Sub_Episode: 186 | avg. reward: -1.7231363204523313\n",
      "Exploration noise: 0.001000128441474823\n",
      "Sub_Episode: 187 | avg. reward: -1.740561682365769\n",
      "Exploration noise: 0.0010001204784370894\n",
      "Sub_Episode: 188 | avg. reward: -1.7142958380915154\n",
      "Exploration noise: 0.0010001130090870065\n",
      "Sub_Episode: 189 | avg. reward: -1.7242157231561088\n",
      "Exploration noise: 0.0010001060028172225\n",
      "Sub_Episode: 190 | avg. reward: -1.706944735188049\n",
      "Exploration noise: 0.0010000994309179619\n",
      "Sub_Episode: 191 | avg. reward: -1.7161143013340592\n",
      "Exploration noise: 0.0010000932664593811\n",
      "Sub_Episode: 192 | avg. reward: -1.6931163550896458\n",
      "Exploration noise: 0.0010000874841812162\n",
      "Sub_Episode: 193 | avg. reward: -1.7044320249111378\n",
      "Exploration noise: 0.0010000820603892745\n",
      "Sub_Episode: 194 | avg. reward: -1.706780166207372\n",
      "Exploration noise: 0.0010000769728583418\n",
      "Sub_Episode: 195 | avg. reward: -1.7120277747534858\n",
      "Exploration noise: 0.0010000722007411087\n",
      "Sub_Episode: 196 | avg. reward: -1.7250271669641848\n",
      "Exploration noise: 0.0010000677244827456\n",
      "Sub_Episode: 197 | avg. reward: -1.7085405343551838\n",
      "Exploration noise: 0.0010000635257407713\n",
      "Sub_Episode: 198 | avg. reward: -1.7272163932538098\n",
      "Exploration noise: 0.0010000595873098906\n",
      "Sub_Episode: 199 | avg. reward: -1.7973903051251585\n",
      "Exploration noise: 0.0010000558930514918\n",
      "Sub_Episode: 200 | avg. reward: -1.6971425307138874\n",
      "Exploration noise: 0.0010000558930514918\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "526fe8424aed4e26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
