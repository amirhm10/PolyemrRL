{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.mpc import *\n",
    "from TD3Agent.agent import *\n",
    "from Simulation.system_functions import PolymerCSTR\n",
    "from BasicFunctions.td3_functions import *\n",
    "from TD3Agent.replay_buffer import ReplayDataset, DataLoader"
   ],
   "id": "7d5344ace6d0a16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:45.835458900Z",
     "start_time": "2025-06-17T23:08:06.229170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from TD3Agent.actor import Actor\n",
    "from TD3Agent.critic import Critic\n",
    "from TD3Agent.replay_buffer import ReplayBuffer\n",
    "from torch.amp import GradScaler, autocast\n",
    "import gc\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 actor_layer_sizes: list,\n",
    "                 critic_layer_sizes: list,\n",
    "                 buffer_capacity: int,\n",
    "                 actor_lr: float,\n",
    "                 critic_lr: float,\n",
    "                 target_policy_smoothing_noise_std: float,\n",
    "                 noise_clip: float,\n",
    "                 exploration_noise_std: float,\n",
    "                 gamma: float,\n",
    "                 tau: float,\n",
    "                 max_action: float,\n",
    "                 policy_delay: int,\n",
    "                 device):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.actor_layer_sizes = actor_layer_sizes\n",
    "        self.critic_layer_sizes = critic_layer_sizes\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.t_std = target_policy_smoothing_noise_std\n",
    "        self.noise_clip = noise_clip\n",
    "        self.exploration_noise_std = exploration_noise_std\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.policy_delay = policy_delay\n",
    "        self.total_it = 0\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "        self.actor_target = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "        self.critic_target = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_capacity, self.state_dim, self.action_dim)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Let's implement Huber loss\n",
    "        # self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # Defining the lists for losses storage for later visualization\n",
    "        self.critic_losses_pretrain = []\n",
    "        self.actor_losses_pretrain = []\n",
    "        self.critic_losses = []\n",
    "        self.actor_losses = []\n",
    "\n",
    "        self.scheduler_actor = StepLR(self.actor_optimizer, step_size=150, gamma=0.1)\n",
    "        self.scheduler_critic = StepLR(self.critic_optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "        self.buffer_save = False\n",
    "\n",
    "        self.exploration_noise_std_min = exploration_noise_std * 0.1\n",
    "        self.exploration_noise_std_initial = exploration_noise_std\n",
    "        self.decay_rate = 0.99995\n",
    "\n",
    "        self.warm_start = True\n",
    "\n",
    "    def pre_train_entire_data(self, path, data_loader, epochs):\n",
    "\n",
    "        scaler = GradScaler('cuda')  # Initialize GradScaler for mixed precision training\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "        save_path = os.path.join(path, \"PLots\", timestamp)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss_critic_epoch = 0.0\n",
    "            loss_actor_epoch = 0.0\n",
    "\n",
    "            for batch_idx, (states_batch, actions_batch, rewards_batch, next_states_batch) in enumerate(data_loader):\n",
    "                # Move data to GPU\n",
    "                states_batch = states_batch.to(self.device)\n",
    "                actions_batch = actions_batch.to(self.device)\n",
    "                rewards_batch = rewards_batch.to(self.device)\n",
    "                next_states_batch = next_states_batch.to(self.device)\n",
    "                batch_size = states_batch.size(0)\n",
    "\n",
    "                # Compute target Q-value without tracking gradients\n",
    "                with torch.no_grad():\n",
    "                    noise = (self.t_std * torch.randn(batch_size, self.action_dim, device=self.device)).clamp(\n",
    "                        -self.noise_clip, self.noise_clip)\n",
    "                    next_actions = (self.actor_target(next_states_batch) + noise).clamp(-self.max_action,\n",
    "                                                                                        self.max_action)\n",
    "                    target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "                    target_Q = torch.min(target_Q1, target_Q2)\n",
    "                    target_value = rewards_batch + self.gamma * target_Q\n",
    "\n",
    "                # --- Critic Update with Mixed Precision ---\n",
    "                with autocast('cuda'):\n",
    "                    current_Q1, current_Q2 = self.critic(states_batch, actions_batch)\n",
    "                    critic_loss = self.loss_fn(current_Q1, target_value) + self.loss_fn(current_Q2, target_value)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                scaler.scale(critic_loss).backward()\n",
    "                scaler.step(self.critic_optimizer)\n",
    "                loss_critic_epoch += critic_loss.item() * batch_size\n",
    "\n",
    "                # --- Actor Update with Mixed Precision ---\n",
    "                with autocast('cuda'):\n",
    "                    actor_output = self.actor(states_batch)\n",
    "                    actor_loss = self.loss_fn(actions_batch, actor_output)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                scaler.scale(actor_loss).backward()\n",
    "                scaler.step(self.actor_optimizer)\n",
    "                loss_actor_epoch += actor_loss.item() * batch_size\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "            # Average losses over the epoch\n",
    "            num_samples = len(data_loader.dataset)\n",
    "            loss_critic_epoch /= num_samples\n",
    "            loss_actor_epoch /= num_samples\n",
    "\n",
    "            self.critic_losses_pretrain.append(loss_critic_epoch)\n",
    "            self.actor_losses_pretrain.append(loss_actor_epoch)\n",
    "\n",
    "            # if epoch == 0 or epoch % 10 == 9:\n",
    "            print(f\"{datetime.datetime.now()} Epoch {epoch + 1},\"\n",
    "                  f\" Actor Loss: {loss_actor_epoch},\"\n",
    "                  f\" Critic Loss: {loss_critic_epoch}\")\n",
    "            if epoch == 0 or epoch % 100 == 99:\n",
    "                current_lr_cr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch + 1}, Learning rate (Critic): {current_lr_cr}\")\n",
    "                current_lr_ac = self.actor_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch + 1}, Learning rate (Actor): {current_lr_ac}\")\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                if epoch > 1:\n",
    "                    # Exclude the first `plot_start` losses to get a clearer plot of later epochs\n",
    "                    if epoch < 110:\n",
    "                        plot_start = 0\n",
    "                    else:\n",
    "                        plot_start = epoch - 100\n",
    "                    critic_losses_plot = self.critic_losses_pretrain[plot_start:]\n",
    "                    actor_losses_plot = self.actor_losses_pretrain[plot_start:]\n",
    "\n",
    "                    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "                    # Plot Critic Loss\n",
    "                    axs[0].plot(range(plot_start + 1, len(critic_losses_plot) + 1 + plot_start), critic_losses_plot,\n",
    "                                label=\"Critic Loss\", color='r', linewidth=2)\n",
    "                    axs[0].set_title(\"Critic Loss (Pre-train)\", fontsize=16)\n",
    "                    axs[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "                    axs[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "                    axs[0].grid(True)\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    # Plot Actor Loss\n",
    "                    axs[1].plot(range(plot_start + 1, len(actor_losses_plot) + 1 + plot_start), actor_losses_plot,\n",
    "                                label=\"Actor Loss\", color='b', linewidth=2)\n",
    "                    axs[1].set_title(\"Actor Loss (Pre-train)\", fontsize=16)\n",
    "                    axs[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "                    axs[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "                    axs[1].grid(True)\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    # Show the plot\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    filename = os.path.join(save_path, f\"{plot_start}_{epoch}.png\")\n",
    "                    plt.savefig(filename)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            # Update the learning rate schedulers at the end of the epoch\n",
    "            # self.scheduler_actor.step()\n",
    "            # self.scheduler_critic.step()\n",
    "\n",
    "            # Clear CUDA cache and collect garbage to free up memory (helpful with large models/datasets)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def pre_train(self, path: str, batch_size: int, epochs: int = 1000, log_interval: int = 1000):\n",
    "        \"\"\"\n",
    "        Pre-train on stored replay buffer samples using TD3-style updates\n",
    "        with mixed-precision (GradScaler + autocast).\n",
    "\n",
    "        Args:\n",
    "            path:            path to store plots\n",
    "            batch_size:      number of samples per gradient update\n",
    "            epochs:          total number of gradient-update iterations\n",
    "            log_interval:    print losses every `log_interval` steps\n",
    "        \"\"\"\n",
    "\n",
    "        # time to store the losses of critic and actor\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "\n",
    "        path = os.path.join(path, \"PLots_pretrain_normal\", timestamp)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "        scaler = GradScaler(str(self.device))\n",
    "        for it in range(1, epochs + 1):\n",
    "            # 1) Sample a random minibatch\n",
    "            states, actions, rewards, next_states = \\\n",
    "                self.replay_buffer.sample_pretrain(batch_size, device=str(self.device))\n",
    "\n",
    "            # 2) Compute target Q-values (no grad)\n",
    "            with torch.no_grad():\n",
    "                noise = (torch.randn(batch_size, self.action_dim, device=self.device)\n",
    "                         * self.t_std).clamp(-self.noise_clip, self.noise_clip)\n",
    "                next_actions = (self.actor_target(next_states) + noise).clamp(\n",
    "                    -self.max_action, self.max_action\n",
    "                )\n",
    "                target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n",
    "                target_Q = torch.min(target_Q1, target_Q2)\n",
    "                target_value = rewards + self.gamma * target_Q\n",
    "\n",
    "            # 3) Critic update (mixed precision)\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            with autocast(str(self.device)):\n",
    "                current_Q1, current_Q2 = self.critic(states, actions)\n",
    "                critic_loss = self.loss_fn(current_Q1, target_value) \\\n",
    "                              + self.loss_fn(current_Q2, target_value)\n",
    "            scaler.scale(critic_loss).backward()\n",
    "            scaler.step(self.critic_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # 4) Delayed actor update\n",
    "            if it % self.policy_delay == 0:\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                with autocast(str(self.device)):\n",
    "                    actor_actions = self.actor(states)\n",
    "                    # Imitation actor loss:\n",
    "                    actor_loss = self.loss_fn(actor_actions, actions)\n",
    "                scaler.scale(actor_loss).backward()\n",
    "                scaler.step(self.actor_optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # 5) Soft update of targets\n",
    "                for p, p_tgt in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    p_tgt.data.mul_(1 - self.tau)\n",
    "                    p_tgt.data.add_(self.tau * p.data)\n",
    "                for p, p_tgt in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    p_tgt.data.mul_(1 - self.tau)\n",
    "                    p_tgt.data.add_(self.tau * p.data)\n",
    "\n",
    "                # store the actor loss\n",
    "                self.actor_losses_pretrain.append(actor_loss.item())\n",
    "\n",
    "            # store the critic loss\n",
    "            self.critic_losses_pretrain.append(critic_loss)\n",
    "\n",
    "            # 6) Logging\n",
    "            if it % log_interval == 0 or it == 1:\n",
    "                print(f\"[Pre-train] Iter {it}/{epochs}: \"\n",
    "                      f\"critic_loss={critic_loss.item():.6e}, \"\n",
    "                      f\"actor_loss={(actor_loss.item() if 'actor_loss' in locals() else float('nan')):.6e}\")\n",
    "\n",
    "            # if it % 10000 == 0:\n",
    "            #         if it > 1:\n",
    "            #             if it < 11000:\n",
    "            #                 plot_start = 0\n",
    "            #             else:\n",
    "            #                 plot_start = it - 10000\n",
    "            #             critic_losses_plot = self.critic_losses_pretrain[plot_start:]\n",
    "            #             actor_losses_plot = self.actor_losses_pretrain[plot_start:]\n",
    "            #\n",
    "            #             fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            #\n",
    "            #             # Plot Critic Loss\n",
    "            #             axs[0].plot(range(plot_start + 1, len(critic_losses_plot) + 1 + plot_start), critic_losses_plot,\n",
    "            #                         label=\"Critic Loss\", color='r', linewidth=2)\n",
    "            #             axs[0].set_title(\"Critic Loss (Pre-train)\", fontsize=16)\n",
    "            #             axs[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "            #             axs[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "            #             axs[0].grid(True)\n",
    "            #             axs[0].legend()\n",
    "            #\n",
    "            #             # Plot Actor Loss\n",
    "            #             axs[1].plot(range(plot_start + 1, len(actor_losses_plot) + 1 + plot_start), actor_losses_plot,\n",
    "            #                         label=\"Actor Loss\", color='b', linewidth=2)\n",
    "            #             axs[1].set_title(\"Actor Loss (Pre-train)\", fontsize=16)\n",
    "            #             axs[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "            #             axs[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "            #             axs[1].grid(True)\n",
    "            #             axs[1].legend()\n",
    "            #\n",
    "            #             # Show the plot\n",
    "            #             plt.tight_layout()\n",
    "            #\n",
    "            #             filename = os.path.join(path, f\"{plot_start}_{it}.png\")\n",
    "            #             plt.savefig(filename)\n",
    "            #             plt.close(fig)\n",
    "\n",
    "\n",
    "    def train(self, n_samples=100):\n",
    "\n",
    "        self.buffer_save = True\n",
    "\n",
    "        (states_batch, actions_batch,\n",
    "         rewards_batch, next_states_batch) = self.replay_buffer.sample(n_samples, device=self.device)\n",
    "        # (states_batch, actions_batch,\n",
    "        #  rewards_batch, next_states_batch) = self.replay_buffer.sample_pretrain(n_samples, device=self.device)\n",
    "\n",
    "        loss_critic = 0\n",
    "        loss_actor = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (self.t_std * np.random.randn(n_samples,\n",
    "                                                  self.action_dim)).clip(\n",
    "                -self.noise_clip, self.noise_clip)\n",
    "            noise = torch.from_numpy(noise).to(dtype=torch.float32, device=self.device)\n",
    "            next_actions = (self.actor_target(next_states_batch) + noise).clip(-1, 1)\n",
    "\n",
    "            target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "            r = rewards_batch + self.gamma * target_Q\n",
    "\n",
    "        q1, q2 = self.critic(states_batch, actions_batch)\n",
    "\n",
    "        loss = self.loss_fn(q1, r) + self.loss_fn(q2, r)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        loss_critic += loss.item()\n",
    "        self.critic_losses.append(loss.item())\n",
    "\n",
    "        # if self.total_it % self.policy_delay == 0:\n",
    "        actions = self.actor(states_batch)\n",
    "        q = self.critic.q1_forward(states_batch, actions)\n",
    "\n",
    "        actor_loss = - torch.mean(q)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        loss_actor += actor_loss.item()\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        # print(f'Critic loss: {self.critic_losses[-1]}')\n",
    "        # print(f'Actor loss: {self.actor_losses[-1]}')\n",
    "        self.total_it += 1\n",
    "\n",
    "        self.decay_exploration()\n",
    "\n",
    "    def decay_exploration(self):\n",
    "\n",
    "        self.exploration_noise_std = max(\n",
    "            self.exploration_noise_std_min,\n",
    "            self.exploration_noise_std_initial * (self.decay_rate ** self.total_it)\n",
    "        )\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        state = state if isinstance(state, torch.Tensor) else torch.from_numpy(state).to(dtype=torch.float32,\n",
    "                                                                                         device=self.device)\n",
    "\n",
    "        # If warm start is active, choose a random action from the action space.\n",
    "        if self.warm_start:\n",
    "            action = np.random.uniform(low=-self.max_action, high=self.max_action, size=self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.actor(state).detach().cpu().numpy()\n",
    "            if explore:\n",
    "                action += np.random.randn(self.action_dim) * self.exploration_noise_std\n",
    "            action = action.clip(-self.max_action, self.max_action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save(self, path: str, name_prefix=\"agent\"):\n",
    "        path = os.path.join(path, \"models\")\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "        filename = os.path.join(path, f\"{name_prefix}_{timestamp}.pkl\")\n",
    "\n",
    "        save_dict = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_target_state_dict': self.actor_target.state_dict(),\n",
    "            'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'agent_attributes': {key: value for key, value in self.__dict__.items() if key not in ['actor', 'critic',\n",
    "                                                                                                   'actor_target',\n",
    "                                                                                                   'critic_target',\n",
    "                                                                                                   'replay_buffer',\n",
    "                                                                                                   'total_it',\n",
    "                                                                                                   'device',\n",
    "                                                                                                   'loss_fn',\n",
    "                                                                                                   'replay_buffer']}\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "        print(f\"Agent saved successfully to {filename}\")\n",
    "        return filename\n",
    "\n",
    "    def load(self, path: str):\n",
    "        with open(path, 'rb') as f:\n",
    "            loaded_dict = pickle.load(f)\n",
    "\n",
    "        self.actor.load_state_dict(loaded_dict['actor_state_dict'])\n",
    "        self.critic.load_state_dict(loaded_dict['critic_state_dict'])\n",
    "        self.actor_target.load_state_dict(loaded_dict['actor_target_state_dict'])\n",
    "        self.critic_target.load_state_dict(loaded_dict['critic_target_state_dict'])\n",
    "\n",
    "        for key, value in loaded_dict['agent_attributes'].items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        self.actor_optimizer.load_state_dict(loaded_dict['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(loaded_dict['critic_optimizer_state_dict'])\n",
    "        self.actor_optimizer.param_groups[0]['params'] = list(self.actor.parameters())\n",
    "        self.critic_optimizer.param_groups[0]['params'] = list(self.critic.parameters())\n",
    "        self.actor_optimizer.param_groups[0]['lr'] = self.actor_lr # * 1e-3\n",
    "        self.critic_optimizer.param_groups[0]['lr'] = self.critic_lr\n",
    "\n",
    "        # self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr * 1e-2)\n",
    "        # self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        print(f\"Agent loaded successfully from: {path}\")\n"
   ],
   "id": "d3ad5ba36454535c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize the system",
   "id": "906ae4312088c426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.486777Z",
     "start_time": "2025-06-19T23:16:56.310434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First initiate the system\n",
    "# Parameters\n",
    "Ad = 2.142e17           # h^-1\n",
    "Ed = 14897              # K\n",
    "Ap = 3.816e10           # L/(molh)\n",
    "Ep = 3557               # K\n",
    "At = 4.50e12            # L/(molh)\n",
    "Et = 843                # K\n",
    "fi = 0.6                # Coefficient\n",
    "m_delta_H_r = -6.99e4   # j/mol\n",
    "hA = 1.05e6             # j/(Kh)\n",
    "rhocp = 1506            # j/(Kh)\n",
    "rhoccpc = 4043          # j/(Kh)\n",
    "Mm = 104.14             # g/mol\n",
    "system_params = np.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])"
   ],
   "id": "fd5e178b0c3baa88",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m rhoccpc = \u001B[32m4043\u001B[39m          \u001B[38;5;66;03m# j/(Kh)\u001B[39;00m\n\u001B[32m     14\u001B[39m Mm = \u001B[32m104.14\u001B[39m             \u001B[38;5;66;03m# g/mol\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m system_params = \u001B[43mnp\u001B[49m.array([Ad, Ed, Ap, Ep, At, Et, fi, m_delta_H_r, hA, rhocp, rhoccpc, Mm])\n",
      "\u001B[31mNameError\u001B[39m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.495861Z",
     "start_time": "2025-06-17T23:08:07.407285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Parameters\n",
    "CIf = 0.5888    # mol/L\n",
    "CMf = 8.6981    # mol/L\n",
    "Qi = 108.       # L/h\n",
    "Qs = 459.       # L/h\n",
    "Tf = 330.       # K\n",
    "Tcf = 295.      # K\n",
    "V = 3000.       # L\n",
    "Vc = 3312.4     # L\n",
    "        \n",
    "system_design_params = np.array([CIf, CMf, Qi, Qs, Tf, Tcf, V, Vc])"
   ],
   "id": "79492d92ea8ebfb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.511637200Z",
     "start_time": "2025-06-17T23:08:07.691889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steady State Inputs\n",
    "Qm_ss = 378.    # L/h\n",
    "Qc_ss = 471.6   # L/h\n",
    "\n",
    "system_steady_state_inputs = np.array([Qc_ss, Qm_ss])"
   ],
   "id": "fefe97d84d165283",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.794055100Z",
     "start_time": "2025-06-17T23:08:07.962059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sampling time of the system\n",
    "delta_t = 0.5 # 30 mins"
   ],
   "id": "978dc0855d95a24c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.810559700Z",
     "start_time": "2025-06-17T23:08:08.520759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate the CSTR for steady state values\n",
    "cstr = PolymerCSTR(system_params, system_design_params, system_steady_state_inputs, delta_t)\n",
    "steady_states={\"ss_inputs\":cstr.ss_inputs,\n",
    "               \"y_ss\":cstr.y_ss}"
   ],
   "id": "de0a1c82e0128367",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the system matrices, min max scaling, and min max of the states",
   "id": "40038af50eec57a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.870593300Z",
     "start_time": "2025-06-17T23:08:09.090588Z"
    }
   },
   "cell_type": "code",
   "source": "dir_path = os.path.join(os.getcwd(), \"Data\")",
   "id": "2dc0a1fc3545317",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.920332100Z",
     "start_time": "2025-06-17T23:08:09.369602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the range of setpoints for data generation\n",
    "setpoint_y = np.array([[3.2, 321],\n",
    "                       [4.5, 325]])\n",
    "u_min = np.array([71.6, 78])\n",
    "u_max = np.array([870, 670])\n",
    "\n",
    "system_data = load_and_prepare_system_data(steady_states=steady_states, setpoint_y=setpoint_y, u_min=u_min, u_max=u_max)"
   ],
   "id": "3f87594c63734151",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.940560Z",
     "start_time": "2025-06-17T23:08:09.614114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_aug = system_data[\"A_aug\"]\n",
    "B_aug = system_data[\"B_aug\"]\n",
    "C_aug = system_data[\"C_aug\"]"
   ],
   "id": "c3b76ba44b3d8640",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.949217400Z",
     "start_time": "2025-06-17T23:08:09.812854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_min = system_data[\"data_min\"]\n",
    "data_max = system_data[\"data_max\"]"
   ],
   "id": "11a0deb5073c3853",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.971811100Z",
     "start_time": "2025-06-17T23:08:10.078157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "# min_max_states"
   ],
   "id": "b310edd0c5253181",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:56.982844100Z",
     "start_time": "2025-06-17T23:08:10.348143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "# min_max_states = system_data[\"min_max_states\"]\n",
    "min_max_states = {'max_s': np.array([267.22485424, 309.39633111,  59.30922216, 176.90730552,\n",
    "         2.93078497,   3.25879914,   2.90161768,   5.05992019,\n",
    "         8.006078  ]),\n",
    "                  'min_s': np.array([-2.45841151e+02, -4.42441998e+03, -7.59124687e+01, -2.66399529e+03,\n",
    "       -2.63938070e+00, -2.98235548e+00, -2.66964174e+00, -5.38486567e+00,\n",
    "       -1.00988888e+02])}"
   ],
   "id": "49c3fda12d45a9d9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.009116600Z",
     "start_time": "2025-06-17T23:08:10.786606Z"
    }
   },
   "cell_type": "code",
   "source": "y_sp_scaled_deviation = system_data[\"y_sp_scaled_deviation\"]",
   "id": "e6e0bf0ab38a021a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.021748200Z",
     "start_time": "2025-06-17T23:08:11.133771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b_min = system_data[\"b_min\"]\n",
    "b_max = system_data[\"b_max\"]"
   ],
   "id": "753421957cb27b91",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.051806200Z",
     "start_time": "2025-06-17T23:08:11.469275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_dict = system_data[\"min_max_dict\"]\n",
    "min_max_dict[\"x_max\"] = np.array([267.22485424, 309.39633111,  59.30922216, 176.90730552,\n",
    "         2.93078497,   3.25879914,   2.90161768,   5.05992019,\n",
    "         8.006078  ])\n",
    "min_max_dict[\"x_min\"] = np.array([-2.45841151e+02, -4.42441998e+03, -7.59124687e+01, -2.66399529e+03,\n",
    "       -2.63938070e+00, -2.98235548e+00, -2.66964174e+00, -5.38486567e+00,\n",
    "       -1.00988888e+02])"
   ],
   "id": "2f816e491df4c70",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.066976300Z",
     "start_time": "2025-06-17T23:08:11.999781Z"
    }
   },
   "cell_type": "code",
   "source": "min_max_dict",
   "id": "dff5ecebf376a4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_max': array([267.22485424, 309.39633111,  59.30922216, 176.90730552,\n",
       "          2.93078497,   3.25879914,   2.90161768,   5.05992019,\n",
       "          8.006078  ]),\n",
       " 'x_min': array([-2.45841151e+02, -4.42441998e+03, -7.59124687e+01, -2.66399529e+03,\n",
       "        -2.63938070e+00, -2.98235548e+00, -2.66964174e+00, -5.38486567e+00,\n",
       "        -1.00988888e+02]),\n",
       " 'y_sp_min': array([-3.11304008, -3.33251984]),\n",
       " 'y_sp_max': array([2.75198906, 1.7855982 ]),\n",
       " 'u_max': array([9.96, 7.3 ]),\n",
       " 'u_min': array([-10. ,  -7.5])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting The hyperparameters for the TD3 Agent",
   "id": "9b6fc672682c9d1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.105270600Z",
     "start_time": "2025-06-17T23:08:13.523433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# class Agent(object):\n",
    "#     def __init__(self,\n",
    "#                  state_dim: int,\n",
    "#                  action_dim: int,\n",
    "#                  actor_layer_sizes: list,\n",
    "#                  critic_layer_sizes: list,\n",
    "#                  buffer_capacity: int,\n",
    "#                  actor_lr: float,\n",
    "#                  critic_lr: float,\n",
    "#                  target_policy_smoothing_noise_std: float,\n",
    "#                  noise_clip: float,\n",
    "#                  exploration_noise_std: float,\n",
    "#                  gamma: float,\n",
    "#                  tau: float,\n",
    "#                  max_action: float,\n",
    "#                  policy_delay: int,\n",
    "#                  device):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.actor_layer_sizes = actor_layer_sizes\n",
    "#         self.critic_layer_sizes = critic_layer_sizes\n",
    "#         self.buffer_capacity = buffer_capacity\n",
    "#\n",
    "#         self.actor_lr = actor_lr\n",
    "#         self.critic_lr = critic_lr\n",
    "#         self.t_std = target_policy_smoothing_noise_std\n",
    "#         self.noise_clip = noise_clip\n",
    "#         self.exploration_noise_std = exploration_noise_std\n",
    "#         self.gamma = gamma\n",
    "#         self.tau = tau\n",
    "#\n",
    "#         self.max_action = max_action\n",
    "#         self.policy_delay = policy_delay\n",
    "#         self.total_it = 0\n",
    "#\n",
    "#         self.device = device\n",
    "#\n",
    "#         self.actor = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "#         self.actor_target = Actor(self.state_dim, self.action_dim, self.actor_layer_sizes).to(self.device)\n",
    "#\n",
    "#         for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "#             target_param.data.copy_(param.data)\n",
    "#\n",
    "#         self.critic = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "#         self.critic_target = Critic(self.state_dim, self.action_dim, self.critic_layer_sizes).to(self.device)\n",
    "#\n",
    "#         for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "#             target_param.data.copy_(param.data)\n",
    "#\n",
    "#         self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "#         self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "#\n",
    "#         self.replay_buffer = ReplayBuffer(self.buffer_capacity, self.state_dim, self.action_dim)\n",
    "#         # self.loss_fn = nn.MSELoss()\n",
    "#\n",
    "#         # Let's implement Huber loss\n",
    "#         self.loss_fn = nn.SmoothL1Loss()\n",
    "#\n",
    "#         # Defining the lists for losses storage for later visualization\n",
    "#         self.critic_losses_pretrain = []\n",
    "#         self.actor_losses_pretrain = []\n",
    "#         self.critic_losses = []\n",
    "#         self.actor_losses = []\n",
    "#\n",
    "#         self.scheduler_actor = StepLR(self.actor_optimizer, step_size=150, gamma=0.1)\n",
    "#         self.scheduler_critic = StepLR(self.critic_optimizer, step_size=150, gamma=0.1)\n",
    "#\n",
    "#         self.buffer_save = False\n",
    "#\n",
    "#         self.exploration_noise_std_min = exploration_noise_std * 0.1\n",
    "#         self.exploration_noise_std_initial = exploration_noise_std\n",
    "#         self.decay_rate = 0.99995\n",
    "#\n",
    "#         self.warm_start = True\n",
    "#\n",
    "#     def pre_train_entire_data(self, path, data_loader, epochs):\n",
    "#\n",
    "#         scaler = GradScaler('cuda')  # Initialize GradScaler for mixed precision training\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "#         save_path = os.path.join(path, \"PLots\", timestamp)\n",
    "#         if not os.path.exists(save_path):\n",
    "#             os.makedirs(save_path)\n",
    "#\n",
    "#         for epoch in range(epochs):\n",
    "#             loss_critic_epoch = 0.0\n",
    "#             loss_actor_epoch = 0.0\n",
    "#\n",
    "#             for batch_idx, (states_batch, actions_batch, rewards_batch, next_states_batch) in enumerate(data_loader):\n",
    "#                 # Move data to GPU\n",
    "#                 states_batch = states_batch.to(self.device)\n",
    "#                 actions_batch = actions_batch.to(self.device)\n",
    "#                 rewards_batch = rewards_batch.to(self.device)\n",
    "#                 next_states_batch = next_states_batch.to(self.device)\n",
    "#                 batch_size = states_batch.size(0)\n",
    "#\n",
    "#                 # Compute target Q-value without tracking gradients\n",
    "#                 with torch.no_grad():\n",
    "#                     noise = (self.t_std * torch.randn(batch_size, self.action_dim, device=self.device)).clamp(\n",
    "#                         -self.noise_clip, self.noise_clip)\n",
    "#                     next_actions = (self.actor_target(next_states_batch) + noise).clamp(-self.max_action,\n",
    "#                                                                                         self.max_action)\n",
    "#                     target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "#                     target_Q = torch.min(target_Q1, target_Q2)\n",
    "#                     target_value = rewards_batch + self.gamma * target_Q\n",
    "#\n",
    "#                 # --- Critic Update with Mixed Precision ---\n",
    "#                 with autocast('cuda'):\n",
    "#                     current_Q1, current_Q2 = self.critic(states_batch, actions_batch)\n",
    "#                     critic_loss = self.loss_fn(current_Q1, target_value) + self.loss_fn(current_Q2, target_value)\n",
    "#                 self.critic_optimizer.zero_grad()\n",
    "#                 scaler.scale(critic_loss).backward()\n",
    "#                 scaler.step(self.critic_optimizer)\n",
    "#                 loss_critic_epoch += critic_loss.item() * batch_size\n",
    "#\n",
    "#                 # --- Actor Update with Mixed Precision ---\n",
    "#                 with autocast('cuda'):\n",
    "#                     # For demonstration, we use MSE between actions and actor outputs.\n",
    "#                     # (You might instead use the critic’s Q-value as in standard TD3.)\n",
    "#                     actor_output = self.actor(states_batch)\n",
    "#                     actor_loss = self.loss_fn(actions_batch, actor_output)\n",
    "#                 self.actor_optimizer.zero_grad()\n",
    "#                 scaler.scale(actor_loss).backward()\n",
    "#                 scaler.step(self.actor_optimizer)\n",
    "#                 loss_actor_epoch += actor_loss.item() * batch_size\n",
    "#\n",
    "#                 scaler.update()\n",
    "#\n",
    "#             # Average losses over the epoch\n",
    "#             num_samples = len(data_loader.dataset)\n",
    "#             loss_critic_epoch /= num_samples\n",
    "#             loss_actor_epoch /= num_samples\n",
    "#\n",
    "#             self.critic_losses_pretrain.append(loss_critic_epoch)\n",
    "#             self.actor_losses_pretrain.append(loss_actor_epoch)\n",
    "#\n",
    "#             if epoch == 0 or epoch % 10 == 9:\n",
    "#                 print(f\"{datetime.datetime.now()} Epoch {epoch + 1},\"\n",
    "#                       f\" Actor Loss: {loss_actor_epoch},\"\n",
    "#                       f\" Critic Loss: {loss_critic_epoch}\")\n",
    "#             if epoch == 0 or epoch % 100 == 99:\n",
    "#                 current_lr_cr = self.critic_optimizer.param_groups[0]['lr']\n",
    "#                 print(f\"Epoch {epoch + 1}, Learning rate (Critic): {current_lr_cr}\")\n",
    "#                 current_lr_ac = self.actor_optimizer.param_groups[0]['lr']\n",
    "#                 print(f\"Epoch {epoch + 1}, Learning rate (Actor): {current_lr_ac}\")\n",
    "#\n",
    "#             if epoch % 100 == 0:\n",
    "#                 if epoch > 1:\n",
    "#                     # Exclude the first `plot_start` losses to get a clearer plot of later epochs\n",
    "#                     if epoch < 110:\n",
    "#                         plot_start = 0\n",
    "#                     else:\n",
    "#                         plot_start = epoch - 100\n",
    "#                     critic_losses_plot = self.critic_losses_pretrain[plot_start:]\n",
    "#                     actor_losses_plot = self.actor_losses_pretrain[plot_start:]\n",
    "#\n",
    "#                     fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "#\n",
    "#                     # Plot Critic Loss\n",
    "#                     axs[0].plot(range(plot_start + 1, len(critic_losses_plot) + 1 + plot_start), critic_losses_plot,\n",
    "#                                 label=\"Critic Loss\", color='r', linewidth=2)\n",
    "#                     axs[0].set_title(\"Critic Loss (Pre-train)\", fontsize=16)\n",
    "#                     axs[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "#                     axs[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "#                     axs[0].grid(True)\n",
    "#                     axs[0].legend()\n",
    "#\n",
    "#                     # Plot Actor Loss\n",
    "#                     axs[1].plot(range(plot_start + 1, len(actor_losses_plot) + 1 + plot_start), actor_losses_plot,\n",
    "#                                 label=\"Actor Loss\", color='b', linewidth=2)\n",
    "#                     axs[1].set_title(\"Actor Loss (Pre-train)\", fontsize=16)\n",
    "#                     axs[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "#                     axs[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "#                     axs[1].grid(True)\n",
    "#                     axs[1].legend()\n",
    "#\n",
    "#                     # Show the plot\n",
    "#                     plt.tight_layout()\n",
    "#\n",
    "#                     filename = os.path.join(save_path, f\"{plot_start}_{epoch}.png\")\n",
    "#                     plt.savefig(filename)\n",
    "#                     plt.close(fig)\n",
    "#\n",
    "#             # Update the learning rate schedulers at the end of the epoch\n",
    "#             self.scheduler_actor.step()\n",
    "#             self.scheduler_critic.step()\n",
    "#\n",
    "#\n",
    "#     def pre_train(self, path, epochs, n_samples=100, log_interval=1000):\n",
    "#\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "#         path = os.path.join(path, \"Pre_training_plots\", timestamp)\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "#\n",
    "#         # 1) set up AMP scaler\n",
    "#         scaler = GradScaler()\n",
    "#\n",
    "#         # logging time\n",
    "#         t0 = time.perf_counter()\n",
    "#\n",
    "#         for epoch in range(epochs):\n",
    "#\n",
    "#             # 1_1) sample a mini‐batch\n",
    "#             states_batch, actions_batch, rewards_batch, next_states_batch = \\\n",
    "#                 self.replay_buffer.sample_pretrain(n_samples, device=self.device)\n",
    "#\n",
    "#             # 2) cast everything to float32 on the correct device\n",
    "#             states_batch      = states_batch.to(self.device, dtype=torch.float32)\n",
    "#             actions_batch     = actions_batch.to(self.device, dtype=torch.float32)\n",
    "#             rewards_batch     = rewards_batch.to(self.device, dtype=torch.float32)\n",
    "#             next_states_batch = next_states_batch.to(self.device, dtype=torch.float32)\n",
    "#\n",
    "#             # 2) compute targets in mixed precision\n",
    "#             with torch.no_grad(), autocast(device_type=str(self.device)):\n",
    "#                 # noise for target policy\n",
    "#                 noise = (\n",
    "#                     self.t_std\n",
    "#                     * torch.randn(n_samples, self.action_dim, device=self.device, dtype=torch.float32)\n",
    "#                 ).clamp(-self.noise_clip, self.noise_clip)\n",
    "#\n",
    "#                 next_actions = (self.actor_target(next_states_batch) + noise).clamp(-1, 1)\n",
    "#                 target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "#                 target_Q = torch.min(target_Q1, target_Q2)\n",
    "#                 r = rewards_batch + self.gamma * target_Q\n",
    "#\n",
    "#             # 3) critic update\n",
    "#             self.critic_optimizer.zero_grad()\n",
    "#             with autocast(device_type=str(self.device)):\n",
    "#                 q1, q2 = self.critic(states_batch, actions_batch)\n",
    "#                 critic_loss = self.loss_fn(q1, r) + self.loss_fn(q2, r)\n",
    "#             scaler.scale(critic_loss).backward()\n",
    "#             scaler.step(self.critic_optimizer)\n",
    "#             scaler.update()\n",
    "#             loss_critic = critic_loss.item()\n",
    "#\n",
    "#             # 4) actor update (delayed if you use policy_delay logic)\n",
    "#             self.actor_optimizer.zero_grad()\n",
    "#             with autocast(device_type=str(self.device)):\n",
    "#                 # imitation Learning\n",
    "#                 actor_out = self.actor(states_batch)\n",
    "#                 actor_loss = self.loss_fn(actions_batch, actor_out)\n",
    "#             scaler.scale(actor_loss).backward()\n",
    "#             scaler.step(self.actor_optimizer)\n",
    "#             scaler.update()\n",
    "#             loss_actor = actor_loss.item()\n",
    "#\n",
    "#\n",
    "#             # 5) soft‐update targets\n",
    "#             for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "#                 target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#             for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "#                 target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#\n",
    "#\n",
    "#             # 6) logging\n",
    "#             self.critic_losses_pretrain.append(loss_critic)\n",
    "#             self.actor_losses_pretrain.append(loss_actor)\n",
    "#\n",
    "#             if epoch == 0 or epoch % log_interval  == (log_interval - 1):\n",
    "#                 print(f\"[Pre-train] Step {epoch+1}/{epochs} |\"\n",
    "#                       f\"Computation Time: {time.perf_counter() - t0:.3f}s |\"\n",
    "#                       f\"Actor Loss: {loss_actor:.6e} |\"\n",
    "#                       f\"Critic Loss: {loss_critic:.6e}\")\n",
    "#                 # logging time\n",
    "#                 t0 = time.perf_counter()\n",
    "#\n",
    "#             # 7) occasional plotting\n",
    "#             if epoch % 2500 == 0:\n",
    "#                 if epoch > 1:\n",
    "#                     if epoch < 1100:\n",
    "#                         plot_start = 0\n",
    "#                     else:\n",
    "#                         plot_start = epoch - 2500\n",
    "#                     critic_losses_plot = self.critic_losses_pretrain[plot_start:]\n",
    "#                     actor_losses_plot = self.actor_losses_pretrain[plot_start:]\n",
    "#\n",
    "#                     fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "#\n",
    "#                     # Plot Critic Loss\n",
    "#                     axs[0].plot(range(plot_start + 1, len(critic_losses_plot) + 1 + plot_start), critic_losses_plot,\n",
    "#                                 label=\"Critic Loss\", color='r', linewidth=2)\n",
    "#                     axs[0].set_title(\"Critic Loss (Pre-train)\", fontsize=16)\n",
    "#                     axs[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "#                     axs[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "#                     axs[0].grid(True)\n",
    "#                     axs[0].legend()\n",
    "#\n",
    "#                     # Plot Actor Loss\n",
    "#                     axs[1].plot(range(plot_start + 1, len(actor_losses_plot) + 1 + plot_start), actor_losses_plot,\n",
    "#                                 label=\"Actor Loss\", color='b', linewidth=2)\n",
    "#                     axs[1].set_title(\"Actor Loss (Pre-train)\", fontsize=16)\n",
    "#                     axs[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "#                     axs[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "#                     axs[1].grid(True)\n",
    "#                     axs[1].legend()\n",
    "#\n",
    "#                     # Show the plot\n",
    "#                     plt.tight_layout()\n",
    "#\n",
    "#                     filename = os.path.join(path, f\"{plot_start}_{epoch}.png\")\n",
    "#                     plt.savefig(filename)\n",
    "#                     plt.close(fig)\n",
    "#\n",
    "#\n",
    "#     def train_online(self, n_samples=100):\n",
    "#\n",
    "#         self.buffer_save = True\n",
    "#\n",
    "#         # (states_batch, actions_batch,\n",
    "#         #  rewards_batch, next_states_batch) = self.replay_buffer.sample(n_samples, device=self.device)\n",
    "#         (states_batch, actions_batch,\n",
    "#          rewards_batch, next_states_batch) = self.replay_buffer.sample_pretrain(n_samples, device=self.device)\n",
    "#\n",
    "#         loss_critic = 0\n",
    "#         loss_actor = 0\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             noise = (self.t_std * np.random.randn(n_samples,\n",
    "#                                                   self.action_dim)).clip(\n",
    "#                 -self.noise_clip, self.noise_clip)\n",
    "#             noise = torch.from_numpy(noise).to(dtype=torch.float32, device=self.device)\n",
    "#             next_actions = (self.actor_target(next_states_batch) + noise).clip(-1, 1)\n",
    "#\n",
    "#             target_Q1, target_Q2 = self.critic_target(next_states_batch, next_actions)\n",
    "#             target_Q = torch.min(target_Q1, target_Q2)\n",
    "#\n",
    "#             r = rewards_batch + self.gamma * target_Q\n",
    "#\n",
    "#         q1, q2 = self.critic(states_batch, actions_batch)\n",
    "#\n",
    "#         loss = self.loss_fn(q1, r) + self.loss_fn(q2, r)\n",
    "#\n",
    "#         self.critic_optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.critic_optimizer.step()\n",
    "#         loss_critic += loss.item()\n",
    "#         self.critic_losses.append(loss.item())\n",
    "#\n",
    "#         # if self.total_it % self.policy_delay == 0:\n",
    "#         actions = self.actor(states_batch)\n",
    "#         q = self.critic.q1_forward(states_batch, actions)\n",
    "#\n",
    "#         actor_loss = - torch.mean(q)\n",
    "#\n",
    "#         self.actor_optimizer.zero_grad()\n",
    "#         actor_loss.backward()\n",
    "#         self.actor_optimizer.step()\n",
    "#         loss_actor += actor_loss.item()\n",
    "#         self.actor_losses.append(actor_loss.item())\n",
    "#\n",
    "#         for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "#             target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#\n",
    "#         for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "#             target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#\n",
    "#         # print(f'Critic loss: {self.critic_losses[-1]}')\n",
    "#         # print(f'Actor loss: {self.actor_losses[-1]}')\n",
    "#         self.total_it += 1\n",
    "#\n",
    "#         self.decay_exploration()\n",
    "#\n",
    "#     def decay_exploration(self):\n",
    "#\n",
    "#         self.exploration_noise_std = max(\n",
    "#             self.exploration_noise_std_min,\n",
    "#             self.exploration_noise_std_initial * (self.decay_rate ** self.total_it)\n",
    "#         )\n",
    "#\n",
    "#     def take_action(self, state, explore=False):\n",
    "#         state = state if isinstance(state, torch.Tensor) else torch.from_numpy(state).to(dtype=torch.float32,\n",
    "#                                                                                          device=self.device)\n",
    "#\n",
    "#         # If warm start is active, choose a random action from the action space.\n",
    "#         if self.warm_start:\n",
    "#             action = np.random.uniform(low=-self.max_action, high=self.max_action, size=self.action_dim)\n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 action = self.actor(state).detach().cpu().numpy()\n",
    "#             if explore:\n",
    "#                 action += np.random.randn(self.action_dim) * self.exploration_noise_std\n",
    "#             action = action.clip(-self.max_action, self.max_action)\n",
    "#\n",
    "#         return action\n",
    "#\n",
    "#     def save(self, path: str, name_prefix=\"agent\"):\n",
    "#         path = os.path.join(path, \"models\")\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "#         filename = os.path.join(path, f\"{name_prefix}_{timestamp}.pkl\")\n",
    "#\n",
    "#         save_dict = {\n",
    "#             'actor_state_dict': self.actor.state_dict(),\n",
    "#             'critic_state_dict': self.critic.state_dict(),\n",
    "#             'actor_target_state_dict': self.actor_target.state_dict(),\n",
    "#             'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "#             'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "#             'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "#             'agent_attributes': {key: value for key, value in self.__dict__.items() if key not in ['actor', 'critic',\n",
    "#                                                                                                    'actor_target',\n",
    "#                                                                                                    'critic_target',\n",
    "#                                                                                                    'replay_buffer',\n",
    "#                                                                                                    'total_it',\n",
    "#                                                                                                    'device',\n",
    "#                                                                                                    'loss_fn',\n",
    "#                                                                                                    'replay_buffer']}\n",
    "#         }\n",
    "#\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             pickle.dump(save_dict, f)\n",
    "#         print(f\"Agent saved successfully to {filename}\")\n",
    "#         return filename\n",
    "#\n",
    "#     def load(self, path: str):\n",
    "#         with open(path, 'rb') as f:\n",
    "#             loaded_dict = pickle.load(f)\n",
    "#\n",
    "#         self.actor.load_state_dict(loaded_dict['actor_state_dict'])\n",
    "#         self.critic.load_state_dict(loaded_dict['critic_state_dict'])\n",
    "#         self.actor_target.load_state_dict(loaded_dict['actor_target_state_dict'])\n",
    "#         self.critic_target.load_state_dict(loaded_dict['critic_target_state_dict'])\n",
    "#\n",
    "#         for key, value in loaded_dict['agent_attributes'].items():\n",
    "#             setattr(self, key, value)\n",
    "#\n",
    "#         self.actor_optimizer.load_state_dict(loaded_dict['actor_optimizer_state_dict'])\n",
    "#         self.critic_optimizer.load_state_dict(loaded_dict['critic_optimizer_state_dict'])\n",
    "#         self.actor_optimizer.param_groups[0]['params'] = list(self.actor.parameters())\n",
    "#         self.critic_optimizer.param_groups[0]['params'] = list(self.critic.parameters())\n",
    "#         self.actor_optimizer.param_groups[0]['lr'] = self.actor_lr # * 1e-3\n",
    "#         self.critic_optimizer.param_groups[0]['lr'] = self.critic_lr\n",
    "#\n",
    "#         # self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr * 1e-2)\n",
    "#         # self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "#\n",
    "#         for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "#             target_param.data.copy_(param.data)\n",
    "#\n",
    "#         for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "#             target_param.data.copy_(param.data)\n",
    "#\n",
    "#         print(f\"Agent loaded successfully from: {path}\")"
   ],
   "id": "b3733bcfb8ce6c89",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:16:57.130780800Z",
     "start_time": "2025-06-17T23:08:13.965755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class AgentPretrainer:\n",
    "#     def __init__(self, agent):\n",
    "#         \"\"\"\n",
    "#         agent: TD3 Agent, with .actor, .critic, .actor_target, .critic_target,\n",
    "#                .actor_optimizer, .critic_optimizer, .gamma, .tau, .t_std, .noise_clip\n",
    "#         buffer: replay_buffer with .sample_pretrain(batch_size, device)\n",
    "#         \"\"\"\n",
    "#         self.agent = agent\n",
    "#         self.buffer = agent.replay_buffer\n",
    "#         self.device = agent.device\n",
    "#         self.loss_fn = agent.loss_fn\n",
    "#         self.critic_optimizer = agent.critic_optimizer\n",
    "#         self.actor_optimizer = agent.actor_optimizer\n",
    "#\n",
    "#         self.bc_losses = []\n",
    "#         self.phase2_losses = []\n",
    "#         self.actor_losses = []\n",
    "#         self.critic_losses = []\n",
    "#\n",
    "#     def phase1_behavioral_cloning(self, epochs, batch_size, log_interval=100):\n",
    "#         \"\"\"Phase 1: freeze critic, train actor to mimic MPC actions via Huber (SmoothL1) loss.\"\"\"\n",
    "#         # 1) Freeze critic\n",
    "#         for p in self.agent.critic.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         for p in self.agent.actor.parameters():\n",
    "#             p.requires_grad = True\n",
    "#         self.agent.actor.train()\n",
    "#         self.agent.critic.eval()\n",
    "#\n",
    "#\n",
    "#         scaler = GradScaler()\n",
    "#\n",
    "#         for ep in range(1, epochs+1):\n",
    "#             # sample (s, u_mpc, _, _)\n",
    "#             states, actions, _, _ = self.buffer.sample_pretrain(batch_size, device=self.device)\n",
    "#             states  = states .to(self.device, dtype=torch.float32)\n",
    "#             targets = actions.to(self.device, dtype=torch.float32)\n",
    "#\n",
    "#             self.actor_optimizer.zero_grad()\n",
    "#             with autocast(device_type=str(self.device)):\n",
    "#                 preds = self.agent.actor(states)\n",
    "#                 loss  = self.loss_fn(preds, targets)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(self.actor_optimizer)\n",
    "#             scaler.update()\n",
    "#\n",
    "#             if ep % log_interval == 0 or ep == 1:\n",
    "#                 print(f\"[BC Phase] Epoch {ep}/{epochs}  Loss: {loss.item():.3e}\")\n",
    "#\n",
    "#             self.bc_losses.append(loss.item())\n",
    "#\n",
    "#     def phase2_critic_warmup(self, epochs, batch_size, log_interval=100):\n",
    "#         \"\"\"Phase 2: freeze actor, train critic on TD targets from MPC data.\"\"\"\n",
    "#         # 1) Freeze actor\n",
    "#         for p in self.agent.critic.parameters():\n",
    "#             p.requires_grad = True\n",
    "#         for p in self.agent.actor.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         self.agent.actor.eval()\n",
    "#         self.agent.critic.train()\n",
    "#\n",
    "#         scaler = GradScaler()\n",
    "#\n",
    "#         for ep in range(1, epochs+1):\n",
    "#             # sample (s, u_mpc, r, s')\n",
    "#             states, actions, rewards, next_states = \\\n",
    "#                 self.buffer.sample_pretrain(batch_size, device=self.device)\n",
    "#             states      = states     .to(self.device, dtype=torch.float32)\n",
    "#             actions     = actions    .to(self.device, dtype=torch.float32)\n",
    "#             rewards     = rewards    .to(self.device, dtype=torch.float32)\n",
    "#             next_states = next_states.to(self.device, dtype=torch.float32)\n",
    "#\n",
    "#             # build TD target in full precision\n",
    "#             with torch.no_grad():\n",
    "#                 noise = (\n",
    "#                     self.agent.t_std\n",
    "#                     * torch.randn(batch_size, self.agent.action_dim,\n",
    "#                                  device=self.device, dtype=torch.float32)\n",
    "#                 ).clamp(-self.agent.noise_clip, self.agent.noise_clip)\n",
    "#                 next_a = (self.agent.actor_target(next_states) + noise).clamp(-1, 1)\n",
    "#                 tQ1, tQ2  = self.agent.critic_target(next_states, next_a)\n",
    "#                 target_Q  = torch.min(tQ1, tQ2)\n",
    "#                 y_target  = rewards + self.agent.gamma * target_Q\n",
    "#\n",
    "#             # critic update\n",
    "#             self.critic_optimizer.zero_grad()\n",
    "#             with autocast(device_type=str(self.device)):\n",
    "#                 Q1, Q2 = self.agent.critic(states, actions)\n",
    "#                 loss   = self.loss_fn(Q1, y_target) + self.loss_fn(Q2, y_target)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(self.critic_optimizer)\n",
    "#             scaler.update()\n",
    "#\n",
    "#             if ep % log_interval == 0 or ep == 1:\n",
    "#                 print(f\"[Critic Phase] Epoch {ep}/{epochs}  Loss: {loss.item():.3e}\")\n",
    "#\n",
    "#             self.phase2_losses.append(loss.item())\n",
    "#\n",
    "#     def phase3_offline_td3(self, epochs, batch_size, policy_delay=4, log_interval=100, rl_factor=0, bc_factor=1):\n",
    "#         \"\"\"\n",
    "#         Phase 3: offline TD3 updates on filled buffer (still no env).\n",
    "#         This alternates critic & actor updates exactly as in TD3, sampling from buffer.\n",
    "#         \"\"\"\n",
    "#         # Unfreeze both\n",
    "#         for p in self.agent.actor.parameters():\n",
    "#             p.requires_grad = True\n",
    "#         for p in self.agent.critic.parameters():\n",
    "#             p.requires_grad = True\n",
    "#         self.agent.actor.train()\n",
    "#         self.agent.critic.train()\n",
    "#\n",
    "#         scaler = GradScaler()\n",
    "#\n",
    "#         for ep in range(1, epochs+1):\n",
    "#             # sample batch\n",
    "#             states, actions, rewards, next_states = \\\n",
    "#                 self.buffer.sample_pretrain(batch_size, device=self.device)\n",
    "#             states      = states     .to(self.device, dtype=torch.float32)\n",
    "#             actions     = actions    .to(self.device, dtype=torch.float32)\n",
    "#             rewards     = rewards    .to(self.device, dtype=torch.float32)\n",
    "#             next_states = next_states.to(self.device, dtype=torch.float32)\n",
    "#\n",
    "#             # 1) critic step\n",
    "#             with torch.no_grad():\n",
    "#                 noise = (\n",
    "#                     self.agent.t_std\n",
    "#                     * torch.randn(batch_size, self.agent.action_dim,\n",
    "#                                  device=self.device, dtype=torch.float32)\n",
    "#                 ).clamp(-self.agent.noise_clip, self.agent.noise_clip)\n",
    "#                 next_a = (self.agent.actor_target(next_states) + noise).clamp(-1, 1)\n",
    "#                 tQ1, tQ2 = self.agent.critic_target(next_states, next_a)\n",
    "#                 y_target = rewards + self.agent.gamma * torch.min(tQ1, tQ2)\n",
    "#\n",
    "#             self.critic_optimizer.zero_grad()\n",
    "#             with autocast(device_type=str(self.device)):\n",
    "#                 Q1, Q2 = self.agent.critic(states, actions)\n",
    "#                 loss_c = self.loss_fn(Q1, y_target) + self.loss_fn(Q2, y_target)\n",
    "#             scaler.scale(loss_c).backward()\n",
    "#             scaler.step(self.critic_optimizer)\n",
    "#             scaler.update()\n",
    "#             self.critic_losses.append(loss_c.item())\n",
    "#\n",
    "#             # 2) actor step (delayed)\n",
    "#             if ep % policy_delay == 0:\n",
    "#                 self.actor_optimizer.zero_grad()\n",
    "#                 with autocast(device_type=str(self.device)):\n",
    "#                     a_pred = self.agent.actor(states)\n",
    "#                     # 1) RL term: maximize Q1 -> minimize neg-Q1\n",
    "#                     q_val, _  = self.agent.critic(states, a_pred)\n",
    "#                     rl_loss = -q_val.mean()\n",
    "#                     # 2) BC term: keep close to u_MPC\n",
    "#                     bc_loss = self.loss_fn(a_pred, actions)\n",
    "#                     # 3) combined\n",
    "#                     loss_a = rl_factor * rl_loss + bc_factor * bc_loss\n",
    "#\n",
    "#                 scaler.scale(loss_a).backward()\n",
    "#                 scaler.step(self.actor_optimizer)\n",
    "#                 scaler.update()\n",
    "#                 self.actor_losses.append(loss_a.item())\n",
    "#\n",
    "#             # 3) soft target‐update\n",
    "#             for p, tp in zip(self.agent.actor.parameters(), self.agent.actor_target.parameters()):\n",
    "#                 tp.data.copy_(self.agent.tau * p.data + (1-self.agent.tau)*tp.data)\n",
    "#             for p, tp in zip(self.agent.critic.parameters(), self.agent.critic_target.parameters()):\n",
    "#                 tp.data.copy_(self.agent.tau * p.data + (1-self.agent.tau)*tp.data)\n",
    "#\n",
    "#             if ep % log_interval == 0 or ep == 1:\n",
    "#                 print(f\"[TD3 Phase] Step {ep}/{epochs}  Critic Loss: {loss_c.item():.3e}\"\n",
    "#                       + (f\", Actor Loss: {loss_a.item():.3e}\" if ep % policy_delay==0 else \"\"))"
   ],
   "id": "f075e2d0c28843ec",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:14.592042Z",
     "start_time": "2025-06-17T23:08:14.572139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_points_number = int(C_aug.shape[0])\n",
    "inputs_number = int(B_aug.shape[1])\n",
    "STATE_DIM = int(A_aug.shape[0]) + set_points_number + inputs_number\n",
    "ACTION_DIM = int(B_aug.shape[1])\n",
    "ACTOR_LAYER_SIZES = [2048, 2048, 2048]\n",
    "CRITIC_LAYER_SIZES = [2048, 2048, 2048]\n",
    "BUFFER_CAPACITY = 10_000_000\n",
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "SMOOTHING_STD = 0.000001\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORATION_NOISE_STD = 0.1\n",
    "GAMMA = 0.95\n",
    "TAU = 0.005\n",
    "MAX_ACTION = 1\n",
    "POLICY_DELAY = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "BATCH_SIZE = 256\n",
    "NUM_BATCHES = 1"
   ],
   "id": "daf58940b5cb9814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:16.017356Z",
     "start_time": "2025-06-17T23:08:15.138970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent\n",
    "agent = Agent(\n",
    "    STATE_DIM,\n",
    "    ACTION_DIM,\n",
    "    ACTOR_LAYER_SIZES,\n",
    "    CRITIC_LAYER_SIZES,\n",
    "    BUFFER_CAPACITY,\n",
    "    ACTOR_LR,\n",
    "    CRITIC_LR,\n",
    "    SMOOTHING_STD,\n",
    "    NOISE_CLIP,\n",
    "    EXPLORATION_NOISE_STD,\n",
    "    GAMMA,\n",
    "    TAU,\n",
    "    MAX_ACTION,\n",
    "    POLICY_DELAY,\n",
    "    DEVICE\n",
    ")"
   ],
   "id": "dbc7a407cef272a0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:16.023535Z",
     "start_time": "2025-06-17T23:08:16.020905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPC parameters\n",
    "predict_h = 9\n",
    "cont_h = 3\n",
    "b1 = (b_min[0], b_max[0])\n",
    "b2 = (b_min[1], b_max[1])\n",
    "bnds = (b1, b2)*cont_h\n",
    "cons = []\n",
    "IC_opt = np.zeros(inputs_number*cont_h)\n",
    "Q1 = 5\n",
    "Q2 = 1\n",
    "R1 = 1\n",
    "R2 = 1\n",
    "Q_rew = np.array([[12, 0], [0, 8]])\n",
    "R_rew = np.array([[1, 0], [0, 1]])"
   ],
   "id": "c1c13a104e1edc77",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:16.433326Z",
     "start_time": "2025-06-17T23:08:16.430917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MPC_obj = MpcSolver(A_aug, B_aug, C_aug,\n",
    "                    Q1, Q2, R1, R2,\n",
    "                    predict_h, cont_h)"
   ],
   "id": "3020ae61ebbd3a4f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:56:28.998358Z",
     "start_time": "2025-05-11T19:56:28.995927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steady_states_samples_number = 100000\n",
    "mpc_pretrain_samples_numbers = BUFFER_CAPACITY - steady_states_samples_number"
   ],
   "id": "e103bf85c8ac0549",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T04:48:12.830569Z",
     "start_time": "2025-05-10T03:45:52.937176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filling_the_buffer(\n",
    "        min_max_dict,\n",
    "        A_aug, B_aug, C_aug,\n",
    "        MPC_obj,\n",
    "        mpc_pretrain_samples_numbers,\n",
    "        Q_rew, R_rew,\n",
    "        agent,\n",
    "        IC_opt, bnds, cons, chunk_size= 100_000)"
   ],
   "id": "422899593ce0dbde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/99\n",
      "Processing chunk 2/99\n",
      "Processing chunk 3/99\n",
      "Processing chunk 4/99\n",
      "Processing chunk 5/99\n",
      "Processing chunk 6/99\n",
      "Processing chunk 7/99\n",
      "Processing chunk 8/99\n",
      "Processing chunk 9/99\n",
      "Processing chunk 10/99\n",
      "Processing chunk 11/99\n",
      "Processing chunk 12/99\n",
      "Processing chunk 13/99\n",
      "Processing chunk 14/99\n",
      "Processing chunk 15/99\n",
      "Processing chunk 16/99\n",
      "Processing chunk 17/99\n",
      "Processing chunk 18/99\n",
      "Processing chunk 19/99\n",
      "Processing chunk 20/99\n",
      "Processing chunk 21/99\n",
      "Processing chunk 22/99\n",
      "Processing chunk 23/99\n",
      "Processing chunk 24/99\n",
      "Processing chunk 25/99\n",
      "Processing chunk 26/99\n",
      "Processing chunk 27/99\n",
      "Processing chunk 28/99\n",
      "Processing chunk 29/99\n",
      "Processing chunk 30/99\n",
      "Processing chunk 31/99\n",
      "Processing chunk 32/99\n",
      "Processing chunk 33/99\n",
      "Processing chunk 34/99\n",
      "Processing chunk 35/99\n",
      "Processing chunk 36/99\n",
      "Processing chunk 37/99\n",
      "Processing chunk 38/99\n",
      "Processing chunk 39/99\n",
      "Processing chunk 40/99\n",
      "Processing chunk 41/99\n",
      "Processing chunk 42/99\n",
      "Processing chunk 43/99\n",
      "Processing chunk 44/99\n",
      "Processing chunk 45/99\n",
      "Processing chunk 46/99\n",
      "Processing chunk 47/99\n",
      "Processing chunk 48/99\n",
      "Processing chunk 49/99\n",
      "Processing chunk 50/99\n",
      "Processing chunk 51/99\n",
      "Processing chunk 52/99\n",
      "Processing chunk 53/99\n",
      "Processing chunk 54/99\n",
      "Processing chunk 55/99\n",
      "Processing chunk 56/99\n",
      "Processing chunk 57/99\n",
      "Processing chunk 58/99\n",
      "Processing chunk 59/99\n",
      "Processing chunk 60/99\n",
      "Processing chunk 61/99\n",
      "Processing chunk 62/99\n",
      "Processing chunk 63/99\n",
      "Processing chunk 64/99\n",
      "Processing chunk 65/99\n",
      "Processing chunk 66/99\n",
      "Processing chunk 67/99\n",
      "Processing chunk 68/99\n",
      "Processing chunk 69/99\n",
      "Processing chunk 70/99\n",
      "Processing chunk 71/99\n",
      "Processing chunk 72/99\n",
      "Processing chunk 73/99\n",
      "Processing chunk 74/99\n",
      "Processing chunk 75/99\n",
      "Processing chunk 76/99\n",
      "Processing chunk 77/99\n",
      "Processing chunk 78/99\n",
      "Processing chunk 79/99\n",
      "Processing chunk 80/99\n",
      "Processing chunk 81/99\n",
      "Processing chunk 82/99\n",
      "Processing chunk 83/99\n",
      "Processing chunk 84/99\n",
      "Processing chunk 85/99\n",
      "Processing chunk 86/99\n",
      "Processing chunk 87/99\n",
      "Processing chunk 88/99\n",
      "Processing chunk 89/99\n",
      "Processing chunk 90/99\n",
      "Processing chunk 91/99\n",
      "Processing chunk 92/99\n",
      "Processing chunk 93/99\n",
      "Processing chunk 94/99\n",
      "Processing chunk 95/99\n",
      "Processing chunk 96/99\n",
      "Processing chunk 97/99\n",
      "Processing chunk 98/99\n",
      "Processing chunk 99/99\n",
      "Replay buffer has been filled with generated samples.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T04:48:19.065812Z",
     "start_time": "2025-05-10T04:48:12.853788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_steady_state_samples(\n",
    "        min_max_dict,\n",
    "        A_aug, B_aug, C_aug,\n",
    "        MPC_obj,\n",
    "        steady_states_samples_number,\n",
    "        Q_rew, R_rew,\n",
    "        agent,\n",
    "        IC_opt, bnds, cons, chunk_size= 100000)"
   ],
   "id": "e4a5c2ef20eea4b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/1\n",
      "Replay buffer has been filled up with the steady_state values.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving and Loading the Replay Buffer to make sure we have the saved replay buffer",
   "id": "1a364467c94a80f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T04:48:19.663717Z",
     "start_time": "2025-05-10T04:48:19.070685Z"
    }
   },
   "cell_type": "code",
   "source": "filename_buffer = agent.replay_buffer.save(dir_path)",
   "id": "7c3672c490f2eb42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer saved to C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\replay_buffer_2505100048.h5\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:19.164403Z",
     "start_time": "2025-06-17T23:08:19.162297Z"
    }
   },
   "cell_type": "code",
   "source": "replay_buffer = ReplayBuffer(BUFFER_CAPACITY, STATE_DIM, ACTION_DIM)",
   "id": "bf7e93e1bfe48a34",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:20.192019Z",
     "start_time": "2025-06-17T23:08:19.523207Z"
    }
   },
   "cell_type": "code",
   "source": "replay_buffer.load(r\"C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\replay_buffer_2505100048.h5\")",
   "id": "5475a49a2aa9ea78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer loaded from C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\replay_buffer_2505100048.h5\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:20.268982Z",
     "start_time": "2025-06-17T23:08:20.266475Z"
    }
   },
   "cell_type": "code",
   "source": "agent.replay_buffer = replay_buffer",
   "id": "b94c1c6074f96715",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:20.283896Z",
     "start_time": "2025-06-17T23:08:20.282431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mu = agent.replay_buffer.rewards.mean()\n",
    "# sigma = agent.replay_buffer.rewards.std() + 1e-6\n",
    "# r_norm = (agent.replay_buffer.rewards - mu)/sigma\n",
    "# r_clipped = np.clip(r_norm,-5.0,+5.0)\n",
    "# agent.replay_buffer.rewards = r_clipped"
   ],
   "id": "678b4e6f9c903d65",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre training the Agent",
   "id": "5738bba63f39b443"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:21.046015Z",
     "start_time": "2025-06-17T23:08:20.962891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "# Clear memory before DataLoader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "3f0870bb0401a8d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:08:22.387764Z",
     "start_time": "2025-06-17T23:08:22.384846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ReplayDataset(torch.from_numpy(replay_buffer.states).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.actions).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.rewards).to(dtype=torch.float32),\n",
    "                        torch.from_numpy(replay_buffer.next_states).to(dtype=torch.float32))\n",
    "data_loader = DataLoader(dataset, batch_size=2056, shuffle=True, num_workers=10, pin_memory=True)"
   ],
   "id": "8f1d8d24a31d46c0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:16:32.374534Z",
     "start_time": "2025-05-12T16:16:32.373119Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "49922dd2fbfad059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T23:10:09.539436Z",
     "start_time": "2025-06-17T23:08:23.986944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pretraining\n",
    "EPOCHS_FOR_PRETRAIN = 1300\n",
    "\n",
    "agent.pre_train_entire_data(dir_path, data_loader, EPOCHS_FOR_PRETRAIN)"
   ],
   "id": "db6699689be81502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 19:09:13.817925 Epoch 1, Actor Loss: 0.0008212525874044106, Critic Loss: 5427385.012223375\n",
      "Epoch 1, Learning rate (Critic): 0.0001\n",
      "Epoch 1, Learning rate (Actor): 0.0001\n",
      "2025-06-17 19:10:03.324002 Epoch 2, Actor Loss: 5.354618446895038e-05, Critic Loss: 4107.190626141406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Pretraining\u001B[39;00m\n\u001B[32m      2\u001B[39m EPOCHS_FOR_PRETRAIN = \u001B[32m1300\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpre_train_entire_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdir_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS_FOR_PRETRAIN\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 132\u001B[39m, in \u001B[36mAgent.pre_train_entire_data\u001B[39m\u001B[34m(self, path, data_loader, epochs)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;66;03m# --- Actor Update with Mixed Precision ---\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m autocast(\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m     actor_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mactor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m     actor_loss = \u001B[38;5;28mself\u001B[39m.loss_fn(actions_batch, actor_output)\n\u001B[32m    135\u001B[39m \u001B[38;5;28mself\u001B[39m.actor_optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\TD3Agent\\actor.py:26\u001B[39m, in \u001B[36mActor.forward\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: torch.Tensor):\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-11T00:25:51.471893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------------------------------\n",
    "# offline only training\n",
    "# -------------------------------------------------------\n",
    "agent_pre = AgentPretrainer(agent=agent)\n",
    "\n",
    "# Phase 1: Behavioral cloning\n",
    "agent_pre.phase1_behavioral_cloning(epochs=300000, batch_size=2048)\n",
    "\n",
    "# Phase 2: Critic warm-up\n",
    "agent_pre.phase2_critic_warmup(epochs=100000, batch_size=2048)\n",
    "\n",
    "# Phase 3: Offline TD3 fine-tuning\n",
    "agent_pre.phase3_offline_td3(epochs=100000, batch_size=2048)"
   ],
   "id": "1169e9e8f4a6359c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BC Phase] Epoch 1/300000  Loss: 1.620e-01\n",
      "[BC Phase] Epoch 100/300000  Loss: 1.689e-03\n",
      "[BC Phase] Epoch 200/300000  Loss: 1.367e-03\n",
      "[BC Phase] Epoch 300/300000  Loss: 4.601e-04\n",
      "[BC Phase] Epoch 400/300000  Loss: 2.340e-04\n",
      "[BC Phase] Epoch 500/300000  Loss: 2.088e-04\n",
      "[BC Phase] Epoch 600/300000  Loss: 1.411e-04\n",
      "[BC Phase] Epoch 700/300000  Loss: 1.313e-04\n",
      "[BC Phase] Epoch 800/300000  Loss: 1.107e-04\n",
      "[BC Phase] Epoch 900/300000  Loss: 1.001e-04\n",
      "[BC Phase] Epoch 1000/300000  Loss: 9.610e-05\n",
      "[BC Phase] Epoch 1100/300000  Loss: 1.010e-04\n",
      "[BC Phase] Epoch 1200/300000  Loss: 8.791e-05\n",
      "[BC Phase] Epoch 1300/300000  Loss: 7.880e-05\n",
      "[BC Phase] Epoch 1400/300000  Loss: 1.071e-04\n",
      "[BC Phase] Epoch 1500/300000  Loss: 6.732e-05\n",
      "[BC Phase] Epoch 1600/300000  Loss: 6.970e-05\n",
      "[BC Phase] Epoch 1700/300000  Loss: 7.588e-05\n",
      "[BC Phase] Epoch 1800/300000  Loss: 7.007e-05\n",
      "[BC Phase] Epoch 1900/300000  Loss: 7.516e-05\n",
      "[BC Phase] Epoch 2000/300000  Loss: 7.875e-05\n",
      "[BC Phase] Epoch 2100/300000  Loss: 8.138e-05\n",
      "[BC Phase] Epoch 2200/300000  Loss: 5.691e-05\n",
      "[BC Phase] Epoch 2300/300000  Loss: 6.125e-05\n",
      "[BC Phase] Epoch 2400/300000  Loss: 5.730e-05\n",
      "[BC Phase] Epoch 2500/300000  Loss: 5.700e-05\n",
      "[BC Phase] Epoch 2600/300000  Loss: 6.206e-05\n",
      "[BC Phase] Epoch 2700/300000  Loss: 4.741e-05\n",
      "[BC Phase] Epoch 2800/300000  Loss: 4.512e-05\n",
      "[BC Phase] Epoch 2900/300000  Loss: 5.745e-05\n",
      "[BC Phase] Epoch 3000/300000  Loss: 5.499e-05\n",
      "[BC Phase] Epoch 3100/300000  Loss: 2.150e-04\n",
      "[BC Phase] Epoch 3200/300000  Loss: 4.968e-05\n",
      "[BC Phase] Epoch 3300/300000  Loss: 8.484e-05\n",
      "[BC Phase] Epoch 3400/300000  Loss: 5.680e-05\n",
      "[BC Phase] Epoch 3500/300000  Loss: 1.295e-04\n",
      "[BC Phase] Epoch 3600/300000  Loss: 4.048e-05\n",
      "[BC Phase] Epoch 3700/300000  Loss: 3.663e-05\n",
      "[BC Phase] Epoch 3800/300000  Loss: 3.999e-05\n",
      "[BC Phase] Epoch 3900/300000  Loss: 3.601e-05\n",
      "[BC Phase] Epoch 4000/300000  Loss: 5.533e-05\n",
      "[BC Phase] Epoch 4100/300000  Loss: 3.497e-05\n",
      "[BC Phase] Epoch 4200/300000  Loss: 3.374e-05\n",
      "[BC Phase] Epoch 4300/300000  Loss: 5.032e-05\n",
      "[BC Phase] Epoch 4400/300000  Loss: 3.962e-05\n",
      "[BC Phase] Epoch 4500/300000  Loss: 3.660e-05\n",
      "[BC Phase] Epoch 4600/300000  Loss: 3.854e-05\n",
      "[BC Phase] Epoch 4700/300000  Loss: 8.143e-05\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:58:37.027476Z",
     "start_time": "2025-05-07T13:58:37.024934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # # Pretraining\n",
    "# # EPOCHS_FOR_PRETRAIN = int(BUFFER_CAPACITY / 5)\n",
    "#\n",
    "# EPOCHS_FOR_PRETRAIN = 100000\n",
    "#\n",
    "# BATCH_SIZE = 256\n",
    "#\n",
    "# agent.pre_train(dir_path, EPOCHS_FOR_PRETRAIN, BATCH_SIZE, log_interval=1000)\n",
    "#\n",
    "# # agent.pre_train(path=dir_path, batch_size=BATCH_SIZE, epochs=EPOCHS_FOR_PRETRAIN, log_interval=10000)"
   ],
   "id": "a4f0b3cb7dd8c5bf",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving and loading the agent to make sure the agent has been stored",
   "id": "70b8ccd60f76155f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:04:54.257926Z",
     "start_time": "2025-05-12T15:04:53.730615Z"
    }
   },
   "cell_type": "code",
   "source": "filename_agent = agent.save(dir_path)",
   "id": "3c2a46a6186bb11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved successfully to C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2505121104.pkl\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:04:58.242057Z",
     "start_time": "2025-05-12T15:04:57.948198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent\n",
    "agent = Agent(\n",
    "    STATE_DIM,\n",
    "    ACTION_DIM,\n",
    "    ACTOR_LAYER_SIZES,\n",
    "    CRITIC_LAYER_SIZES,\n",
    "    BUFFER_CAPACITY,\n",
    "    ACTOR_LR,\n",
    "    CRITIC_LR,\n",
    "    SMOOTHING_STD,\n",
    "    NOISE_CLIP,\n",
    "    EXPLORATION_NOISE_STD,\n",
    "    GAMMA,\n",
    "    TAU,\n",
    "    MAX_ACTION,\n",
    "    POLICY_DELAY,\n",
    "    DEVICE\n",
    ")"
   ],
   "id": "867000e4048aaee9",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:04:59.170086Z",
     "start_time": "2025-05-12T15:04:58.392436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.load(filename_agent)\n",
    "agent.replay_buffer = replay_buffer"
   ],
   "id": "8dedc170773c152e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully from: C:\\Users\\HAMEDI\\OneDrive - McMaster University\\PythonProjects\\Polymer_example\\Data\\models\\agent_2505121104.pkl\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the accuracy of the agent and compare it to the MPC actions",
   "id": "7dbf22273b26a139"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:05:00.489466Z",
     "start_time": "2025-05-12T15:05:00.333456Z"
    }
   },
   "cell_type": "code",
   "source": "print_accuracy(agent.replay_buffer, agent, n_samples=5000, device=DEVICE)",
   "id": "88cd0cc887b9686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent r2 score for the predicted inputs compare to MPC inputs: 1.000000\n",
      "Agent r2 score for the predicted input 1 compare to MPC input 1: 1.000000\n",
      "Agent r2 score for the predicted input 1 compare to MPC input 2: 1.000000\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "95b901ffc64f4111",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
